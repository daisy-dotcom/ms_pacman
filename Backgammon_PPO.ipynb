{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Backgammon : An Atari Environment\n",
        "https://ale.farama.org/environments/backgammon/\n",
        "\n",
        "Agent: PPO\n",
        "\n",
        "Goal: To move all pieces off the board for either the RED or WHITE Player"
      ],
      "metadata": {
        "id": "SxgAN1H92x6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Action Space:      Discrete(0: FIRE, 1:RIGHT, 2: LEFT)\n",
        "*   Actions:           18 possible Actions\n",
        "*   Observation Space: Box(0, 255, (210, 160, 3) uint8)\n",
        "*   Environment Import: gymnasium.make(\"ALEBackgammon-v5\")\n",
        "*   Observation Type:   rgb, grayscale, ram\n",
        "*   Variants:           v5 or ram-v5\n",
        "*   Difficulty          3 choices"
      ],
      "metadata": {
        "id": "ckuNpYZa2xS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version History:\n",
        "*"
      ],
      "metadata": {
        "id": "IjjRV50R38Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"content/drive\")"
      ],
      "metadata": {
        "id": "erwVfSPM4Bln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "e86f9971-031c-4ddb-ddff-35166dc90721"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must be in a directory that exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ff6139bdd04e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mnormed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must be in a directory that exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_signal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must be in a directory that exists"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required Installations"
      ],
      "metadata": {
        "id": "aWAVQV-ZW_jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO be run once per Google Colab session\n",
        "!apt update\n",
        "!apt-get install xvfb x11-utils\n",
        "!apt-get install -y xvfb\n",
        "!python -m pip install gymnasium[atari]\n",
        "!python -m pip install pyvirtualdisplay\n",
        "!python -m pip install -- upgrade swig\n",
        "!python -m pip install --upgrade pyvirtualdisplay moviepy\n",
        "!python -m pip install --upgrade gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]\n",
        "!python -m pip install torchinfo\n",
        "!python -m pip install torch\n",
        "from torch.utils.data import Dataset\n",
        "!pip install box2d-py==2.3.5"
      ],
      "metadata": {
        "id": "KQQk9hh3XLzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba6d290-1b9c-49f9-c5ca-c3884e30c4b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5build2).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.10.1)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for upgrade\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.35.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (10.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (0.10.1)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached mujoco-3.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.35.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (10.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.9.4)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.20.2)\n",
            "Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Using cached mujoco-3.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation of Libraries"
      ],
      "metadata": {
        "id": "CGJSZGGhXPj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gym.vector import SyncVectorEnv\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display\n",
        "import moviepy.editor as mpy\n",
        "from torchinfo import summary\n",
        "import ale_py\n",
        "#import ipython_input_0_b35bc061a8ca import SACPolicyNetwork\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "# create random number generator\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# create and start virtual display\n",
        "display = Display(backend='xvfb')\n",
        "display.start()"
      ],
      "metadata": {
        "id": "-uKRoyA1XSNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c4793d3-06b9-4dfe-88ac-8082c81b6bae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7d6b04757880>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CPU setup"
      ],
      "metadata": {
        "id": "rVbmfUgXXmxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  torch.set_default_device(torch.device(device))\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "O2_SeNTsXibm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1548218b-3374-4ce1-87af-1031e524fca5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enabling Torch"
      ],
      "metadata": {
        "id": "gCLggwzOXsIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.get_default_device()"
      ],
      "metadata": {
        "id": "V5NhJ1CoXtqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c10897-2ad5-4a33-b483-64250e7db069"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for Classes"
      ],
      "metadata": {
        "id": "Wm0hbH0nXvQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register functions as methods\n",
        "# in classes\n",
        "def add_to_class(Class):\n",
        "  def wrapper(obj):\n",
        "    setattr(Class, obj.__name__, obj)\n",
        "  return wrapper"
      ],
      "metadata": {
        "id": "ONPVLmPKXxYt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seed to Test Correct Implementation"
      ],
      "metadata": {
        "id": "JPyg9Hj8Xz1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#seed = 12345\n",
        "#random.seed(seed)\n",
        "#np.random.seed(seed)\n",
        "#torch.manual_seed(seed)\n",
        "#if torch.cuda.is_available():\n",
        "#  torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "SU379OWDX2ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class to Store Rollout**\n",
        "Creates a dataset with the rollout data which will be passed to a Dataloader to create the mini-batches to update the actor and critic networks.\n",
        "\n",
        "Source: Ehsan Kamalinejad. (2023). PPO Training. https://colab.research.google.com/drive/1AmdtDNd_DUVRJlluaKIGT19BzBZ7loVZ?usp=sharing&authuser=1#scrollTo=56b3d5ca"
      ],
      "metadata": {
        "id": "SSisLHoAX5YW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Storage(Dataset):\n",
        "    def __init__(self, rollout, advantages, returns, envs):\n",
        "        # fill in the storage and flatten the parallel trajectories\n",
        "        self.observations = rollout['obs'].reshape((-1,) +  envs.single_observation_space.shape)\n",
        "        self.logprobs = rollout['logprobs'].reshape(-1)\n",
        "        self.actions = rollout['actions'].reshape((-1,) +  envs.single_action_space.shape).long()\n",
        "        self.advantages = advantages.reshape(-1)\n",
        "        self.returns = returns.reshape(-1)\n",
        "\n",
        "    def __getitem__(self, ix: int):\n",
        "        item = [\n",
        "            self.observations[ix],\n",
        "            self.logprobs[ix],\n",
        "            self.actions[ix],\n",
        "            self.advantages[ix],\n",
        "            self.returns[ix]\n",
        "        ]\n",
        "        return item\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.observations)"
      ],
      "metadata": {
        "id": "JsTKj1ByX-D-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified Value Network (Critic) & Policy Network (Actor)\n",
        "Use Tanh activation and orthogonal weight initilisation over Xavier weight initilisation to improve returns per episode."
      ],
      "metadata": {
        "id": "zawgo3hMYAZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOVNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hidden_sizes,\n",
        "                 output_size,\n",
        "                 learning_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        # create network layers\n",
        "        layers = nn.ModuleList()\n",
        "\n",
        "        # input layers\n",
        "        layer = self.init_weights((nn.Linear(in_channels, hidden_sizes[0])))\n",
        "        layers.append(layer)\n",
        "\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layer = self.init_weights(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            activation = nn.Tanh()\n",
        "            layers.append(layer)\n",
        "            layers.append(activation)\n",
        "\n",
        "        # output layers\n",
        "        layers.append(self.init_weights(nn.Linear(hidden_sizes[-1], output_size), std=1.0))\n",
        "\n",
        "        # combine layers into feed-forward network\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # select loss function and optimizer\n",
        "        # note: original paper uses modified MSE loss and RMSprop\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.device = torch.device('cuda:0')\n",
        "        else:\n",
        "          self.device = torch.device('cpu')\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convert input to torch format\n",
        "        if not torch.is_tensor(x):# or x.dtype != torch.float:\n",
        "            x = torch.tensor(x, dtype=torch.float)\n",
        "        x = x.to(self.device)\n",
        "        # return output of Q-network\n",
        "        return self.net(x)\n",
        "\n",
        "    def update(self, value_loss):\n",
        "        # update network weights for a minibatch of inputs and targets:\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = value_loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def copy_from(self, qnetwork):\n",
        "        # copy weights from another Q-network\n",
        "        self.net.load_state_dict(qnetwork.net.state_dict())"
      ],
      "metadata": {
        "id": "qoR7ijLKYEtm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Network"
      ],
      "metadata": {
        "id": "I6lMeCyPYH3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPOPolicyNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 hidden_sizes,\n",
        "                 output_size,\n",
        "                 learning_rate):\n",
        "        super().__init__()\n",
        "        # create network layers\n",
        "        layers = nn.ModuleList()\n",
        "\n",
        "        # input layers\n",
        "        layer = self.init_weights((nn.Linear(input_size, hidden_sizes[0])))\n",
        "        layers.append(layer)\n",
        "\n",
        "        layers.append(nn.Tanh())\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layer = self.init_weights(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            activation = nn.Tanh()\n",
        "            layers.append(layer)\n",
        "            layers.append(activation)\n",
        "\n",
        "        # output layers\n",
        "        # outputs a 1D tensor of size 1 with the Q value\n",
        "        layers.append(self.init_weights(nn.Linear(hidden_sizes[-1], output_size), std=1.0))\n",
        "\n",
        "        # combine layers into feed-forward network\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # select loss function and optimizer\n",
        "        # note: original paper uses modified MSE loss and RMSprop\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          self.device = torch.device('cuda:0')\n",
        "        else:\n",
        "          self.device = torch.device('cpu')\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convert input to torch format\n",
        "        if not torch.is_tensor(x) :#or x.dtype != torch.float:\n",
        "            x = torch.tensor(x, dtype=torch.float)\n",
        "        # return output of policy network\n",
        "        x = x.to(self.device)\n",
        "        return self.net(x)\n",
        "\n",
        "    def update(self, policy_loss, entropy_loss):\n",
        "        # update network weights for a given transition or trajectory\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = policy_loss + entropy_loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "KUIfOD5dYJGw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Initialisation"
      ],
      "metadata": {
        "id": "NgmBEXmOYLlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@add_to_class(PPOPolicyNetwork)\n",
        "def init_weights(self, layer, std=np.sqrt(2), bias_const=0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "_1QtoZ_TYNyP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@add_to_class(PPOVNetwork)\n",
        "def init_weights(self, layer, std=np.sqrt(2), bias_const=0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "YQuU0uyJYPdH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.vector.SyncVectorEnv([lambda: gym.make(\"ALE/Backgammon-v5\") for _ in range(5)])\n",
        "next_obs = test_env.reset()[0].astype(np.float32)\n",
        "next_obs\n"
      ],
      "metadata": {
        "id": "4tDIqda0YQ9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cf8b31-be93-4d1d-eb0b-dc7590c4cbaa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         ...,\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.],\n",
              "         [0., 0., 0.]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single Environment Creation"
      ],
      "metadata": {
        "id": "9B6oOqnFYU7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_env(env_name, seed=None):\n",
        "# def make_env(name, seed=None):\n",
        "    env = gym.make(env_name)\n",
        "#   env = gym.make(name, render_mode=\"rgb_array_list\")\n",
        "\n",
        "    if seed is not None:\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "    return env"
      ],
      "metadata": {
        "id": "WdP6hChHYTTi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO Agent"
      ],
      "metadata": {
        "id": "F5_oXDlWYZQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import stat_result\n",
        "class PPOAgent():\n",
        "    def __init__(self,\n",
        "                 env_name='Backgammon-v1',\n",
        "                 n_envs=2,\n",
        "                 gamma=0.99,\n",
        "                 hidden_sizes=(32, 32),\n",
        "                 learning_rate=0.001,\n",
        "                 batch_size=32,\n",
        "                 max_steps=200,\n",
        "                 lam=0.95,\n",
        "                 update_epochs=4,\n",
        "                 clip_epsilon=0.2,\n",
        "                 ent_coef=0.01,\n",
        "                 max_grad_norm=0.5,\n",
        "                 target_kl=None,\n",
        "                 num_returns_to_avg=3,\n",
        "                 num_episodes_to_avg=23,\n",
        "                 total_timesteps=100000):\n",
        "\n",
        "        self.n_envs = n_envs\n",
        "        self.env_name = env_name\n",
        "        self.envs = gym.vector.SyncVectorEnv([lambda: make_env(env_name, seed) for _ in range(self.n_envs)])\n",
        "\n",
        "        # check if the state space has correct type\n",
        "        #continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
        "        #assert continuous, 'Observation space must be continuous with shape (n,)'\n",
        "        self.state_dims = self.envs.single_observation_space.shape[0]\n",
        "        #self.state_dims = env.observation_space.n\n",
        "\n",
        "        # check if the action space has correct type\n",
        "        assert isinstance(self.envs.action_space, spaces.MultiDiscrete), 'Action space must be discrete'\n",
        "        self.num_actions = self.envs.single_action_space.n\n",
        "\n",
        "        # create actor & critic network\n",
        "        self.actor = PPOPolicyNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
        "        self.critic = PPOVNetwork(self.state_dims, hidden_sizes, 1, learning_rate)\n",
        "\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.max_steps = max_steps\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.update_epochs = update_epochs\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.ent_coef = ent_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.num_returns_to_avg = num_returns_to_avg\n",
        "        self.num_episodes_to_avg = num_episodes_to_avg\n",
        "        self.total_timesteps = total_timesteps\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def policy(self, state, stochastic=True):\n",
        "        # convert state to torch format\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor(state, dtype=torch.float)\n",
        "\n",
        "        # calculate action probabilities\n",
        "        logits = self.actor(state).detach()\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        if stochastic:\n",
        "            # sample action using action probabilities\n",
        "            return dist.sample().item()\n",
        "        else:\n",
        "            # select action with the highest probability\n",
        "            # note: we ignore breaking ties randomly (low chance of happening)\n",
        "            return dist.probs.argmax().item()\n",
        "\n",
        "    def train(self):\n",
        "        # train the agent for a number of updates/episodes\n",
        "        self.num_steps = 0\n",
        "\n",
        "        num_updates = self.total_timesteps//self.batch_size\n",
        "\n",
        "        self.all_returns = []\n",
        "\n",
        "        for update in range(1, num_updates):\n",
        "          cur_obs = torch.tensor(self.envs.reset()[0], dtype=torch.float32)\n",
        "          cur_done = torch.zeros(self.n_envs)\n",
        "\n",
        "          rollout = self.create_rollout(cur_obs, cur_done)\n",
        "\n",
        "          cur_done = rollout['cur_done']\n",
        "          cur_obs = rollout['cur_obs']\n",
        "          rewards = rollout['rewards']\n",
        "          dones = rollout['dones']\n",
        "          values = rollout['values']\n",
        "\n",
        "          advantage, returns = self.generalised_advantage_estimation(cur_obs, rewards, dones, values)\n",
        "          dataset = Storage(rollout, advantage, returns, self.envs)\n",
        "\n",
        "          kwargs = {'generator': torch.Generator(device=torch.get_default_device())} if torch.cuda.is_available() else {}\n",
        "\n",
        "          train_loader = DataLoader(dataset,\n",
        "                                    batch_size=self.batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    **kwargs)\n",
        "\n",
        "          self.ppo_update(train_loader, update, num_updates)\n",
        "\n",
        "\n",
        "          if len(self.all_returns) > self.num_returns_to_avg:\n",
        "            print(f\"\\rUpdate {update}/{num_updates} Avg episode reward for {self.num_returns_to_avg} eps: {np.mean(self.all_returns[-self.num_returns_to_avg:]):.2f}\", end='')\n",
        "          #print(f'Update{update}')\n",
        "        self.envs.close()\n",
        "\n",
        "    def save(self, path):\n",
        "        # save network weights to a file\n",
        "        torch.save(self.agent.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        # load network weights from a file\n",
        "        self.agent.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "FQ4V7hRFYaSZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO training"
      ],
      "metadata": {
        "id": "CK0HzsZyYe2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_to_class(PPOAgent)\n",
        "def get_action_and_value(self, state, action=None):\n",
        "    if not torch.is_tensor(state):# or state.dtype != torch.float  :\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "\n",
        "    state = state.to(self.actor.device)\n",
        "    #print(state)\n",
        "    logits = self.actor(state).detach()\n",
        "    probs = torch.distributions.Categorical(logits=logits)\n",
        "    if action is None:\n",
        "        action = probs.sample()\n",
        "\n",
        "    return action, probs.log_prob(action), probs.entropy(), self.critic(state)"
      ],
      "metadata": {
        "id": "1D4FafKhYhfP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generalised Advantage Estimation (GAE)"
      ],
      "metadata": {
        "id": "F3aAgLVoYjF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_to_class(PPOAgent)\n",
        "def generalised_advantage_estimation(self, cur_obs, rewards, dones, values):\n",
        "  advantages = torch.zeros((self.n_envs, self.max_steps))\n",
        "  last_advantage = 0\n",
        "\n",
        "  # the value after the last step\n",
        "  with torch.no_grad():\n",
        "      last_value = self.critic(cur_obs).reshape(1, -1)\n",
        "\n",
        "  # reverse recursive to calculate advantages based on the delta formula\n",
        "  for t in reversed(range(self.max_steps)):\n",
        "      # mask if episode completed after step t\n",
        "      mask = 1.0 - dones[:, t]\n",
        "      last_value = last_value * mask\n",
        "      last_advantage = last_advantage * mask\n",
        "      delta = rewards[:, t] + self.gamma * last_value - values[:, t]\n",
        "      last_advantage = delta + self.gamma * self.lam * last_advantage\n",
        "      advantages[:, t] = last_advantage\n",
        "      last_value = values[:, t]\n",
        "\n",
        "  advantages = advantages\n",
        "  returns = advantages + values\n",
        "\n",
        "  return advantages, returns"
      ],
      "metadata": {
        "id": "TO6O-oBLYmlp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Rollout"
      ],
      "metadata": {
        "id": "199yAC5XYpI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_to_class(PPOAgent)\n",
        "# starting obs, current termination status, track all returns\n",
        "def create_rollout(self, cur_observation, cur_done):\n",
        "  \"\"\"\n",
        "  create trajectories from all environments and store them\n",
        "  \"\"\"\n",
        "  #cur_observation = cur_observation.to(dtype=torch.float)\n",
        "  # empty tensors to store the rollouts\n",
        "  observations = torch.zeros((self.n_envs, self.max_steps ) + self.envs.single_observation_space.shape, dtype=torch.float)\n",
        "  actions = torch.zeros((self.n_envs, self.max_steps) + self.envs.single_action_space.shape, dtype=torch.float)\n",
        "  logprobs = torch.zeros((self.n_envs, self.max_steps),dtype=torch.float )\n",
        "  rewards = torch.zeros((self.n_envs,self.max_steps), dtype=torch.float)\n",
        "  dones = torch.zeros((self.n_envs, self.max_steps), dtype=torch.float)\n",
        "  values = torch.zeros((self.n_envs, self.max_steps), dtype=torch.float)\n",
        "\n",
        "  for t in range(self.max_steps):\n",
        "      observations[:,t] = cur_observation\n",
        "      dones[:,t] = cur_done\n",
        "      # give observation to the model and collect action, logprobs of actions, entropy and value\n",
        "      with torch.no_grad():\n",
        "          action, logprob, entropy, value = self.get_action_and_value(cur_observation)\n",
        "\n",
        "      values[:,t] = value.flatten()\n",
        "      actions[:,t] = action\n",
        "      logprobs[:,t] = logprob\n",
        "\n",
        "      # apply the action to the env and collect observation and reward\n",
        "      cur_observation, reward, cur_done, _, info = self.envs.step(action.cpu().numpy())\n",
        "      is_done = cur_done\n",
        "      rewards[:,t] = torch.tensor(reward).view(-1)\n",
        "      cur_observation = torch.tensor(cur_observation, dtype=torch.float32)\n",
        "      cur_done = torch.tensor(cur_done)\n",
        "\n",
        "      # if an episode ended store its total reward for progress report\n",
        "      #if is_done.any():\n",
        "      #  pos = np.where(is_done == True)\n",
        "      #  if info:\n",
        "      #    for item in info:\n",
        "      #      if item and \"episode\" in item:\n",
        "      #        #print(f\"Reward at end: {info['episode']['r'][pos]}\")\n",
        "      #        self.all_returns = self.all_returns + list(info['episode']['r'][pos])\n",
        "      #        break\n",
        "\n",
        "  self.all_returns.append(rewards.sum(dim=1).mean().cpu().numpy())\n",
        "\n",
        "  # create the rollout storage\n",
        "  rollout = {\n",
        "      'cur_obs': cur_observation,\n",
        "      'cur_done': cur_done,\n",
        "      'obs': observations,\n",
        "      'actions': actions,\n",
        "      'logprobs': logprobs,\n",
        "      'values': values,\n",
        "      'dones': dones,\n",
        "      'rewards': rewards\n",
        "  }\n",
        "\n",
        "  return rollout"
      ],
      "metadata": {
        "id": "m260Kao5YrF4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Policy Limits"
      ],
      "metadata": {
        "id": "PNLSAKJFYtvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_to_class(PPOAgent)\n",
        "def loss_clip(self, mb_oldlogporb, mb_newlogprob, mb_advantages):\n",
        "\n",
        "  # ratio between new and old policy\n",
        "  ratio = torch.exp(mb_newlogprob - mb_oldlogporb)\n",
        "  policy_loss = -mb_advantages * ratio\n",
        "\n",
        "  # clipped policy gradient loss enforces closeness\n",
        "  clipped_loss = -mb_advantages * torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
        "  pessimistic_loss = torch.max(policy_loss, clipped_loss).mean()\n",
        "  return pessimistic_loss"
      ],
      "metadata": {
        "id": "3Ym2MXP6YvoY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Loss of Value Function"
      ],
      "metadata": {
        "id": "R-RvKJQDYyxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@add_to_class(PPOAgent)\n",
        "def loss_vf(self, mb_oldreturns, mb_newvalues):\n",
        "    mb_newvalues = mb_newvalues.view(-1)\n",
        "    loss = 0.5 * ((mb_newvalues - mb_oldreturns) ** 2).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "qguXwyXHY1FI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini-Batch Update"
      ],
      "metadata": {
        "id": "ndEm-ys8Y2w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@add_to_class(PPOAgent)\n",
        "def ppo_update(self, trainloader, update, num_updates):\n",
        "\n",
        "  # linearly shrink the learning rate from the initial lr to zero\n",
        "  # for both the actor and critic networks\n",
        "  frac = 1.0 - (update - 1.0) / num_updates\n",
        "  self.actor.optimizer.param_groups[0][\"lr\"] = frac * self.lr\n",
        "  self.critic.optimizer.param_groups[0][\"lr\"] = frac * self.lr\n",
        "\n",
        "  # update loop\n",
        "  for epoch in range(self.update_epochs):\n",
        "      for batch in trainloader:\n",
        "          mb_observations, mb_logprobs, mb_actions, mb_advantages, mb_returns = batch\n",
        "\n",
        "          # calculate the distribution of actions through the updated model revisiting the old trajectories\n",
        "          _, mb_newlogprob, mb_entropy, mb_newvalues = self.get_action_and_value(mb_observations, mb_actions)\n",
        "\n",
        "          policy_loss = self.loss_clip(mb_logprobs, mb_newlogprob, mb_advantages)\n",
        "\n",
        "          value_loss = self.loss_vf(mb_returns, mb_newvalues)\n",
        "          self.critic.update(value_loss)\n",
        "\n",
        "          # average entory of the action space\n",
        "          entropy_loss = mb_entropy.mean()\n",
        "\n",
        "          self.actor.update(policy_loss.clone().detach().requires_grad_(True), entropy_loss.clone().detach().requires_grad_(True))\n",
        "\n",
        "          # extra clipping of the gradients to avoid overshoots\n",
        "          nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
        "          nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n"
      ],
      "metadata": {
        "id": "UF_831hdY5Y4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing in Environment"
      ],
      "metadata": {
        "id": "wPrdjUhYY8V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env_name = 'Backgammon-v1'\n",
        "n_envs = 32\n",
        "gamma = 0.99\n",
        "hidden_sizes = (64, 64, 64)\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "max_steps = 64\n",
        "lam = 0.95\n",
        "update_epochs = 2\n",
        "clip_epsilon = 0.2\n",
        "ent_coef = 0.01\n",
        "max_grad_norm = 0.5\n",
        "num_returns_to_avg = 3\n",
        "num_episodes_to_avg = 23\n",
        "total_timesteps = 30000\n",
        "\n",
        "agent = PPOAgent(\n",
        "    env_name = env_name,\n",
        "    n_envs = n_envs,\n",
        "    gamma = gamma,\n",
        "    hidden_sizes = hidden_sizes,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    max_steps = max_steps,\n",
        "    lam = lam,\n",
        "    update_epochs = update_epochs,\n",
        "    clip_epsilon = clip_epsilon,\n",
        "    ent_coef = ent_coef,\n",
        "    max_grad_norm = max_grad_norm,\n",
        "    num_returns_to_avg = num_returns_to_avg,\n",
        "    num_episodes_to_avg = num_episodes_to_avg,\n",
        "    total_timesteps = total_timesteps\n",
        ")\n"
      ],
      "metadata": {
        "id": "hk6BUqJVY-DC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "8d23f94f-08af-43cc-fe3e-ac89d322182c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'seed' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-6467ca535b4f>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m agent = PPOAgent(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mn_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8dd123e26846>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_name, n_envs, gamma, hidden_sizes, learning_rate, batch_size, max_steps, lam, update_epochs, clip_epsilon, ent_coef, max_grad_norm, target_kl, num_returns_to_avg, num_episodes_to_avg, total_timesteps)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyncVectorEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# check if the state space has correct type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/vector/sync_vector_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns, copy, observation_mode)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Initialise all sub-environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Define core attributes using the sub-environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gymnasium/vector/sync_vector_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Initialise all sub-environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menv_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv_fn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Define core attributes using the sub-environments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8dd123e26846>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSyncVectorEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# check if the state space has correct type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train()"
      ],
      "metadata": {
        "id": "0UjjHI-kY__g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot rewards"
      ],
      "metadata": {
        "id": "ngKAWwLxZDKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not len(agent.all_returns)%agent.num_episodes_to_avg==0:\n",
        "    all_returns_truncated = np.array(agent.all_returns[:-(len(agent.all_returns)%agent.num_episodes_to_avg)])\n",
        "else:\n",
        "    all_returns_truncated = np.array(agent.all_returns)\n",
        "#all_returns_smoothed = np.average(all_returns_truncated.reshape(-1, agent.num_episodes_to_avg), axis=1)\n",
        "print('mean reward:', np.mean(all_returns_truncated))\n",
        "print('std reward:', np.std(all_returns_truncated))\n",
        "print('max reward:', np.max(all_returns_truncated))\n",
        "print('converge mean reward:', np.mean(all_returns_truncated[-1]))\n",
        "plt.xlabel('Number of Terminated Episodes')\n",
        "plt.ylabel('Max Reward at the Termination of an Env')\n",
        "plt.plot(all_returns_truncated)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4isffNz1ZEmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = make_env(env_name, seed)\n",
        "terminated = False\n",
        "truncated = False\n",
        "steps = 0\n",
        "total_reward = 0\n",
        "observation, _ = test_env.reset()\n",
        "observation = torch.unsqueeze(torch.tensor(observation, torch.float32),dim=0)\n",
        "\n",
        "while not (terminated or truncated):\n",
        "    # take action based on policy\n",
        "    with torch.no_grad():\n",
        "      action, _, _, _ = agent.get_action_and_value(observation)\n",
        "    action = action.cpu().item()\n",
        "\n",
        "    # environment receives the action and returns:\n",
        "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
        "    observation, reward, terminated, truncated, info = test_env.step(action)\n",
        "    observation = torch.unsqueeze(torch.tensor(observation),dim=0)\n",
        "    #print(f'Reward: {reward}')\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "print(f'Reward: {total_reward}')\n",
        "\n",
        "# store RGB frames for the entire episode\n",
        "frames = test_env.render()\n",
        "\n",
        "# close the environment\n",
        "test_env.close()\n",
        "\n",
        "# create and play video clip using the frames and given fps\n",
        "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
        "clip.ipython_display(rd_kwargs=dict(logger=None))"
      ],
      "metadata": {
        "id": "yWzSj01WZHWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backgammon Results\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Initial hyperparameters set using Schulman et al. (2017) Proximal Policy Optimisation Algorithms paper's parameter for Atari environments\n",
        "Horizon - 128\n",
        "Number of environments - 8\n",
        "Learning Rate -\n",
        "Batch size - 32\n",
        "Discount - 0.99\n",
        "GAE parameter (lambda) - 0.95\n",
        "Clipping parameter - 0.1\n",
        "Entropy coef - 0.01"
      ],
      "metadata": {
        "id": "2zXt_10ZZMKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'ALE/MsPacman-ram-v5'\n",
        "n_envs = 8\n",
        "gamma = 0.99\n",
        "hidden_sizes = (64, 64, 64)\n",
        "learning_rate = 0.00025\n",
        "batch_size = 32\n",
        "max_steps = 128\n",
        "lam = 0.95\n",
        "update_epochs = 2\n",
        "clip_epsilon = 0.1\n",
        "ent_coef = 0.01\n",
        "max_grad_norm = 0.5\n",
        "num_returns_to_avg = 3\n",
        "num_episodes_to_avg = 5\n",
        "total_timesteps = 100000\n",
        "\n",
        "agent = PPOAgent(\n",
        "    env_name = env_name,\n",
        "    n_envs = n_envs,\n",
        "    gamma = gamma,\n",
        "    hidden_sizes = hidden_sizes,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    max_steps = max_steps,\n",
        "    lam = lam,\n",
        "    update_epochs = update_epochs,\n",
        "    clip_epsilon = clip_epsilon,\n",
        "    ent_coef = ent_coef,\n",
        "    max_grad_norm = max_grad_norm,\n",
        "    num_returns_to_avg = num_returns_to_avg,\n",
        "    num_episodes_to_avg = num_episodes_to_avg,\n",
        "    total_timesteps = total_timesteps\n",
        ")\n"
      ],
      "metadata": {
        "id": "FNmx3AHlZOjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent.train()"
      ],
      "metadata": {
        "id": "ceJpGe7NZS_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not len(agent.all_returns)%agent.num_episodes_to_avg==0:\n",
        "    all_returns_truncated = np.array(agent.all_returns[:-(len(agent.all_returns)%agent.num_episodes_to_avg)])\n",
        "else:\n",
        "    all_returns_truncated = np.array(agent.all_returns)\n",
        "#all_returns_smoothed = np.average(all_returns_truncated.reshape(-1, agent.num_episodes_to_avg), axis=1)\n",
        "print('mean reward:', np.mean(all_returns_truncated))\n",
        "print('std reward:', np.std(all_returns_truncated))\n",
        "print('max reward:', np.max(all_returns_truncated))\n",
        "print('converge mean reward:', np.mean(all_returns_truncated[-1]))\n",
        "plt.xlabel('Number of Terminated Episodes')\n",
        "plt.ylabel('Max Reward at the Termination of an Env')\n",
        "plt.plot(all_returns_truncated)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XtUqO92QZVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing Trajectory Length"
      ],
      "metadata": {
        "id": "mX7R_fUKZXnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = 'ALE/MsPacman-ram-v5'\n",
        "n_envs = 8\n",
        "gamma = 0.99\n",
        "hidden_sizes = (64, 64, 64)\n",
        "learning_rate = 0.00025\n",
        "batch_size = 32\n",
        "max_steps = 256\n",
        "lam = 0.95\n",
        "update_epochs = 2\n",
        "clip_epsilon = 0.1\n",
        "ent_coef = 0.01\n",
        "max_grad_norm = 0.5\n",
        "num_returns_to_avg = 3\n",
        "num_episodes_to_avg = 5\n",
        "total_timesteps = 100000\n",
        "\n",
        "agent = PPOAgent(\n",
        "    env_name = env_name,\n",
        "    n_envs = n_envs,\n",
        "    gamma = gamma,\n",
        "    hidden_sizes = hidden_sizes,\n",
        "    learning_rate = learning_rate,\n",
        "    batch_size = batch_size,\n",
        "    max_steps = max_steps,\n",
        "    lam = lam,\n",
        "    update_epochs = update_epochs,\n",
        "    clip_epsilon = clip_epsilon,\n",
        "    ent_coef = ent_coef,\n",
        "    max_grad_norm = max_grad_norm,\n",
        "    num_returns_to_avg = num_returns_to_avg,\n",
        "    num_episodes_to_avg = num_episodes_to_avg,\n",
        "    total_timesteps = total_timesteps\n",
        ")"
      ],
      "metadata": {
        "id": "orpPAFiCZZ9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent.train()"
      ],
      "metadata": {
        "id": "vtDNBExwZcz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not len(agent.all_returns)%agent.num_episodes_to_avg==0:\n",
        "    all_returns_truncated = np.array(agent.all_returns[:-(len(agent.all_returns)%agent.num_episodes_to_avg)])\n",
        "else:\n",
        "    all_returns_truncated = np.array(agent.all_returns)\n",
        "#all_returns_smoothed = np.average(all_returns_truncated.reshape(-1, agent.num_episodes_to_avg), axis=1)\n",
        "print('mean reward:', np.mean(all_returns_truncated))\n",
        "print('std reward:', np.std(all_returns_truncated))\n",
        "print('max reward:', np.max(all_returns_truncated))\n",
        "print('converge mean reward:', np.mean(all_returns_truncated[-1]))\n",
        "plt.xlabel('Number of Terminated Episodes')\n",
        "plt.ylabel('Max Reward at the Termination of an Env')\n",
        "plt.plot(all_returns_truncated)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ceZyUJEBZe9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = make_env(env_name, seed)\n",
        "#test_env = gym.wrappers.RecordVideo(test_env, f\"videos/{env_name}test\")\n",
        "max_ep_steps = 5000\n",
        "#observation, _ = test_env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "steps = 0\n",
        "total_reward = 0\n",
        "observation, _ = test_env.reset()\n",
        "observation = torch.unsqueeze(torch.tensor(observation, dtype=torch.float32),dim=0)\n",
        "\n",
        "while not (terminated or truncated):\n",
        "    # take action based on policy\n",
        "    with torch.no_grad():\n",
        "      action, _, _, _ = agent.get_action_and_value(observation)\n",
        "    action = action.cpu().item()\n",
        "\n",
        "    # environment receives the action and returns:\n",
        "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
        "    observation, reward, terminated, truncated, info = test_env.step(action)\n",
        "    observation = torch.unsqueeze(torch.tensor(observation, dtype=torch.float32),dim=0)\n",
        "    #print(f'Reward: {reward}')\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "print(f'Reward: {total_reward}')\n",
        "\n",
        "# store RGB frames for the entire episode\n",
        "frames = test_env.render()\n",
        "\n",
        "# close the environment\n",
        "test_env.close()\n",
        "\n",
        "# create and play video clip using the frames and given fps\n",
        "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
        "clip.ipython_display(rd_kwargs=dict(logger=None))"
      ],
      "metadata": {
        "id": "pzmNbH5kZglA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}