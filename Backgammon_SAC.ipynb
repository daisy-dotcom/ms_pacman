{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANtL3p9HwbJk"
      },
      "source": [
        "**Backgammon : An Atari Environment**\n",
        "https://ale.farama.org/environments/backgammon/\n",
        "\n",
        "Agent: SAC\n",
        "\n",
        "Goal: To move all pieces off the board for either the RED or WHITE Player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF0UFZ9rxkc_"
      },
      "source": [
        "*   Action Space:      3 = Discrete(0: FIRE, 1:RIGHT, 2: LEFT)\n",
        "*   Observation Space: Box(0, 255, (210, 160, 3) uint8)\n",
        "*   Environment Import: gymnasium.make(\"ALEBackgammon-v5\")\n",
        "*   Observation Type:   rgb, grayscale, ram\n",
        "*   Variants:           v5 or ram-v5\n",
        "*   Difficulty          3 choices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBCGITgzyTFC"
      },
      "source": [
        "Version History\n",
        "*   Version One: Based upon \"\"\n",
        "*   Version Two: Improvements to the Practicum\n",
        "*   Referencing Daisy's research (team member)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMDWVG_twZaH",
        "outputId": "08b2474d-2c7d-4a35-fee1-a3e5caab2fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItBieg8oyYqy"
      },
      "source": [
        "Required Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhVj46EEybA5",
        "outputId": "8556af18-45a1-49fc-b42c-06a8844ea17f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,451 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,648 kB]\n",
            "Fetched 4,356 kB in 2s (1,841 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "53 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5build2).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.10.1)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for upgrade\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.35.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (10.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.6.1)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.* (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached mujoco-3.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.35.1)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (0.10.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (10.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.9.4)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.20.2)\n",
            "Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Using cached mujoco-3.2.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# TO be run once per Google Colab session\n",
        "!apt update\n",
        "!apt-get install xvfb x11-utils\n",
        "!apt-get install -y xvfb\n",
        "!python -m pip install gymnasium[atari]\n",
        "!python -m pip install pyvirtualdisplay\n",
        "!python -m pip install -- upgrade swig\n",
        "!python -m pip install --upgrade pyvirtualdisplay moviepy\n",
        "!python -m pip install --upgrade gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]\n",
        "!python -m pip install torchinfo\n",
        "!pip install box2d-py==2.3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK9ypKP0ygyq"
      },
      "source": [
        "Importation of Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "J8WfHQsqyjny",
        "outputId": "37393c69-7188-4a77-a858-cc01093aa85e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'SACPolicyNetwork'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-66f00a3a277c>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0male_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#import ipython_input_0_b35bc061a8ca import SACPolicyNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mSACPolicyNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'SACPolicyNetwork'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display\n",
        "import moviepy.editor as mpy\n",
        "from torchinfo import summary\n",
        "import ale_py\n",
        "#import ipython_input_0_b35bc061a8ca import SACPolicyNetwork\n",
        "#import SACPolicyNetwork\n",
        "import random\n",
        "\n",
        "gym.register_envs(ale_py)\n",
        "# create random number generator\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# create and start virtual display\n",
        "display = Display(backend='xvfb')\n",
        "display.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxKIaiqyyqaa"
      },
      "source": [
        "The Environment settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6dKpwVGys5T",
        "outputId": "bae54d1a-448d-4309-cb37-140e1a3dee1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (210, 160, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "env = gym.make('ALE/Backgammon-v5', render_mode=\"rgb_array_list\")\n",
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3acQXPY9q15",
        "outputId": "c6f76283-e189-41ca-b41a-7e264c45c198"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "torch.get_default_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYvqJWak9uVg",
        "outputId": "c91bfa71-b01d-4531-b8ad-9538e8034186"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  torch.set_default_device(torch.device(device))\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKruZMiG9wV4",
        "outputId": "3826883b-263f-4e22-95cc-d7c4c53d553a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "torch.get_default_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iyoyh9DY9yoY"
      },
      "outputs": [],
      "source": [
        "#seed = 4\n",
        "#random.seed(seed)\n",
        "#np.random.seed(seed)\n",
        "#torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MNh6g8xr94T5"
      },
      "outputs": [],
      "source": [
        "# SAC Q-network for approximating action-value function\n",
        "class SACQNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hidden_sizes,\n",
        "                 output_size,\n",
        "                 learning_rate):\n",
        "        super().__init__()\n",
        "        # create network layers\n",
        "        layers = nn.ModuleList()\n",
        "        # Flattening the SHAPE for backgammon\n",
        "        self.flatten_size = 210 * 160 * 3\n",
        "        # input layers\n",
        "        layers = nn.ModuleList()\n",
        "        layers.append(nn.Linear(self.flatten_size, hidden_sizes[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        # output layers\n",
        "        # outputs a 1D tensor of size 1 with the Q value\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "\n",
        "        # combine layers into feed-forward network\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # select loss function and optimizer\n",
        "        # note: original paper uses modified MSE loss and RMSprop\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "        # initialise the weights according to dueling network architecture\n",
        "        self.net.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "    # Xavier initialisation - well suited for ReLU activations\n",
        "\n",
        "    def forward(self, x):\n",
        "        # convert input to torch format\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x, dtype=torch.float)\n",
        "        x = x.view(x.size(0), -1) # Flattening the SHAPE for backgammon\n",
        "        # return output of Q-network\n",
        "        return self.net(x)\n",
        "\n",
        "    def update(self, state, action, targets):\n",
        "        # Placing tensors\n",
        "        state = state.to(next(self.parameters()).device)\n",
        "        action = torch.tensor(action, dtype=torch.long).to(state.device)\n",
        "        targets = targets.to(state,device)\n",
        "        # update network weights for a minibatch of inputs and targets:\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.forward(state)\n",
        "        #print(outputs.shape)\n",
        "        #print(targets.shape)\n",
        "        outputs = outputs.gather(1, action.long())\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    # Gather method - selecting Q-values corresponding to taken actions\n",
        "\n",
        "    def copy_from(self, qnetwork):\n",
        "        # copy weights from another Q-network\n",
        "        self.net.load_state_dict(qnetwork.net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gVkMGluP98Sp"
      },
      "outputs": [],
      "source": [
        "class TwinQNetwork(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_sizes, output_size, learning_rate):\n",
        "        super().__init__()\n",
        "        # create twin network - standard practice to stablise training in double Q-learning\n",
        "        self.q1 = SACQNetwork(in_channels, hidden_sizes, output_size, learning_rate)\n",
        "        self.q2 = SACQNetwork(in_channels, hidden_sizes, output_size, learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return output of Q-network for the state + action input\n",
        "        return self.q1(x), self.q2(x)\n",
        "\n",
        "    def update(self, inputs, action, targets):\n",
        "        # update network weights for a minibatch of inputs and targets:\n",
        "        # both Q-networks are updated, learning from same batch of experience\n",
        "        self.q1.update(inputs, action, targets)\n",
        "        self.q2.update(inputs, action, targets)\n",
        "\n",
        "    def copy_from(self, sac_qnetwork):\n",
        "        # copy weights from another Q-network\n",
        "        self.q1.copy_from(sac_qnetwork.q1)\n",
        "        self.q2.copy_from(sac_qnetwork.q2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "psSXomA7-Aep"
      },
      "outputs": [],
      "source": [
        "# Policy network for approximating policy function\n",
        "# source: https://arxiv.org/pdf/1910.07207\n",
        "class SACPolicyNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 hidden_sizes,\n",
        "                 output_size,\n",
        "                 learning_rate):\n",
        "        super().__init__()\n",
        "        # create network layers\n",
        "        layers = nn.ModuleList()\n",
        "        # define the flatten_size here\n",
        "        self.flatten_size = input_size\n",
        "        # input layer\n",
        "        layers.append(nn.Linear(self.flatten_size, hidden_sizes[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        # output layer (preferences/logits/unnormalised log-probabilities)\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "        #layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        # combine layers into feed-forward network\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # select optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
        "\n",
        "        # initialise the weights according to dueling network architecture\n",
        "        self.net.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "        # Xavier initialization - effective for ReLU activations\n",
        "\n",
        "    def forward(self, x):\n",
        "      # Processing input and returns output of policy network\n",
        "        # convert input to torch format\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x, dtype=torch.float)\n",
        "        x = x.view(x.size(0), -1) # flatten the input for backgammon\n",
        "        # return output of policy network\n",
        "        return self.net(x)\n",
        "\n",
        "    def get_action_and_log_prob(self, state):\n",
        "      logits = self.forward(state)\n",
        "      dist = torch.distributions.Categorical(logits=logits)\n",
        "      action_probs = dist.probs\n",
        "      action = dist.sample()\n",
        "      z = action_probs == 0\n",
        "      z = z.float() * 1e-8\n",
        "      log_pi = torch.log(action_probs + z)\n",
        "      return action, action_probs, log_pi\n",
        "\n",
        "    def update(self, policy_loss):\n",
        "        # update network weights for a given transition or trajectory\n",
        "        self.optimizer.zero_grad()\n",
        "        #logits = self.net(states)\n",
        "        #dist = torch.distributions.Categorical(logits=logits)\n",
        "        #loss = torch.mean(-dist.log_prob(actions)*entropy_coef)\n",
        "        loss = policy_loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "71XH7or_-Tgi"
      },
      "outputs": [],
      "source": [
        "from os import stat_result\n",
        "# source: https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.py\n",
        "class SAC():\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 gamma,\n",
        "                 hidden_sizes=(32, 32),\n",
        "                 replay_size=10000,\n",
        "                 minibatch_size=32,\n",
        "                 actor_learning_rate=0.001,\n",
        "                 critic_learning_rate=0.005,\n",
        "                 target_update=20,\n",
        "                 warmup_target=100,\n",
        "                 training_ep_per_eval=4,\n",
        "                 target_entropy_ratio=0.98,\n",
        "                 optim_time=4):\n",
        "        # Defining expected_size within the class\n",
        "        #self.expected_size = 210 * 160 * 3\n",
        "        # check if the state space has correct type\n",
        "        #continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
        "        #assert continuous, 'Observation space must be continuous with shape (n,)'\n",
        "        self.state_dims = env.observation_space.shape[0]\n",
        "        #self.state_dims = env.observation_space.n\n",
        "        example_state, _ = env.reset()\n",
        "        self.expected_seze = example_state.flatten().shape[0]\n",
        "        #self.expected_size = env.observation_space.shape[0]*env.observation_space.shape[1]*env.observation_space.shape[2]\n",
        "\n",
        "        # check if the action space has correct type\n",
        "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "#        self.online_critic = TwinQNetwork(self.state_dims, hidden_sizes, self.num_actions, critic_learning_rate)\n",
        "#        self.target_critic = TwinQNetwork(self.state_dims, hidden_sizes, self.num_actions, critic_learning_rate)\n",
        "\n",
        "        self.online_critic = TwinQNetwork(self.expected_size, hidden_sizes, self.num_actions, critic_learning_rate)\n",
        "        self.target_critic = TwinQNetwork(self.expected_size, hidden_sizes, self.num_actions, critic_learning_rate)\n",
        "\n",
        "        self.target_critic.copy_from(self.online_critic)\n",
        "\n",
        "        # create policy network\n",
        "        self.actor = SACPolicyNetwork(self.expected_size, hidden_sizes, self.num_actions, actor_learning_rate)\n",
        "\n",
        "        # replay buffer\n",
        "        self.replay_buffer = deque(maxlen=replay_size)\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.target_update = target_update\n",
        "        self.target_update_steps = 0\n",
        "        self.target_update_idx = 0\n",
        "\n",
        "        # automatic entropy tuning\n",
        "        self.target_entropy = target_entropy_ratio * -np.array(-np.log(1.0/self.num_actions), dtype=np.float32)\n",
        "        self.log_entropy_coef = torch.zeros(1, requires_grad=True)\n",
        "        self.entropy_optim = torch.optim.Adam([self.log_entropy_coef], lr=actor_learning_rate)\n",
        "        self.entropy_coef = self.log_entropy_coef.exp()\n",
        "\n",
        "        self.warmup_target = warmup_target\n",
        "        self.training_ep_per_eval = training_ep_per_eval\n",
        "        self.optim_time = optim_time\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "\n",
        "\n",
        "    def policy(self, state, stochastic=True, eval=False):\n",
        "        # convert state to torch format\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor(state, dtype=torch.float)\n",
        "\n",
        "        # calculate action probabilities\n",
        "        if not eval:\n",
        "          logits = self.actor(state).detach()\n",
        "        else:\n",
        "          with torch.no_grad():\n",
        "            logits = self.actor(state).detach()\n",
        "\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        if stochastic:\n",
        "            # sample action using action probabilities\n",
        "            return dist.sample().item()\n",
        "        else:\n",
        "            # select action with the highest probability\n",
        "            # note: we ignore breaking ties randomly (low chance of happening)\n",
        "            return dist.probs.argmax().item()\n",
        "\n",
        "    def get_experiences(self):\n",
        "      batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False)\n",
        "      #print(self.replay_buffer)\n",
        "\n",
        "      replay_state = np.zeros((self.minibatch_size, self.expected_size))\n",
        "      replay_action = np.zeros((self.minibatch_size, 1))\n",
        "      replay_reward = np.zeros((self.minibatch_size, 1))\n",
        "      replay_next_state = np.zeros((self.minibatch_size, self.state_dims))\n",
        "      is_failure = np.zeros((self.minibatch_size, 1))\n",
        "\n",
        "      i = 0\n",
        "      for row in batch:\n",
        "        replay_state[i] = self.replay_buffer[row][0]\n",
        "        replay_action[i] = self.replay_buffer[row][1]\n",
        "        replay_reward[i] = self.replay_buffer[row][2]\n",
        "        replay_next_state[i] = self.replay_buffer[row][3]\n",
        "        is_failure[i] = self.replay_buffer[row][4]\n",
        "        i += 1\n",
        "\n",
        "      #np_replay_buffer = np.array(self.replay_buffer)\n",
        "      #replay_state, replay_action, replay_reward, replay_next_state, is_failure = list(zip(*np_replay_buffer[batch]))\n",
        "\n",
        "      replay_state = torch.tensor(replay_state, dtype=torch.float)\n",
        "      replay_action = torch.tensor(replay_action, dtype=torch.float)\n",
        "      replay_reward = torch.tensor(replay_reward, dtype=torch.float)\n",
        "      replay_next_state = torch.tensor(replay_next_state, dtype=torch.float)\n",
        "      is_failure = torch.tensor(is_failure, dtype=torch.float)\n",
        "\n",
        "      return replay_state, replay_action, replay_reward, replay_next_state, is_failure\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "      # unpack trajectory\n",
        "      #batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False)\n",
        "      #t_replay_buffer = torch.Tensor(self.replay_buffer)\n",
        "      #if len(self.replay_buffer) >= self.minibatch_size:\n",
        "\n",
        "      replay_state, replay_action, replay_reward, replay_next_state, is_failure = self.get_experiences()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        action, next_action_probs, next_log_pi = self.actor.get_action_and_log_prob(replay_next_state)\n",
        "        #dist = torch.distributions.Categorical(logits=logits)\n",
        "        #next_log_pi = dist.logits\n",
        "        #next_action_probs = torch.exp(next_log_pi)\n",
        "\n",
        "        next_target_q1, next_target_q2 = self.target_critic(replay_next_state)\n",
        "        min_next_q = torch.min(next_target_q1, next_target_q2)\n",
        "\n",
        "        min_next_target_q = next_action_probs * (min_next_q - self.entropy_coef * next_log_pi)\n",
        "        min_next_target_q = min_next_target_q.sum(dim=1, keepdim=True)\n",
        "\n",
        "        next_q = replay_reward + (1.0 - is_failure) * self.gamma * min_next_target_q\n",
        "\n",
        "      # update online critic\n",
        "      self.online_critic.update(replay_next_state, replay_action, next_q)\n",
        "\n",
        "      # calculate policy loss - actor\n",
        "      # get q-value estimates of visited states\n",
        "      q1, q2 = self.online_critic(replay_state.view(-1, self.expected_size))\n",
        "      min_q = torch.min(q1, q2)\n",
        "\n",
        "      action, action_probs, log_pi = self.actor.get_action_and_log_prob(replay_state)\n",
        "      #dist = torch.distributions.Categorical(logits=logits)\n",
        "      #log_pi = dist.logits\n",
        "      #action_probs = torch.exp(log_pi)\n",
        "\n",
        "      policy_loss = action_probs * ((self.entropy_coef * log_pi)  - min_q)\n",
        "      policy_loss = policy_loss.sum(dim=1).mean()\n",
        "\n",
        "      # entropy tuning\n",
        "      entropy_loss = -(self.log_entropy_coef * (log_pi + self.target_entropy).detach()).mean()\n",
        "\n",
        "      # update actor\n",
        "      self.actor.update(policy_loss)\n",
        "\n",
        "      # update entropy\n",
        "      self.entropy_optim.zero_grad()\n",
        "      entropy_loss.backward()\n",
        "      self.entropy_optim.step()\n",
        "      self.entropy_coef = self.log_entropy_coef.exp().detach()\n",
        "\n",
        "      # update target critic network\n",
        "      #self.target_update_idx += 1\n",
        "      if self.target_update_steps % self.target_update == 0:\n",
        "        self.target_critic.copy_from(self.online_critic)\n",
        "\n",
        "\n",
        "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
        "        # train the agent for a number of episodes\n",
        "        self.num_steps = 0\n",
        "\n",
        "        episode_rewards = []\n",
        "        training_rewards = []\n",
        "        for episode in range(max_episodes):\n",
        "            steps = 0\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            # convert state to torch format\n",
        "            #state = torch.tensor(state, dtype=torch.float)\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "\n",
        "\n",
        "            rewards = []\n",
        "            trajectory = []\n",
        "            eval_ep = episode % self.training_ep_per_eval == 0\n",
        "            if eval_ep:\n",
        "              episode_rewards.append(0)\n",
        "            else:\n",
        "              training_rewards.append(0)\n",
        "\n",
        "            # generate trajectory\n",
        "            while not (terminated or truncated):\n",
        "                # select action by following policy\n",
        "\n",
        "                action = self.policy(state, stochastic=not eval_ep, eval=eval_ep)\n",
        "\n",
        "                # send the action to the environment\n",
        "                next_state, reward, terminated, truncated, _  = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                if eval_ep:\n",
        "                  episode_rewards[-1] += reward\n",
        "                else:\n",
        "                  training_rewards[-1] += reward\n",
        "\n",
        "                is_failure = terminated and not truncated\n",
        "\n",
        "                # add transition to trajectory\n",
        "                #experience = np.concatenate(())\n",
        "                if not eval_ep:\n",
        "                  self.replay_buffer.append((state,\n",
        "                                            action,\n",
        "                                            reward,\n",
        "                                            next_state,\n",
        "                                            float(is_failure)))\n",
        "\n",
        "                  #self.update()\n",
        "\n",
        "                # convert next state to torch format\n",
        "                #next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "\n",
        "                state = next_state\n",
        "                self.num_steps += 1\n",
        "                steps += 1\n",
        "\n",
        "                # update policy & critic networks\n",
        "                done_warmup = self.num_steps > self.warmup_target\n",
        "                enough_exp = len(self.replay_buffer) >= self.minibatch_size\n",
        "                optim_time = self.num_steps % self.optim_time == 0\n",
        "                if (done_warmup and enough_exp and optim_time):\n",
        "                  self.update()\n",
        "\n",
        "            if eval_ep:\n",
        "              print(f'\\rEpisode {episode+1} done: steps = {self.num_steps}, '\n",
        "                    f'rewards = {np.sum(rewards)}', end='')\n",
        "\n",
        "            else:\n",
        "              print(f'\\r Training Episode {episode+1} done: steps = {self.num_steps}, '\n",
        "                    f'rewards = {np.sum(rewards)}', end='')\n",
        "\n",
        "            if episode >= criterion_episodes-1 and stop_criterion(episode_rewards[-criterion_episodes:]):\n",
        "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
        "                break\n",
        "\n",
        "        # plot rewards received during training\n",
        "        plt.figure(dpi=100)\n",
        "        plt.plot(range(1, len(episode_rewards)+1), episode_rewards, label=f'Rewards')\n",
        "\n",
        "        plt.xlabel('Eval Episodes')\n",
        "        plt.ylabel('Rewards per episode')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(dpi=100)\n",
        "        plt.plot(range(1, len(training_rewards)+1), training_rewards, label=f'Rewards')\n",
        "\n",
        "        plt.xlabel('Training Episodes')\n",
        "        plt.ylabel('Rewards per episode')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate_agent(self, num_episodes=1):\n",
        "      eval_rewards = []\n",
        "      steps = 0\n",
        "      for episode in range(num_episodes):\n",
        "        state, _ = self.env.reset()\n",
        "\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        eval_rewards.append(0)\n",
        "        # generate trajectory\n",
        "        while not (terminated or truncated):\n",
        "            # select action by following a greedy policy\n",
        "            with torch.no_grad():\n",
        "              action = self.policy(state, stochastic=False)\n",
        "\n",
        "            # send the action to the environment\n",
        "            next_state, reward, terminated, truncated, _  = self.env.step(action)\n",
        "            eval_rewards[-1] += reward\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "      return np.mean(eval_rewards), steps\n",
        "\n",
        "\n",
        "    def copy_from(self, policynet):\n",
        "       # copy weights from another Q-network\n",
        "       self.net.copy_from(policynet)\n",
        "\n",
        "    def save(self, path):\n",
        "        # save network weights to a file\n",
        "        torch.save(self.actor.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        # load network weights from a file\n",
        "        self.actor.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "lqMLgt57-WGq",
        "outputId": "ed4004af-7256-4bc4-c7dc-13279f20d0fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SAC' object has no attribute 'expected_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1014845a4622>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Activating the SAC agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m agent = SAC(env,\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-5b23d31780ff>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, gamma, hidden_sizes, replay_size, minibatch_size, actor_learning_rate, critic_learning_rate, target_update, warmup_target, training_ep_per_eval, target_entropy_ratio, optim_time)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#        self.target_critic = TwinQNetwork(self.state_dims, hidden_sizes, self.num_actions, critic_learning_rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monline_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwinQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwinQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SAC' object has no attribute 'expected_size'"
          ]
        }
      ],
      "source": [
        "# create the atari environment\n",
        "env = gym.make('ALE/Backgammon-v5', render_mode=\"rgb_array_list\")\n",
        "\n",
        "# Hyperparameter Definitions\n",
        "gamma = 0.99\n",
        "hidden_sizes = (64, 64)\n",
        "actor_learning_rate = 0.001\n",
        "critic_learning_rate = 0.005\n",
        "#epsilon = 0.01\n",
        "#rep_omega = 0.2\n",
        "replay_size = 100000\n",
        "minibatch_size = 64\n",
        "target_update = 20000\n",
        "training_ep_per_eval = 5\n",
        "warmup_target = 3000\n",
        "target_entropy_ratio = 0.9\n",
        "\n",
        "max_episodes = 10\n",
        "max_steps = 18000\n",
        "criterion_episodes = 5\n",
        "\n",
        "# Activating the SAC agent\n",
        "agent = SAC(env,\n",
        "            gamma=gamma,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            actor_learning_rate=actor_learning_rate,\n",
        "            critic_learning_rate=critic_learning_rate,\n",
        "            replay_size=replay_size,\n",
        "            minibatch_size=minibatch_size,\n",
        "            target_update=target_update,\n",
        "            training_ep_per_eval=training_ep_per_eval,\n",
        "            warmup_target = warmup_target,\n",
        "            target_entropy_ratio=target_entropy_ratio)\n",
        "\n",
        "\n",
        "agent.train(max_episodes, lambda x : min(x) >= -150, criterion_episodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RdK_Y9x-ZlF"
      },
      "outputs": [],
      "source": [
        "# visualise one episode\n",
        "state, _ = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "steps = 0\n",
        "total_reward = 0\n",
        "while not (terminated or truncated or steps > max_steps):\n",
        "    # take action based on policy\n",
        "    with torch.no_grad():\n",
        "      action = agent.policy(state, stochastic=False)\n",
        "\n",
        "    # environment receives the action and returns:\n",
        "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    #print(f'Reward: {reward}')\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "print(f'Reward: {total_reward}')\n",
        "\n",
        "# store RGB frames for the entire episode\n",
        "frames = env.render()\n",
        "\n",
        "# close the environment\n",
        "env.close()\n",
        "\n",
        "# create and play video clip using the frames and given fps\n",
        "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
        "clip.ipython_display(rd_kwargs=dict(logger=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8ICB7WT-dAK"
      },
      "outputs": [],
      "source": [
        "# create the acrobot environment\n",
        "env = gym.make('ALE/Backgammon-v5', render_mode=\"rgb_array_list\")\n",
        "\n",
        "gamma = 0.99\n",
        "hidden_sizes = (1024, 1024, 1024)\n",
        "actor_learning_rate = 0.001\n",
        "critic_learning_rate = 0.005\n",
        "replay_size = 100000\n",
        "minibatch_size = 64\n",
        "target_update = 8000\n",
        "training_ep_per_eval = 5\n",
        "warmup_target = 20000\n",
        "target_entropy_ratio = 0.9\n",
        "optim_time = 4\n",
        "\n",
        "max_episodes = 3000\n",
        "max_steps = 18000\n",
        "criterion_episodes = 5\n",
        "\n",
        "agent = SAC(env,\n",
        "            gamma=gamma,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            actor_learning_rate=actor_learning_rate,\n",
        "            critic_learning_rate=critic_learning_rate,\n",
        "            replay_size=replay_size,\n",
        "            minibatch_size=minibatch_size,\n",
        "            target_update=target_update,\n",
        "            training_ep_per_eval=training_ep_per_eval,\n",
        "            warmup_target = warmup_target,\n",
        "            target_entropy_ratio=target_entropy_ratio)\n",
        "\n",
        "\n",
        "agent.train(max_episodes, lambda x : min(x) >= 1000, criterion_episodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHDLR8Ts-fYB"
      },
      "outputs": [],
      "source": [
        "# visualise one episode\n",
        "state, _ = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "steps = 0\n",
        "total_reward = 0\n",
        "while not (terminated or truncated or steps > max_steps):\n",
        "    # take action based on policy\n",
        "    with torch.no_grad():\n",
        "      action = agent.policy(state, stochastic=False)\n",
        "\n",
        "    # environment receives the action and returns:\n",
        "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    #print(f'Reward: {reward}')\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "print(f'Reward: {total_reward}')\n",
        "\n",
        "# store RGB frames for the entire episode\n",
        "frames = env.render()\n",
        "\n",
        "# close the environment\n",
        "env.close()\n",
        "\n",
        "# create and play video clip using the frames and given fps\n",
        "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
        "clip.ipython_display(rd_kwargs=dict(logger=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88RC6YKf-ica"
      },
      "outputs": [],
      "source": [
        "# create the acrobot environment\n",
        "env = gym.make('ALE/Backgammon-v5', render_mode=\"rgb_array_list\")\n",
        "\n",
        "gamma = 0.99\n",
        "hidden_sizes = (64, 64, 64)\n",
        "actor_learning_rate = 0.001\n",
        "critic_learning_rate = 0.005\n",
        "replay_size = 100000\n",
        "minibatch_size = 64\n",
        "target_update = 8000\n",
        "training_ep_per_eval = 5\n",
        "warmup_target = 20000\n",
        "target_entropy_ratio = 0.9\n",
        "optim_time = 4\n",
        "\n",
        "max_episodes = 3000\n",
        "max_steps = 18000\n",
        "criterion_episodes = 5\n",
        "\n",
        "agent = SAC(env,\n",
        "            gamma=gamma,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            actor_learning_rate=actor_learning_rate,\n",
        "            critic_learning_rate=critic_learning_rate,\n",
        "            replay_size=replay_size,\n",
        "            minibatch_size=minibatch_size,\n",
        "            target_update=target_update,\n",
        "            training_ep_per_eval=training_ep_per_eval,\n",
        "            warmup_target = warmup_target,\n",
        "            target_entropy_ratio=target_entropy_ratio)\n",
        "\n",
        "\n",
        "agent.train(max_episodes, lambda x : min(x) >= 1000, criterion_episodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLFg6rSP-lTT"
      },
      "outputs": [],
      "source": [
        "# visualise one episode\n",
        "state, _ = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "steps = 0\n",
        "total_reward = 0\n",
        "while not (terminated or truncated or steps > max_steps):\n",
        "    # take action based on policy\n",
        "    with torch.no_grad():\n",
        "      action = agent.policy(state, stochastic=False)\n",
        "\n",
        "    # environment receives the action and returns:\n",
        "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    #print(f'Reward: {reward}')\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "print(f'Reward: {total_reward}')\n",
        "\n",
        "# store RGB frames for the entire episode\n",
        "frames = env.render()\n",
        "\n",
        "# close the environment\n",
        "env.close()\n",
        "\n",
        "# create and play video clip using the frames and given fps\n",
        "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
        "clip.ipython_display(rd_kwargs=dict(logger=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbybqFQV-n5q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "res_dir = 'drive/MyDrive/Colab Notebooks/ReinforcementLearning/SACresults'\n",
        "res_path = os.path.join(os.getcwd(),res_dir)\n",
        "res_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ertpe6md-qJ6"
      },
      "outputs": [],
      "source": [
        "agent.save(f\"{res_path+'/backgammon.ram.128.sac.pt'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyPYet83FE7p"
      },
      "source": [
        "# source:\n",
        "*   Soft Actor-Critic Algorithms and Application (2018). https://arxiv.org/abs/1812.05905\n",
        "*   List item\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}