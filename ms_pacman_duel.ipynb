{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisy-dotcom/ms_pacman/blob/main/ms_pacman_duel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lgQ9gEJyLit",
        "outputId": "4d12813c-5fa8-46d0-9534-3e85c0055e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyBUgVBcyF5S",
        "outputId": "34629f4c-3ae2-46ed-ba0c-4346ecbdf547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [1 InRelease 0 B/3,626 B \u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Connected to r2u.stat.il\u001b[0m\r                                                                                                    \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,031 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,378 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,320 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,200 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,593 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,601 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,448 kB]\n",
            "Fetched 23.0 MB in 2s (10.1 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-utils x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 12 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 8,045 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.11 [28.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.11 [863 kB]\n",
            "Fetched 8,045 kB in 2s (3,740 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123621 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../01-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../02-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../03-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../04-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../05-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../06-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../10-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.11_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../11-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.11_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n",
            "Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.35.1)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (10.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.8.30)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (4.2.1)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Downloading mujoco-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2.35.1)\n",
            "Collecting ale-py>=0.9 (from gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (10.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (1.9.4)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text]) (3.20.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading mujoco-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m192.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m366.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m352.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376132 sha256=050acc97765bb0fa8482c73eb9834f2bca73ce683a49f039d10cca69b8fa8c3a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-02k76ouw/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: glfw, farama-notifications, box2d-py, gymnasium, ale-py, mujoco\n",
            "Successfully installed ale-py-0.10.1 box2d-py-2.3.5 farama-notifications-0.0.4 glfw-2.7.0 gymnasium-1.0.0 mujoco-3.2.3\n"
          ]
        }
      ],
      "source": [
        "# run this cell once per Colab session\n",
        "!apt update\n",
        "!apt-get install xvfb x11-utils\n",
        "!python -m pip install --upgrade swig\n",
        "!python -m pip install --upgrade pyvirtualdisplay moviepy\n",
        "!python -m pip install --upgrade gymnasium[accept-rom-license,atari,box2d,classic_control,mujoco,toy_text] --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQHfo6VU3SvR",
        "outputId": "e889f705-aa33-405a-b992-cebd77b99654"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x79ea28696830>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from collections import deque\n",
        "from pyvirtualdisplay import Display\n",
        "import moviepy.editor as mpy\n",
        "#from torchinfo import summary\n",
        "import os\n",
        "\n",
        "# create random number generator\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "# create and start virtual display\n",
        "display = Display(backend='xvfb')\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1dLeRcg5TGr"
      },
      "outputs": [],
      "source": [
        "import ale_py\n",
        "gym.register_envs(ale_py)\n",
        "#env = gym.make('ALE/MsPacman-ram-v5', render_mode=\"rgb_array_list\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtlJeZnz3Ozd",
        "outputId": "b883aa6a-a3a5-4372-ae2a-dff5097243ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  torch.set_default_device(torch.device(device))\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKov3eZ537jf",
        "outputId": "8114645d-7f03-4f08-a918-989729a8b350"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "torch.get_default_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umVnm2mnyAd6"
      },
      "outputs": [],
      "source": [
        "# source: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/rl/dqn/model.py\n",
        "class DuelingQNetworkRAM(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_sizes, output_size, learning_rate):\n",
        "        super().__init__()\n",
        "        # create network layers\n",
        "        layers = nn.ModuleList()\n",
        "\n",
        "        # input layers\n",
        "        layers.append(nn.Linear(in_channels, hidden_sizes[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "\n",
        "        # output layers\n",
        "        #  outputs a 1D tensor of size 1 with the state value\n",
        "        self.state_value =  nn.Linear(hidden_sizes[-1], 1)\n",
        "\n",
        "        # outputs a 1D tensor of size output_size with the action\n",
        "        # advantage values\n",
        "        self.adv = nn.Linear(hidden_sizes[-1], output_size)\n",
        "\n",
        "        # combine layers into feed-forward network\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "        # select loss function and optimizer\n",
        "        # note: original paper uses modified MSE loss and RMSprop\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "            {'params': self.net.parameters()},\n",
        "            {'params': self.state_value.parameters()},\n",
        "            {'params': self.adv.parameters()}],\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "        # initialise the weights according to dueling network architecture\n",
        "        self.net.apply(self.init_weights)\n",
        "        self.state_value.apply(self.init_weights)\n",
        "        self.adv.apply(self.init_weights)\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.net(x)\n",
        "\n",
        "        # state value output\n",
        "        state_value = self.state_value(x1)\n",
        "\n",
        "        # advantage output\n",
        "        adv = self.adv(x1)\n",
        "\n",
        "        # return output of Q-network for the input x\n",
        "        return state_value + adv - adv.mean(dim=-1, keepdim=True)\n",
        "\n",
        "    def update(self, inputs, targets):\n",
        "        # update network weights for a minibatch of inputs and targets:\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.forward(inputs)\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def copy_from(self, qnetwork):\n",
        "        # copy weights from another Q-network\n",
        "        self.net.load_state_dict(qnetwork.net.state_dict())\n",
        "        self.state_value.load_state_dict(qnetwork.state_value.state_dict())\n",
        "        self.adv.load_state_dict(qnetwork.adv.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oloTIYbVx7jL"
      },
      "outputs": [],
      "source": [
        "# Dueling Deep Q-network with Prioritised Experience Replay\n",
        "class DuelingDQNREPRAM():\n",
        "    def __init__(self, env, gamma,\n",
        "                 hidden_sizes=(32, 32),\n",
        "                 learning_rate=0.001,\n",
        "                 epsilon=0.1,\n",
        "                 min_epsilon=0.01,\n",
        "                 tau=0.1,\n",
        "                 rep_omega=0.2,\n",
        "                 replay_size=10000,\n",
        "                 minibatch_size=32,\n",
        "                 epsilon_update=50000,\n",
        "                 target_update=20):\n",
        "\n",
        "        # check if the state space has correct type\n",
        "        #continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
        "        #assert continuous, 'Observation space must be continuous with shape (n,)'\n",
        "        self.state_channels = env.observation_space.shape[0]\n",
        "        self.state_dims = env.observation_space.shape\n",
        "\n",
        "        # check if the action space has correct type\n",
        "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
        "        self.num_actions = env.action_space.n\n",
        "\n",
        "        # create dueling Q-networks for action-value function\n",
        "        self.qnet = DuelingQNetworkRAM(self.state_channels, hidden_sizes, self.num_actions, learning_rate)\n",
        "        self.target_qnet = DuelingQNetworkRAM(self.state_channels, hidden_sizes, self.num_actions, learning_rate)\n",
        "\n",
        "        # copy weights from Q-network to target Q-network\n",
        "        self.target_qnet.copy_from(self.qnet)\n",
        "\n",
        "        # initialise replay buffer\n",
        "        self.replay_buffer = deque(maxlen=replay_size)\n",
        "\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.init_epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.tau = tau\n",
        "        self.rep_omega = rep_omega\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.target_update = target_update\n",
        "        self.target_update_idx = 0\n",
        "        self.epsilon_update = epsilon_update\n",
        "        self.epsilon_update_idx = 0\n",
        "\n",
        "    def _linear_decay_epsilon_update(self):\n",
        "      epsilon = 1 - self.epsilon_update_idx / self.epsilon_update\n",
        "      epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n",
        "      epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n",
        "      self.epsilon_update_idx += 1\n",
        "      self.epsilon = epsilon\n",
        "\n",
        "    def behaviour(self, state):\n",
        "        # exploratory behaviour policy\n",
        "        if rng.uniform() >= self.epsilon:\n",
        "            # convert state to torch format\n",
        "            if not torch.is_tensor(state):\n",
        "                state = torch.tensor(np.array(state), dtype=torch.float)\n",
        "\n",
        "            # exploitation with probability 1-epsilon; break ties randomly\n",
        "            q = self.qnet(state).detach()\n",
        "            j = rng.permutation(self.num_actions)\n",
        "            return j[q[j].argmax().item()]\n",
        "        else:\n",
        "            # exploration with probability epsilon\n",
        "            return self.env.action_space.sample()\n",
        "\n",
        "        self._linear_decay_epsilon_update()\n",
        "\n",
        "    def policy(self, state):\n",
        "        # convert state to torch format\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor(np.array(state), dtype=torch.float)\n",
        "\n",
        "        # greedy policy\n",
        "        # q = self.qnet(state).detach()\n",
        "        q = self.qnet(state).detach()\n",
        "        return q.argmax().item()\n",
        "\n",
        "    def td_error(self, state, action, reward, next_state, terminated):\n",
        "        # calculate td error for prioritised experience replay\n",
        "        #next_action = self.qnet(next_state).detach().argmax()\n",
        "        #next_q = self.target_qnet(next_state).detach()\n",
        "\n",
        "        next_action = self.qnet(next_state).detach().argmax()\n",
        "        next_q = self.target_qnet(next_state).detach()\n",
        "\n",
        "        td_target = reward + self.gamma*next_q[next_action]\n",
        "        td_error = td_target - self.qnet(state)[action]\n",
        "        #td_error = td_target - self.qnet(state)[action]\n",
        "\n",
        "        return td_error.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        # update Q-network if there is enough experience\n",
        "        if len(self.replay_buffer) >= self.minibatch_size:\n",
        "\n",
        "            # select mini-batch of experiences using prioritised experience replay_buffer\n",
        "\n",
        "            # add a small constant to the td_error in the replay buffer\n",
        "            # to make sure samples with a TD error = 0 can be replayed\n",
        "            # so expereinces are sampled stochastically & not greedily\n",
        "            priorities = np.array([np.abs(sample[5] + 0.02) for idx, sample in enumerate(self.replay_buffer)])\n",
        "            scaled_priorities = priorities ** self.rep_omega\n",
        "            pri_sum = np.sum(scaled_priorities)\n",
        "            probs = scaled_priorities / pri_sum\n",
        "\n",
        "            #print(probs)\n",
        "\n",
        "            batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False, p=probs)\n",
        "            #, p=probs)\n",
        "\n",
        "            # calculate inputs and targets for the transitions in the mini-batch\n",
        "            inputs = torch.zeros((self.minibatch_size,)+self.state_dims)\n",
        "            targets = torch.zeros((self.minibatch_size, self.num_actions))\n",
        "\n",
        "            for n, index in enumerate(batch):\n",
        "                state, action, reward, next_state, terminated, _ = self.replay_buffer[index]\n",
        "                # inputs are states\n",
        "                inputs[n] = state\n",
        "\n",
        "                # targets are TD targets\n",
        "                #targets[n, :] = self.target_qnet(state).detach()\n",
        "                targets[n, :] = torch.squeeze(self.target_qnet(state), dim=0).detach()\n",
        "\n",
        "                if terminated:\n",
        "                    targets[n, action] = reward\n",
        "                else:\n",
        "                    # double learning\n",
        "                    # note: we don't break ties randomly (ties are unlikely when weights are initialised randomly)\n",
        "                    # next_action = self.qnet(next_state).detach().argmax()\n",
        "                    # next_q = self.target_qnet(next_state).detach()\n",
        "                    next_action = self.qnet(next_state).detach().argmax()\n",
        "                    next_q = self.target_qnet(next_state).detach()\n",
        "                    targets[n, action] = reward + self.gamma*next_q[next_action]\n",
        "\n",
        "            # train Q-network on the mini-batch\n",
        "            self.qnet.update(inputs, targets)\n",
        "\n",
        "        # periodically copy a portion of the weights from\n",
        "        # Q-network to target Q-network\n",
        "        self.target_update_idx += 1\n",
        "        if self.target_update_idx % self.target_update == 0:\n",
        "            self.update_networks()\n",
        "\n",
        "    def update_networks(self):\n",
        "      for target, online in zip(self.target_qnet.parameters(),\n",
        "                                self.qnet.parameters()):\n",
        "\n",
        "        target_ratio = (1.0-self.tau) * target.data\n",
        "        online_ratio = self.tau * online.data\n",
        "        mix = target_ratio + online_ratio\n",
        "        target.data.copy_(mix)\n",
        "\n",
        "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
        "        # train the agent for a number of episodes\n",
        "        rewards = []\n",
        "        num_steps = 0\n",
        "        for episode in range(max_episodes):\n",
        "            state, _ = env.reset()\n",
        "            # convert state to torch format\n",
        "            state = torch.tensor(np.array(state), dtype=torch.float)\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            rewards.append(0)\n",
        "            while not (terminated or truncated):\n",
        "                # select action by following behaviour policy\n",
        "                action = self.behaviour(state)\n",
        "\n",
        "                # send the action to the environment\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                # convert next state to torch format\n",
        "                next_state = torch.tensor(np.array(next_state), dtype=torch.float)\n",
        "\n",
        "                # calculate td error for prioritised experience replay and add experience to replay buffer\n",
        "                per = self.td_error(state, action, reward, next_state, terminated)\n",
        "                self.replay_buffer.append((state, action, reward, next_state, terminated, per))\n",
        "\n",
        "                # update Q-network\n",
        "                self.update()\n",
        "\n",
        "                state = next_state\n",
        "                rewards[-1] += reward\n",
        "                num_steps += 1\n",
        "\n",
        "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, rewards = {rewards[episode]}     ', end='')\n",
        "\n",
        "            if episode >= criterion_episodes-1 and stop_criterion(rewards[-criterion_episodes:]):\n",
        "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
        "                break\n",
        "\n",
        "        # plot rewards received during training\n",
        "        plt.figure(dpi=100)\n",
        "        plt.plot(range(1, len(rewards)+1), rewards, label=f'Rewards')\n",
        "\n",
        "        plt.xlabel('Episodes')\n",
        "        plt.ylabel('Rewards per episode')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "    def save(self, path):\n",
        "        # save network weights to a file\n",
        "        torch.save(self.qnet.state_dict(), path)\n",
        "        torch.save(self.target_qnet.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        # load network weights from a file\n",
        "        self.qnet.load_state_dict(torch.load(path))\n",
        "        self.target_qnet.copy_from(self.qnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehBvlwNxySZw",
        "outputId": "62b3a094-77f9-4d45-e918-641cdea14ad9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 32 done: steps = 16236, rewards = 120.0     "
          ]
        }
      ],
      "source": [
        "# create the environment\n",
        "# source: https://github.com/chengxi600/RLStuff/blob/master/Q%20Learning/Atari_DQN.ipynb\n",
        "env = gym.make('ALE/MsPacman-ram-v5', render_mode=\"rgb_array_list\")\n",
        "\n",
        "gamma = 0.99\n",
        "hidden_sizes = (64, 64, 64)\n",
        "learning_rate = 2.5e-4\n",
        "epsilon = 1.0\n",
        "min_epsilon = 0.01\n",
        "tau = 0.1\n",
        "rep_omega = 0.2\n",
        "replay_size = 50000\n",
        "minibatch_size = 32\n",
        "target_update = 1000\n",
        "epsilon_update= 1500000\n",
        "max_episodes = 700\n",
        "max_steps = 18000\n",
        "criterion_episodes = 5\n",
        "\n",
        "agent = DuelingDQNREPRAM(env,\n",
        "                gamma=gamma,\n",
        "                hidden_sizes=hidden_sizes,\n",
        "                learning_rate=learning_rate,\n",
        "                epsilon=epsilon,\n",
        "                min_epsilon=min_epsilon,\n",
        "                tau=tau,\n",
        "                rep_omega=rep_omega,\n",
        "                replay_size=replay_size,\n",
        "                minibatch_size=minibatch_size,\n",
        "                target_update=target_update,\n",
        "                epsilon_update=epsilon_update)\n",
        "\n",
        "\n",
        "agent.train(max_episodes, lambda x : min(x) >= 1000, criterion_episodes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmFjJnn_2KMl"
      },
      "outputs": [],
      "source": [
        "# visualise one episode\n",
        "state, _ = env.reset()\n",
        "terminated = False\n",
        "truncated = False\n",
        "steps = 0\n",
        "total_reward = 0\n",
        "while not (terminated or truncated or steps > max_steps):\n",
        "    # take action based on policy\n",
        "    with torch.no_grad():\n",
        "      action = agent.policy(state)\n",
        "\n",
        "    # environment receives the action and returns:\n",
        "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
        "    state, reward, terminated, truncated, info = env.step(action)\n",
        "    #print(f'Reward: {reward}')\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "print(f'Reward: {total_reward}')\n",
        "\n",
        "# store RGB frames for the entire episode\n",
        "frames = env.render()\n",
        "\n",
        "# close the environment\n",
        "env.close()\n",
        "\n",
        "# create and play video clip using the frames and given fps\n",
        "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
        "clip.ipython_display(rd_kwargs=dict(logger=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSxfarWV293a"
      },
      "outputs": [],
      "source": [
        "res_dir = 'drive/MyDrive/Colab Notebooks/Reinforcement Learning/results'\n",
        "res_path = os.path.join(os.getcwd(),res_dir)\n",
        "res_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SdeyzWAH2Vql"
      },
      "outputs": [],
      "source": [
        "agent.save(f\"{res_path+'/mspacman.ram.128.dueldqnrep.pt'}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyP6wp4sdY+ZZ7PHFCYkcoke",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}