{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ba5814-1df6-4bee-a36a-f4ee77692be0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Assignment RL \n",
    "Structure:\n",
    "1. [Environment explaination](#Environment-Explaination)\n",
    "***\n",
    "2. [Agent Choice](#Agent-Choice)\n",
    "***\n",
    "3. [Dependencies](#Dependencies)\n",
    "***\n",
    "4. [Implementations](#Implementations)\n",
    "   <div class=\"alert alert-block alert-danger\">\n",
    "       \n",
    "    1. <b>[Deep-Q-Network](#Deep-Q-Network)</b>\n",
    "        1. <b>[Deep-Q-Network](#Deep-Q-Network-Implementation)</b>\n",
    "            1. <b>[RAM Observations](#Deep-Q-Network-RAM-Observations)</b>\n",
    "            2. <b>[Image Observations](#Deep-Q-Network-Image-Observations)</b>\n",
    "            3. <b>[Hyper parameter sweep(RAM)](#Hyperparameter-Sweep-Deep-Q-Network-(RAM))</b>\n",
    "        2. <b>[Dueling Double DQN Impementation](#Dueling-Double-DQN-Impementation)</b>\n",
    "            1. <b>[RAM Observations](#Dueling-Double-DQN-RAM-Observations)</b>\n",
    "            2. <b>[Image Observations](#Dueling-Double-DQN-Image-Observations)</b>\n",
    "            3. <b>[Hyper parameter sweep(RAM)](#Hyperparameter-Sweep-Dueling-Double-DQN-(RAM))</b>\n",
    "   </div>\n",
    "    <div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "    2. <b>[Soft-Actor-Critic-(SAC).](#Soft-Actor-Critic-(SAC))</b>\n",
    "        1. <b>[Actor-Critic Impementation](#Actor-Critic-Impementation)</b>\n",
    "            1. <b>[Image Observations](#Actor-Critic-Image-Observations)</b>\n",
    "        2. <b>[Soft Actor-Critic (SAC) Impementation](#Soft-Actor-Critic-(SAC)-Impementation)</b>\n",
    "            1. <b>[RAM Observations](#Soft-Actor-Critic-(SAC)-RAM-Observations)</b>\n",
    "            2. <b>[Image Observations](#Soft-Actor-Critic-(SAC)-Image-Observations)</b>\n",
    "            3. <b>[Hyper parameter sweep(RAM)](#Hyperparameter-Sweep-Soft-Actor-Critic-(SAC)(RAM))</b>\n",
    "     </div>\n",
    "    <div class=\"alert alert-block alert-success\">\n",
    "        \n",
    "    3. <b>[Proximal Policy Optimization (PPO)](#Proximal-Policy-Optimization-(PPO))</b>\n",
    "        1. <b>[Proximal Policy Optimization (PPO) Impementation](#Proximal-Policy-Optimization-(PPO)-Impementation)</b>\n",
    "           1. <b>[RAM Observations](#Proximal-Policy-Optimization-(PPO)-RAM-Observations)</b>\n",
    "           2. <b>[Image Observations](#Proximal-Policy-Optimization-(PPO)-Image-Observations)</b>\n",
    "           3. <b>[Hyper parameter sweep(RAM)](#Hyperparameter-Sweep-PPO-(RAM))</b>\n",
    "    </div>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b74dd3-269d-4423-a71c-aae18763f085",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Environment Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2643182-8ef7-4e12-bd67-2bf9c6f98fa9",
   "metadata": {},
   "source": [
    "- Overview <br>\n",
    "In the Boxing environment, the agent's task is to engage in a boxing match in a ring, where it must land punches on the opponent to score points. A knockout occurs when the agent scores 100 points, ending the match in victory. The environment provides a relatively simple yet challenging reinforcement learning problem, where strategic planning and control must be executed to maximize the score.\n",
    "\n",
    "\n",
    "| Attribute           | Value                                             |\n",
    "|---------------------|---------------------------------------------------|\n",
    "| Action Space        | Discrete(18) - 18 possible actions                |\n",
    "| Observation Space   | Box(0, 255, (210, 160, 3), uint8)                 |\n",
    "| RGB Shape           | (210, 160, 3)                                     |\n",
    "| Grayscale Shape     | (210, 160)                                        |\n",
    "| RAM Shape           | (128,)                                            |\n",
    "| Reward              | Points for landing punches (100 points = Knockout)|\n",
    "| Frameskip           | (2, 5) or 4 (depending on variant)                |\n",
    "| Repeat Action Probability | 0.25 (default) or 0.0 (some variants)       |\n",
    "| Difficulty          | [0, 1, 2, 3] (default: 0)                         |\n",
    "| Mode                | [0] (default: 0)                                  |\n",
    " strategies, and gene\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b74aba-19b0-47fb-b30b-16449610acd1",
   "metadata": {},
   "source": [
    "\n",
    "- Action Space<br>\n",
    "The environment offers an action space of Discrete(18), meaning there are 18 possible actions that the agent can take. These actions cover various directions (e.g., moving up, down, left, or right) and combinations of actions with punching (e.g., UPFIRE, LEFTFIRE). The table below explains. Each action corresponds to a movement or punch combination, providing the agent with diverse strategies to interact with the environment.<br>\n",
    "<br>\n",
    "\n",
    "| Value | Meaning         | Description                                                |\n",
    "|-------|-----------------|------------------------------------------------------------|\n",
    "| 0     | NOOP            | No operation, do nothing                                   |\n",
    "| 1     | FIRE            | Press the fire button without updating the joystick position|\n",
    "| 2     | UP              | Apply a Δ-movement upwards on the joystick                  |\n",
    "| 3     | RIGHT           | Apply a Δ-movement rightward on the joystick                |\n",
    "| 4     | LEFT            | Apply a Δ-movement leftward on the joystick                 |\n",
    "| 5     | DOWN            | Apply a Δ-movement downward on the joystick                |\n",
    "| 6     | UPRIGHT         | Execute UP and RIGHT                                       |\n",
    "| 7     | UPLEFT          | Execute UP and LEFT                                        |\n",
    "| 8     | DOWNRIGHT       | Execute DOWN and RIGHT                                     |\n",
    "| 9     | DOWNLEFT        | Execute DOWN and LEFT                                      |\n",
    "| 10    | UPFIRE          | Execute UP and FIRE                                        |\n",
    "| 11    | RIGHTFIRE       | Execute RIGHT and FIRE                                     |\n",
    "| 12    | LEFTFIRE        | Execute LEFT and FIRE                                      |\n",
    "| 13    | DOWNFIRE        | Execute DOWN and FIRE                                      |\n",
    "| 14    | UPRIGHTFIRE     | Execute UP and RIGHT and FIRE                              |\n",
    "| 15    | UPLEFTFIRE      | Execute UP and LEFT and FIRE                               |\n",
    "| 16    | DOWNRIGHTFIRE   | Execute DOWN and RIGHT and FIRE                            |\n",
    "| 17    | DOWNLEFTFIRE    | Execute DOWN and LEFT and FIRE            \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33203c1-4089-4511-8bb0-7ff224a5e7d0",
   "metadata": {},
   "source": [
    "\n",
    "- Observation Space\n",
    "  <br>The observation space for the Boxing environment varies based on the configuration chosen (e.g., RGB, grayscale or RAM-based observations):\n",
    "        \n",
    "  - RGB observations:\n",
    "    <br>A color image of the boxing ring is provided with a dimension of 210 (height) x 160 (width) and 3 color channels, which is typical for Atari games. This provides rich visual information that the agent must interpret to learn meaningful behavior. <br>\n",
    "              ```obs_type=\"rgb\" -> observation_space=Box(0, 255, (210, 160, 3), np.uint8)```\n",
    "  \n",
    "  - Grayscale Obsercations:\n",
    "    <br>A grayscal image of the boxing ring is provided of dimenasion 210 height, 160 width and 1 channel rather than 3. This was used for all image experimaents as it provideds the required information withoughout the greater over head of 3 colour channels. <br>\n",
    "              ```obs_type=\"grayscale\" -> Box(0, 255, (210, 160), np.uint8)```\n",
    "  - RAM observations:\n",
    "    <br>An alternative hich contrary to the other spaces that reliy on the game screen for data, the ram observation space uses the console's memory. Atari 2600 has 1024 bits of random access memory that stores the internal state of the gane, including posisitons of game entities, timers and health conditions or score for boxing.  This RAM observation space os Marlovian capturing the full state at any given time without the need for prior context. We us the npint8 adition as well and thus 1024 bits becomes 128 ints (1024/8). There is also some other processing such as applying linear function approximations to capture the values of the the multibit groups. Aswell as adding the logical-AND of every possible bit pair to the feature vector (Bellemare et al., 2013). Allowing the function to capture combinations of values, as the profuct of two multibit groupings can be represented as the sum of their bitwise product   (Bellemare et al., 2013). <br>\n",
    "         ```obs_type=\"ram\" -> observation_space=Box(0, 255, (128,), np.uint8)```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae2f6c-99a8-4fe1-94c3-ed95b4df996f",
   "metadata": {},
   "source": [
    "- Rewards <br>\n",
    "The reward system in the Boxing environment is simple the agent receives points for each punch that successfully hits the opponent. A reward of 100 points triggers a knockout, and scoring higher overall leads to better performance. This aligns with the goal of maximizing cumulative rewards in reinforcement learning. If the Agent has scored a positive reward at the end of the run the agent has won.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d3a014-7121-4a1d-bc06-54861cdc094f",
   "metadata": {},
   "source": [
    "![image info](RLassignment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fdd699-f159-43d8-acfb-74a3636cf50f",
   "metadata": {
    "citation-manager": {
     "citations": {
      "gqmj4": [
       {
        "id": "18759246/FUI8Z6RK",
        "source": "zotero"
       }
      ],
      "zc629": [
       {
        "id": "18759246/KRBTYLUS",
        "source": "zotero"
       }
      ]
     }
    }
   },
   "source": [
    "<cite id=\"gqmj4\"><a href=\"#zotero%7C18759246%2FFUI8Z6RK\">(Machado et al., 2018)</a></cite>\n",
    "<cite id=\"zc629\"><a href=\"#zotero%7C18759246%2FKRBTYLUS\">(Bellemare et al., 2013)</a></cite>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec5266-a9c5-4b89-8c3c-14e4985dda30",
   "metadata": {},
   "source": [
    "- Environment Variants <br>\n",
    "    Several variants of the Boxing environment exist, offering differences in terms of observation type, frame-skipping, and repeataction probability: <br>\n",
    "    - Boxing-v0: Uses \"RGB\" observations with frame-skipping values between 2 and 5 and a 0.25 action repeat probability.\n",
    "    - Boxing-ram-v0: Uses RAM observations under the same conditions as Boxing-v0.\n",
    "    - BoxingDeterministic-v0: A deterministic version using RGB observations with no randomness in frame-skipping and action selection.\n",
    "    - BoxingNoFrameskip-v0: A variant where no frame-skipping is applied, making each action selection critical for performance.\n",
    " <br>\n",
    " \n",
    "***\n",
    "\n",
    " <br>\n",
    "\n",
    "- Difficulty and Modes <br>\n",
    "    Boxing includes adjustable difficulty settings and game modes to customize the agent's experience. The default difficulty is 0, and possible values for difficulty range from 0 to 3. Adjusting difficulty impacts the responsiveness and behavior of the opponent, making the environment more or less challenging depending on the choice.<br>\n",
    "    \n",
    " <br>\n",
    " \n",
    "***\n",
    "\n",
    " <br>\n",
    "\n",
    "- Research and Reinforcement Learning <br>\n",
    "        As explored in the Arcade Learning Environment by Bellemare et al., Boxing is one of the many Atari games used to benchmark AI agents, particularly those employing reinforcement learning techniques like SARSA(λ), Q-learning, or deep reinforcement learning models. The simplicity of the environment, combined with its discrete action space and challenging dynamics, makes it an excellent testbed for evaluating the general competency of agents. Researchers often use Boxing to analyze how well agents can learn to exploit the opponent's weaknesses and optimize long-term reward accumulation​(Bellemare et al., 2013).\n",
    "\n",
    "In conclusion, the Boxing environment presents a straightforward, yet non-trivial task for RL agents, with its rich action space and varying observation types. It offers a useful benchmark for researeneral game-playing AI.on reinforcement learning, action selection strategies, and gene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ad7f26-a53e-4fdf-92b7-3b9efe1295da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Agent Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb273728-e081-4725-a7a3-aed26a345c69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b25fe-c37d-4bc9-b4cd-9b1dd9f46959",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd956e-c62a-4810-9cbc-25fbeb7f0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38af1f-7398-4a81-a463-aadc0223219f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d429774-8d48-4abf-b54b-1b2269d602b8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-17T05:55:18.017538Z",
     "iopub.status.busy": "2024-10-17T05:55:18.017068Z",
     "iopub.status.idle": "2024-10-17T05:55:35.784840Z",
     "shell.execute_reply": "2024-10-17T05:55:35.784414Z",
     "shell.execute_reply.started": "2024-10-17T05:55:18.017511Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvirtualdisplay in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (3.0)\n",
      "Requirement already satisfied: opencv-python in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: imageio[ffmpeg] in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (2.33.1)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from imageio[ffmpeg]) (10.4.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from imageio[ffmpeg]) (0.5.1)\n",
      "Requirement already satisfied: psutil in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from imageio[ffmpeg]) (5.9.0)\n",
      "Requirement already satisfied: setuptools in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (69.5.1)\n",
      "Requirement already satisfied: wandb in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (0.18.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (5.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (2.16.0)\n",
      "Requirement already satisfied: setproctitle in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: docutils==0.17.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (0.17.1)\n",
      "Collecting jupyterlab-citation-manager\n",
      "  Downloading jupyterlab_citation_manager-1.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.0.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyterlab-citation-manager) (2.14.1)\n",
      "Collecting jupyterlab-citation-data (from jupyterlab-citation-manager)\n",
      "  Downloading jupyterlab_citation_data-0.1-py3-none-any.whl.metadata (908 bytes)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (7.10.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (5.9.2)\n",
      "Requirement already satisfied: overrides>=5.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (7.4.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (24.1)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.14.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.17.1)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (3.10.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (4.19.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (6.0.1)\n",
      "Requirement already satisfied: referencing in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.30.2)\n",
      "Requirement already satisfied: rfc3339-validator in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.16.2)\n",
      "Requirement already satisfied: ptyprocess in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.7.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.5.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (0.10.6)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=1.11 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading webcolors-24.8.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.5)\n",
      "Requirement already satisfied: pycparser in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager) (2.21)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.0.1->jupyterlab-citation-manager)\n",
      "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
      "Downloading jupyterlab_citation_manager-1.0.0-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.1/167.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_citation_data-0.1-py3-none-any.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading webcolors-24.8.0-py3-none-any.whl (15 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
      "Installing collected packages: webcolors, uri-template, types-python-dateutil, jupyterlab-citation-data, jsonpointer, fqdn, arrow, isoduration, jupyterlab-citation-manager\n",
      "Successfully installed arrow-1.3.0 fqdn-1.5.1 isoduration-20.11.0 jsonpointer-3.0.0 jupyterlab-citation-data-0.1 jupyterlab-citation-manager-1.0.0 types-python-dateutil-2.9.0.20241003 uri-template-1.3.0 webcolors-24.8.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtristancarlisle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvirtualdisplay\n",
    "!pip install opencv-python imageio[ffmpeg]\n",
    "!pip install wandb\n",
    "!pip install docutils==0.17.1\n",
    "!pip install jupyterlab-citation-manager\n",
    "!wandb login\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import io\n",
    "from IPython.display import display, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6128842e-a5a3-44ea-b611-204d779d61f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T12:04:40.885807Z",
     "iopub.status.busy": "2024-10-17T12:04:40.885525Z",
     "iopub.status.idle": "2024-10-17T12:04:43.157861Z",
     "shell.execute_reply": "2024-10-17T12:04:43.157558Z",
     "shell.execute_reply.started": "2024-10-17T12:04:40.885796Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "os.environ['XDG_RUNTIME_DIR'] = '/tmp/runtime-tristan'\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "import ast\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import random\n",
    "import ast\n",
    "import math\n",
    "import imageio\n",
    "import cv2\n",
    "import math\n",
    "from gymnasium.wrappers import ResizeObservation\n",
    "from gymnasium.wrappers import NormalizeObservation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import random\n",
    "import ast  # For safely evaluating tuple strings\n",
    "\n",
    "# create random number generator\n",
    "rng = np.random.default_rng()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3f1df-85a4-4843-aea6-034a3f1bc168",
   "metadata": {},
   "outputs": [],
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab0bf1ce-2047-4205-886d-b636246c17e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T11:15:09.211847Z",
     "iopub.status.busy": "2024-10-16T11:15:09.211497Z",
     "iopub.status.idle": "2024-10-16T11:15:09.214502Z",
     "shell.execute_reply": "2024-10-16T11:15:09.214175Z",
     "shell.execute_reply.started": "2024-10-16T11:15:09.211833Z"
    }
   },
   "outputs": [],
   "source": [
    "class VideoRecorderRAM:\n",
    "    def __init__(self, dir_name, fps=30):\n",
    "        self.dir_name = dir_name\n",
    "        self.fps = fps\n",
    "        self.frames = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.frames = []\n",
    "\n",
    "    def record(self, frame):\n",
    "        self.frames.append(frame)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        path = os.path.join(self.dir_name, file_name)\n",
    "        imageio.mimsave(path, self.frames, fps=self.fps, macro_block_size = None)\n",
    "\n",
    "#v= VideoRecorderRAM('DQN')\n",
    "#v.frames=frames\n",
    "#v.save(\"prac_DQN.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39020e-0d60-40ff-a2f6-04040dc720e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoRecorder:\n",
    "    def __init__(self, dir_name, fps=30):\n",
    "        self.dir_name = dir_name\n",
    "        self.fps = fps\n",
    "        self.frames = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.frames = []\n",
    "\n",
    "    def record(self, frame):\n",
    "        self.frames.append(frame)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        path = os.path.join(self.dir_name, file_name)\n",
    "        frames_np = [np.array(frame) for frame in self.frames]\n",
    "        imageio.mimsave(path, frames_np, fps=self.fps, macro_block_size = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91526f19-8ad7-43a8-8969-c4cfc3229391",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-16T08:03:39.966906Z",
     "iopub.status.busy": "2024-10-16T08:03:39.966783Z",
     "iopub.status.idle": "2024-10-16T08:03:39.968796Z",
     "shell.execute_reply": "2024-10-16T08:03:39.968539Z",
     "shell.execute_reply.started": "2024-10-16T08:03:39.966887Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d746a58-6d5f-4aa1-8b79-5e53c770ddc5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6211d06-f433-4a1c-bb25-ce922da97edb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Deep-Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df6ed5-eb64-4f42-9eac-1f00dc2de042",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Deep-Q-Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331d980-561b-4b66-ae13-7420e4ed63ff",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-14T02:11:04.215055Z",
     "iopub.status.busy": "2024-10-14T02:11:04.214848Z",
     "iopub.status.idle": "2024-10-14T02:11:04.218209Z",
     "shell.execute_reply": "2024-10-14T02:11:04.217821Z",
     "shell.execute_reply.started": "2024-10-14T02:11:04.215036Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Practical DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825841e-69a0-4d80-94a9-06c2a7438736",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-14T02:52:25.423681Z",
     "iopub.status.busy": "2024-10-14T02:52:25.423203Z",
     "iopub.status.idle": "2024-10-14T02:52:25.425641Z",
     "shell.execute_reply": "2024-10-14T02:52:25.425319Z",
     "shell.execute_reply.started": "2024-10-14T02:52:25.423665Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Setup weights and bias to compare the original implementation seen in the practical and other implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b406aa3a-f6b0-4333-86fd-fca0724dc43e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T05:46:28.119868Z",
     "iopub.status.busy": "2024-10-16T05:46:28.119444Z",
     "iopub.status.idle": "2024-10-16T05:46:28.130818Z",
     "shell.execute_reply": "2024-10-16T05:46:28.130418Z",
     "shell.execute_reply.started": "2024-10-16T05:46:28.119852Z"
    }
   },
   "source": [
    "QNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e3958b-e479-4014-8f75-f67e9ac76ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T08:10:25.048522Z",
     "iopub.status.busy": "2024-10-16T08:10:25.048172Z",
     "iopub.status.idle": "2024-10-16T08:10:25.052344Z",
     "shell.execute_reply": "2024-10-16T08:10:25.052040Z",
     "shell.execute_reply.started": "2024-10-16T08:10:25.048508Z"
    }
   },
   "outputs": [],
   "source": [
    "class PracQNetwork(nn.Module): #changed name to PracQNetwork for logging \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        # create network layers\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # combine layers into feed-forward network\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # select loss function and optimizer\n",
    "        # note: original paper uses modified MSE loss and RMSprop\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return output of Q-network for the input x\n",
    "        return self.net(x)\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        # update network weights for a minibatch of inputs and targets:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.net(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def copy_from(self, qnetwork):\n",
    "        # copy weights from another Q-network\n",
    "        self.net.load_state_dict(qnetwork.net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e86b03-3c41-449f-b5ef-a7d770c902fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52f38bf3-f72d-4130-aa27-f58ba5b68878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T10:31:30.685524Z",
     "iopub.status.busy": "2024-10-16T10:31:30.685282Z",
     "iopub.status.idle": "2024-10-16T10:31:35.003957Z",
     "shell.execute_reply": "2024-10-16T10:31:35.003617Z",
     "shell.execute_reply.started": "2024-10-16T10:31:30.685510Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:17sp1bec) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_num</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▅█▃▇▇▇▇▁▇▃▆▂▇▇▇▃█▂▅▅▇█▇▅▂▅▅▇▅▇▃▇▂█▇▆▇▇▆▇</td></tr><tr><td>epsilon</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>local reward</td><td>████████████████▁▁██████████████████████</td></tr><tr><td>loss</td><td>▄█▅▃▅▅▅▆▇▄▄▂▄▃▄▂▂▆▅▂▄▄▃▄▆▅▅▃▁▅▂▂▁▃▁▇▃▅▂▂</td></tr><tr><td>steps</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>total steps</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>57</td></tr><tr><td>episode_num</td><td>58</td></tr><tr><td>episode_reward</td><td>-4</td></tr><tr><td>epsilon</td><td>0.2</td></tr><tr><td>local reward</td><td>0</td></tr><tr><td>loss</td><td>7.77793</td></tr><tr><td>steps</td><td>105145</td></tr><tr><td>total steps</td><td>103588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-morning-19</strong> at: <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/17sp1bec' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/17sp1bec</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_180611-17sp1bec/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:17sp1bec). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241016_183130-kqjpahr9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/kqjpahr9' target=\"_blank\">cosmic-serenity-20</a></strong> to <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/kqjpahr9' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/kqjpahr9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"DD=wandb.init(project='DQN-Original-comparison-Final', config={\\n    'gamma': gamma,\\n    'learning_rate': learning_rate,\\n    'epsilon': epsilon,\\n    'replay_size': replay_size,\\n    'minibatch_size': minibatch_size,\\n    'target_update_freq': target_update,\\n    'num_episodes': max_episodes,\\n    'max_steps_per_episode': max_steps,\\n    'criterion_episodes':criterion_episodes\\n})\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "print(len(env.observation_space.high))\n",
    "print(env.observation_space.low)\n",
    "\n",
    "gamma = 0.99\n",
    "hidden_sizes = (128, 128)\n",
    "learning_rate = 0.001\n",
    "epsilon = 0.2\n",
    "replay_size = 10000\n",
    "minibatch_size = 64\n",
    "target_update = 20\n",
    "max_episodes = 100\n",
    "max_steps = 1000\n",
    "criterion_episodes = 5\n",
    "\n",
    "#settings=wandb.Settings(start_method=\"fork\")\n",
    "\n",
    "wandb.init(project='DQN-Original-comparison-Final', config={\n",
    "    'gamma': gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'replay_size': replay_size,\n",
    "    'minibatch_size': minibatch_size,\n",
    "    'target_update_freq': target_update,\n",
    "    'num_episodes': max_episodes,\n",
    "    'max_steps_per_episode': max_steps,\n",
    "    'criterion_episodes':criterion_episodes\n",
    "})\n",
    "\n",
    "'''DD=wandb.init(project='DQN-Original-comparison-Final', config={\n",
    "    'gamma': gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'replay_size': replay_size,\n",
    "    'minibatch_size': minibatch_size,\n",
    "    'target_update_freq': target_update,\n",
    "    'num_episodes': max_episodes,\n",
    "    'max_steps_per_episode': max_steps,\n",
    "    'criterion_episodes':criterion_episodes\n",
    "})'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c731031-5ee3-40fe-928a-9b7be13794fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T06:06:44.793085Z",
     "iopub.status.busy": "2024-10-16T06:06:44.792729Z",
     "iopub.status.idle": "2024-10-16T06:06:44.814028Z",
     "shell.execute_reply": "2024-10-16T06:06:44.813678Z",
     "shell.execute_reply.started": "2024-10-16T06:06:44.793072Z"
    }
   },
   "source": [
    "AgentClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf8fe8d4-5456-4886-8abd-4ac2280a1a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T10:32:43.623538Z",
     "iopub.status.busy": "2024-10-16T10:32:43.623301Z",
     "iopub.status.idle": "2024-10-16T11:14:13.130182Z",
     "shell.execute_reply": "2024-10-16T11:14:13.129908Z",
     "shell.execute_reply.started": "2024-10-16T10:32:43.623524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/kqjpahr9?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x73824e186f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99 done: steps = 176814, rewards = -21.0     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_1537464/255753251.py:155: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 done: steps = 178600, rewards = -26.0     "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VideoRecorderRAM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 210\u001b[0m\n\u001b[1;32m    208\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(max_episodes, \u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;28mmin\u001b[39m(x) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, criterion_episodes)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     agent\u001b[38;5;241m.\u001b[39mevaluate(Run\u001b[38;5;241m=\u001b[39mx)\n\u001b[1;32m    211\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[54], line 173\u001b[0m, in \u001b[0;36mAgentDQN.evaluate\u001b[0;34m(self, Run)\u001b[0m\n\u001b[1;32m    171\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \n\u001b[1;32m    172\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender() \u001b[38;5;66;03m# Render environment frame and store for video\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m v\u001b[38;5;241m=\u001b[39m VideoRecorderRAM(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDQN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    174\u001b[0m v\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m=\u001b[39mframe\n\u001b[1;32m    175\u001b[0m vfilename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDQN_Prac_RAM_run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VideoRecorderRAM' is not defined"
     ]
    }
   ],
   "source": [
    "%%wandb #displays wand b plotting\n",
    "class AgentDQN():\n",
    "    def __init__(self, env, gamma,\n",
    "                 hidden_sizes=(32, 32),\n",
    "                 learning_rate=0.001,\n",
    "                 epsilon=0.1,\n",
    "                 replay_size=10000,\n",
    "                 minibatch_size=32,\n",
    "                 target_update=20\n",
    "                 ):\n",
    "        # check if the state space has correct type\n",
    "        continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "\n",
    "        # check if the action space has correct type\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        # create Q-networks for action-value function\n",
    "        self.qnet = PracQNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
    "        self.target_qnet = PracQNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
    "\n",
    "        # copy weights from Q-network to target Q-network\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "        # initialise replay buffer\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.target_update = target_update\n",
    "        self.target_update_idx = 0\n",
    "\n",
    "    def behaviour(self, state):\n",
    "        # exploratory behaviour policy\n",
    "        if rng.uniform() >= self.epsilon:\n",
    "            # convert state to torch format\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "            # exploitation with probability 1-epsilon; break ties randomly\n",
    "            q = self.qnet(state).detach()\n",
    "            j = rng.permutation(self.num_actions)\n",
    "            return j[q[j].argmax().item()]\n",
    "        else:\n",
    "            # exploration with probability epsilon\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "    def policy(self, state):\n",
    "        # convert state to torch format\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "        # greedy policy\n",
    "        q = self.qnet(state).detach()\n",
    "        return q.argmax().item()\n",
    "\n",
    "    def update(self):\n",
    "        # update Q-network if there is enough experience\n",
    "        if len(self.replay_buffer) >= self.minibatch_size:\n",
    "            # select mini-batch of experiences uniformly at random without replacement                                \n",
    "            batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False)\n",
    "\n",
    "            # calculate inputs and targets for the transitions in the mini-batch\n",
    "            inputs = torch.zeros((self.minibatch_size, self.state_dims))\n",
    "            targets = torch.zeros((self.minibatch_size, self.num_actions))\n",
    "\n",
    "            for n, index in enumerate(batch):\n",
    "                state, action, reward, next_state, terminated = self.replay_buffer[index]\n",
    "                # inputs are states\n",
    "                inputs[n, :] = state\n",
    "\n",
    "                # targets are TD targets\n",
    "                targets[n, :] = self.target_qnet(state).detach()\n",
    "\n",
    "                if terminated:\n",
    "                    targets[n, action] = reward\n",
    "                else:\n",
    "                    targets[n, action] = reward + self.gamma*self.target_qnet(next_state).detach().max()\n",
    "            \n",
    "            # train Q-network on the mini-batch\n",
    "            logging_loss=self.qnet.update(inputs, targets)\n",
    "            #print(logging_loss)\n",
    "            return logging_loss\n",
    "        # periodically copy weights from Q-network to target Q-network\n",
    "        self.target_update_idx += 1\n",
    "        if self.target_update_idx % self.target_update == 0:\n",
    "            self.target_qnet.copy_from(self.qnet)\n",
    "        return None\n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # train the agent for a number of episodes\n",
    "        rewards = []\n",
    "        num_steps = 0\n",
    "        \n",
    "        for episode in range(max_episodes):\n",
    "            steps=0\n",
    "            state, _ = env.reset()\n",
    "            # convert state to torch format\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            rewards.append(0)\n",
    "            while not (terminated or truncated):\n",
    "                # select action by following behaviour policy\n",
    "                action = self.behaviour(state)\n",
    "\n",
    "                # send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # convert next state to torch format and add experience to replay buffer\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "                self.replay_buffer.append((state, action, reward, next_state, terminated))\n",
    "\n",
    "                # update Q-network\n",
    "                logging_loss=self.update() #and log loss\n",
    "                if logging_loss is not None:\n",
    "                    wandb.log({\n",
    "                    'episode_num': episode,\n",
    "                    'loss':logging_loss,\n",
    "                    'steps': steps,\n",
    "                    'local reward':reward,\n",
    "\n",
    "                })\n",
    "\n",
    "                state = next_state\n",
    "                rewards[-1] += reward\n",
    "                num_steps += 1\n",
    "                steps += 1\n",
    "                \n",
    "\n",
    "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, rewards = {rewards[episode]}     ', end='')\n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                    'episode': episode,\n",
    "                    'total steps':num_steps,\n",
    "                    'episode_reward': rewards[episode],\n",
    "                    'epsilon': self.epsilon,\n",
    "                })\n",
    "\n",
    "            if episode >= criterion_episodes-1 and stop_criterion(rewards[-criterion_episodes:]):\n",
    "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
    "                break\n",
    "\n",
    "        \n",
    "        # plot rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(rewards)+1), rewards, label=f'Rewards')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards per episode')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    # changed evaluationto a function just to help break it up, also added logging and changed video script as mpy doesnt work for me for some reason\n",
    "    def evaluate(self, Run):\n",
    "        state, _ = self.env.reset()\n",
    "        state = state.flatten()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        steps=0\n",
    "        frames = []\n",
    "\n",
    "        while not (terminated or truncated): #removed max steps, boxing is timed so it will end after 2 minutes of the game we want to see how the agent goes over the full game\n",
    "\n",
    "            action = self.policy(state)\n",
    "            state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            steps +=1 \n",
    "        frame = self.env.render() # Render environment frame and store for video\n",
    "        v= VideoRecorderRAM('DQN')\n",
    "        v.frames=frame\n",
    "        vfilename = f'DQN_Prac_RAM_run_{Run}.mp4'\n",
    "        v.save(vfilename)\n",
    "        video_path=\"DQN/\" + vfilename\n",
    "        # Save video of the evaluation episode\n",
    "        #video = np.stack(frames)\n",
    "        #video_path = f\"evaluation_episode_{episode_num}.mp4\"\n",
    "        #mpy_clip = mpy.ImageSequenceClip(list(video), fps=30)\n",
    "        #mpy_clip.write_videofile(video_path, codec=\"libx264\")\n",
    "\n",
    "        # Log evaluation results and video to W&B\n",
    "        wandb.log({\n",
    "            'Evaluation Run': Run,\n",
    "            'Evaluation Reward': total_reward,\n",
    "            'Evaluation Video': wandb.Video(video_path, fps=30, format=\"mp4\")\n",
    "        })\n",
    "\n",
    "        # Logging evaluation result\n",
    "        print(f\"Evaluation run: {Run} Total Reward = {total_reward}\")\n",
    "        env.close()\n",
    "\n",
    "    def save(self, path):\n",
    "        # save network weights to a file\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        # load network weights from a file\n",
    "        self.qnet.load_state_dict(torch.load(path))\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "\n",
    "agent = AgentDQN(env,gamma=gamma, hidden_sizes=hidden_sizes,learning_rate=learning_rate,epsilon=epsilon,replay_size=replay_size,minibatch_size=minibatch_size,)\n",
    "\n",
    "#agent.load('acrobot.128x128.DQN.pt')\n",
    "agent.train(max_episodes, lambda x : min(x) >= 100, criterion_episodes)\n",
    "for x in range(5):\n",
    "    agent.evaluate(Run=x)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f8c7ad2-63a0-4974-909d-f69452c40b05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T11:15:29.716073Z",
     "iopub.status.busy": "2024-10-16T11:15:29.715649Z",
     "iopub.status.idle": "2024-10-16T11:15:41.153077Z",
     "shell.execute_reply": "2024-10-16T11:15:41.152696Z",
     "shell.execute_reply.started": "2024-10-16T11:15:29.716052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation run: 0 Total Reward = -49.0\n",
      "Evaluation run: 1 Total Reward = -54.0\n",
      "Evaluation run: 2 Total Reward = -54.0\n",
      "Evaluation run: 3 Total Reward = -54.0\n",
      "Evaluation run: 4 Total Reward = -47.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.120 MB of 1.120 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Evaluation Reward</td><td>▆▁▁▁█</td></tr><tr><td>Evaluation Run</td><td>▁▃▅▆█</td></tr><tr><td>episode</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>episode_num</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>episode_reward</td><td>▅▂▆▂▅▆▃▄▄▅▅█▄▅▆█▇█▂▂▁▄▆▅▃▆▃▄▅▇▇█▄▃▅▄▇▇▅▄</td></tr><tr><td>epsilon</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>local reward</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▁▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>loss</td><td>▆▅▃▅▃▁▅▂▅▅▃▁▇▂▃▄▂▄▄▃▃▁▄▄▄▁▇▁▃▆▆▇▅▄▂▁▃▅▃█</td></tr><tr><td>steps</td><td>▂▇▃▃▂▇▂▂▄▅▁▃▆▂██▂▆▃▃▄▃▄▂▆▅▁▃▁▃▅▄▄▆▁▇▅█▄▂</td></tr><tr><td>total steps</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Evaluation Reward</td><td>-47</td></tr><tr><td>Evaluation Run</td><td>4</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_num</td><td>99</td></tr><tr><td>episode_reward</td><td>-26</td></tr><tr><td>epsilon</td><td>0.2</td></tr><tr><td>local reward</td><td>0</td></tr><tr><td>loss</td><td>1.36791</td></tr><tr><td>steps</td><td>1785</td></tr><tr><td>total steps</td><td>178600</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-serenity-20</strong> at: <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/kqjpahr9' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/kqjpahr9</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_183130-kqjpahr9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in range(5):\n",
    "    agent.evaluate(Run=x)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbbce6de-7425-440c-af59-315ffbf1247a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T11:18:50.953996Z",
     "iopub.status.busy": "2024-10-16T11:18:50.953681Z",
     "iopub.status.idle": "2024-10-16T11:18:50.956661Z",
     "shell.execute_reply": "2024-10-16T11:18:50.956395Z",
     "shell.execute_reply.started": "2024-10-16T11:18:50.953980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function wandb.sdk.wandb_run.finish(exit_code: Optional[int] = None, quiet: Optional[bool] = None) -> None>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640a652-38bb-4b86-90db-d55aa479d09b",
   "metadata": {},
   "source": [
    "Make environment and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df785ba8-a341-4f47-9741-b1806826c3dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T03:59:22.197138Z",
     "iopub.status.busy": "2024-10-14T03:59:22.196795Z",
     "iopub.status.idle": "2024-10-14T03:59:22.199798Z",
     "shell.execute_reply": "2024-10-14T03:59:22.199496Z",
     "shell.execute_reply.started": "2024-10-14T03:59:22.197124Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save('Boxing.DQN.prac.agent1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d85df3-0fab-4bf6-b9f0-f1a5e9d65bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d4d88d-9099-46a1-a01a-646438b23f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T13:11:54.430331Z",
     "iopub.status.busy": "2024-10-14T13:11:54.429964Z",
     "iopub.status.idle": "2024-10-14T13:11:54.433120Z",
     "shell.execute_reply": "2024-10-14T13:11:54.432837Z",
     "shell.execute_reply.started": "2024-10-14T13:11:54.430315Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "#%%writefile SAC_spedup_and_vectorised.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "998a6ee5-c5c1-44da-bef6-dbd7d0542694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T13:12:03.129476Z",
     "iopub.status.busy": "2024-10-14T13:12:03.129203Z",
     "iopub.status.idle": "2024-10-14T13:12:03.132811Z",
     "shell.execute_reply": "2024-10-14T13:12:03.132500Z",
     "shell.execute_reply.started": "2024-10-14T13:12:03.129461Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7675b-777c-4d97-ac4e-5ee21c1f3ac6",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Deep-Q-Network RAM Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb86dbd-9607-48cd-a3e6-ef4ff1708d73",
   "metadata": {},
   "source": [
    "The original implementation didn seen to be improving throughout the training run. After a lot of testing it believ it was due to the loss function logging resulting in gradients not being updated correctly. Thus the loss logging was removed for V2. The hidden size was also increased and the learning rate was reduced. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28a26755-ed55-418e-8ce7-ba8cf01d5300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T12:33:22.155153Z",
     "iopub.status.busy": "2024-10-16T12:33:22.154883Z",
     "iopub.status.idle": "2024-10-16T12:33:22.160207Z",
     "shell.execute_reply": "2024-10-16T12:33:22.159909Z",
     "shell.execute_reply.started": "2024-10-16T12:33:22.155136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # training on GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        # create network layers\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # combine layers into feed-forward network\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # select loss function and optimizer\n",
    "        # note: original paper uses modified MSE loss and RMSprop\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return output of Q-network for the input x\n",
    "        return self.net(x)\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        # update network weights for a minibatch of inputs and targets:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.net(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        #return loss Removed returning the loss function for logging \n",
    "\n",
    "    def copy_from(self, qnetwork):\n",
    "        # copy weights from another Q-network\n",
    "        self.net.load_state_dict(qnetwork.net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1765a0c-f55c-4c6a-a56b-f504d41d2124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T12:33:23.035514Z",
     "iopub.status.busy": "2024-10-16T12:33:23.035341Z",
     "iopub.status.idle": "2024-10-16T12:33:23.662345Z",
     "shell.execute_reply": "2024-10-16T12:33:23.662064Z",
     "shell.execute_reply.started": "2024-10-16T12:33:23.035500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241016_203323-rh8p31zj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj' target=\"_blank\">DQN_V2_2</a></strong> to <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73824e401d50>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "print(len(env.observation_space.high))\n",
    "print(env.observation_space.low)\n",
    "\n",
    "gamma = 0.99\n",
    "hidden_sizes = (128, 128, 128)\n",
    "learning_rate = 0.0001\n",
    "epsilon = 0.2\n",
    "replay_size = 10000\n",
    "minibatch_size = 64\n",
    "target_update = 20\n",
    "max_episodes = 100\n",
    "max_steps = 1000\n",
    "criterion_episodes = 5\n",
    "\n",
    "wandb.init(project='DQN-Original-comparison-Final', name='DQN_V2_2', config={\n",
    "    'gamma': gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'replay_size': replay_size,\n",
    "    'minibatch_size': minibatch_size,\n",
    "    'target_update_freq': target_update,\n",
    "    'num_episodes': max_episodes,\n",
    "    'max_steps_per_episode': max_steps,\n",
    "    'criterion_episodes':criterion_episodes\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd62b4cc-c4cf-4deb-b32e-d4633ba8b12e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-16T12:33:24.523162Z",
     "iopub.status.busy": "2024-10-16T12:33:24.522995Z",
     "iopub.status.idle": "2024-10-16T13:25:05.862858Z",
     "shell.execute_reply": "2024-10-16T13:25:05.862492Z",
     "shell.execute_reply.started": "2024-10-16T12:33:24.523149Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x73824e5c1150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 done: steps = 178600, rewards = -21.0     Evaluation run: 0 Total Reward = 2.0\n",
      "Evaluation run: 1 Total Reward = 14.0\n",
      "Evaluation run: 2 Total Reward = 3.0\n",
      "Evaluation run: 3 Total Reward = 2.0\n",
      "Evaluation run: 4 Total Reward = 4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.208 MB of 1.208 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Evaluation Reward</td><td>▁█▂▁▂</td></tr><tr><td>Evaluation Run</td><td>▁▃▅▆█</td></tr><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>episode_reward</td><td>▁▃▅▅▃▇▅▄▅▇▆▃▅▃▅▁▅▅▄▄▄▃▄▅▅▃▄▄▅▅▅▄▅▅▄▆▄▅▅█</td></tr><tr><td>epsilon</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total steps</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Evaluation Reward</td><td>4</td></tr><tr><td>Evaluation Run</td><td>4</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-21</td></tr><tr><td>epsilon</td><td>0.2</td></tr><tr><td>total steps</td><td>178600</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_V2_2</strong> at: <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/runs/rh8p31zj</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_203323-rh8p31zj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%wandb\n",
    "###########################################################################################################################################################################################################\n",
    "class V2AgentDQN():\n",
    "    def __init__(self, env, gamma,\n",
    "                 hidden_sizes=(128, 128,128), #Increased Depth and size\n",
    "                 learning_rate=0.0001, #decrease learning rate\n",
    "                 epsilon=0.2, #increased epsilon\n",
    "                 replay_size=10000,\n",
    "                 minibatch_size=64, #increase batch size\n",
    "                 target_update=20):\n",
    "        # check if the state space has correct type\n",
    "        continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "\n",
    "        # check if the action space has correct type\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        # create Q-networks for action-value function\n",
    "        self.qnet = QNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
    "        self.target_qnet = QNetwork(self.state_dims, hidden_sizes, self.num_actions, learning_rate)\n",
    "\n",
    "        # copy weights from Q-network to target Q-network\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "        # initialise replay buffer\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.target_update = target_update\n",
    "        self.target_update_idx = 0\n",
    "###########################################################################################################################################################################################################\n",
    "    def behaviour(self, state):\n",
    "        # exploratory behaviour policy\n",
    "        if rng.uniform() >= self.epsilon:\n",
    "            # convert state to torch format\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "            # exploitation with probability 1-epsilon; break ties randomly\n",
    "            q = self.qnet(state).detach()\n",
    "            j = rng.permutation(self.num_actions)\n",
    "            return j[q[j].argmax().item()]\n",
    "        else:\n",
    "            # exploration with probability epsilon\n",
    "            return self.env.action_space.sample()\n",
    "###########################################################################################################################################################################################################\n",
    "    def policy(self, state):\n",
    "        # convert state to torch format\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "        # greedy policy\n",
    "        q = self.qnet(state).detach()\n",
    "        return q.argmax().item()\n",
    "###########################################################################################################################################################################################################\n",
    "    def update(self):\n",
    "        # update Q-network if there is enough experience\n",
    "        if len(self.replay_buffer) >= self.minibatch_size:\n",
    "            # select mini-batch of experiences uniformly at random without replacement                                \n",
    "            batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False)\n",
    "\n",
    "            # calculate inputs and targets for the transitions in the mini-batch\n",
    "            inputs = torch.zeros((self.minibatch_size, self.state_dims))\n",
    "            targets = torch.zeros((self.minibatch_size, self.num_actions))\n",
    "\n",
    "            for n, index in enumerate(batch):\n",
    "                state, action, reward, next_state, terminated = self.replay_buffer[index]\n",
    "                # inputs are states\n",
    "                inputs[n, :] = state\n",
    "\n",
    "                # targets are TD targets\n",
    "                targets[n, :] = self.target_qnet(state).detach()\n",
    "\n",
    "                if terminated:\n",
    "                    targets[n, action] = reward\n",
    "                else:\n",
    "                    targets[n, action] = reward + self.gamma*self.target_qnet(next_state).detach().max()\n",
    "            \n",
    "                        # train Q-network on the mini-batch\n",
    "            self.qnet.update(inputs, targets)\n",
    "            #logging_loss=\n",
    "            #print(logging_loss)\n",
    "            #return logging_loss\n",
    "\n",
    "        # periodically copy weights from Q-network to target Q-network\n",
    "        self.target_update_idx += 1\n",
    "        if self.target_update_idx % self.target_update == 0:\n",
    "            self.target_qnet.copy_from(self.qnet)\n",
    "        #return None\n",
    "###########################################################################################################################################################################################################    \n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "                # train the agent for a number of episodes\n",
    "        rewards = []\n",
    "        num_steps = 0\n",
    "        \n",
    "        for episode in range(max_episodes):\n",
    "            steps=0\n",
    "            state, _ = env.reset()\n",
    "            # convert state to torch format\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            rewards.append(0)\n",
    "            while not (terminated or truncated):\n",
    "                # select action by following behaviour policy\n",
    "                action = self.behaviour(state)\n",
    "\n",
    "                # send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # convert next state to torch format and add experience to replay buffer\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "                self.replay_buffer.append((state, action, reward, next_state, terminated))\n",
    "                self.update() \n",
    "                # update Q-network\n",
    "                #logging_loss=#and log loss\n",
    "                '''if logging_loss is not None:\n",
    "                    wandb.log({\n",
    "                    'episode_num': episode,\n",
    "                    'loss':logging_loss,\n",
    "                    'steps': steps,\n",
    "                    'local reward':reward,\n",
    "                })'''\n",
    "                state = next_state\n",
    "                rewards[-1] += reward\n",
    "                num_steps += 1\n",
    "                steps += 1\n",
    "\n",
    "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, rewards = {rewards[episode]}     ', end='')\n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                    'episode': episode,\n",
    "                    'total steps':num_steps,\n",
    "                    'episode_reward': rewards[episode],\n",
    "                    'epsilon': self.epsilon,\n",
    "                })\n",
    "\n",
    "            if episode >= criterion_episodes-1 and stop_criterion(rewards[-criterion_episodes:]):\n",
    "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
    "                break\n",
    "###############################################################################################################################################################################################################\n",
    "     # changed evaluationto a function just to help break it up, also added logging and changed video script as mpy doesnt work for me for some reason\n",
    "    def evaluate(self, Run):\n",
    "        state, _ = self.env.reset()\n",
    "        state = state.flatten()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        steps=0\n",
    "        frames = []\n",
    "\n",
    "        while not (terminated or truncated): #removed max steps, boxing is timed so it will end after 2 minutes of the game we want to see how the agent goes over the full game\n",
    "\n",
    "            action = self.policy(state)\n",
    "            state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            steps +=1 \n",
    "        frame = self.env.render() # Render environment frame and store for video\n",
    "        v= VideoRecorderRAM('DQN')\n",
    "        v.frames=frame\n",
    "        vfilename = f'DQN_Prac_RAMV2_run_{Run}.mp4'\n",
    "        v.save(vfilename)\n",
    "        video_path=\"DQN/\" + vfilename\n",
    "        # Save video of the evaluation episode\n",
    "        #video = np.stack(frames)\n",
    "        #video_path = f\"evaluation_episode_{episode_num}.mp4\"\n",
    "        #mpy_clip = mpy.ImageSequenceClip(list(video), fps=30)\n",
    "        #mpy_clip.write_videofile(video_path, codec=\"libx264\")\n",
    "\n",
    "        # Log evaluation results and video to W&B\n",
    "        wandb.log({\n",
    "            'Evaluation Run': Run,\n",
    "            'Evaluation Reward': total_reward,\n",
    "            'Evaluation Video': wandb.Video(video_path, fps=30, format=\"mp4\")\n",
    "        })\n",
    "\n",
    "        # Logging evaluation result\n",
    "        print(f\"Evaluation run: {Run} Total Reward = {total_reward}\")\n",
    "        env.close()\n",
    "###########################################################################################################################################################################################################\n",
    "    def save(self, path):\n",
    "        # save network weights to a file\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "###########################################################################################################################################################################################################\n",
    "    def load(self, path):\n",
    "        # load network weights from a file\n",
    "        self.qnet.load_state_dict(torch.load(path))\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "###########################################################################################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Initialize, train and evaluate\n",
    "\n",
    "\n",
    "agent = V2AgentDQN(env,\n",
    "                 gamma=gamma,\n",
    "                 hidden_sizes=hidden_sizes,\n",
    "                 learning_rate=learning_rate,\n",
    "                 epsilon=epsilon,\n",
    "                 replay_size=replay_size,\n",
    "                 minibatch_size=minibatch_size,\n",
    "                 target_update=target_update)\n",
    "\n",
    "\n",
    "\n",
    "#agent.load('acrobot.128x128.DQN.pt')\n",
    "\n",
    "agent.train(max_episodes, lambda x : min(x) >= 100, criterion_episodes)\n",
    "for x in range(5):\n",
    "    agent.evaluate(Run=x)\n",
    "wandb.finish()\n",
    "# create and play video clip using the frames and given fps\n",
    "#clip = mpy.ImageSequenceClip(frames, fps=15)\n",
    "#clip.ipython_display(rd_kwargs=dict(logger=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a7bc6c5-492b-4859-8fa9-b05d0e965d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T05:20:43.693063Z",
     "iopub.status.busy": "2024-10-14T05:20:43.692659Z",
     "iopub.status.idle": "2024-10-14T05:20:43.695885Z",
     "shell.execute_reply": "2024-10-14T05:20:43.695616Z",
     "shell.execute_reply.started": "2024-10-14T05:20:43.693050Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save('Boxing.DQN.prac.agent2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424cd68-49b5-4255-9810-9c7a9a591043",
   "metadata": {},
   "source": [
    "Successful runs in evaluation !! When comparing DQN_V2 and DQN_V2_2 the only difference is the logging of the loss. Thus, it removing it allowed the agent to learn effectively. The comparison can be seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25dae47a-3087-499c-a6b5-9082ce20e25f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T14:13:15.652507Z",
     "iopub.status.busy": "2024-10-16T14:13:15.652227Z",
     "iopub.status.idle": "2024-10-16T14:13:15.656089Z",
     "shell.execute_reply": "2024-10-16T14:13:15.655832Z",
     "shell.execute_reply.started": "2024-10-16T14:13:15.652494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/DQN-Original-comparison-Final/reports/Loss-logging-impact--Vmlldzo5NzQzMjEy?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x73824e160710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb tristancarlisle/DQN-Original-comparison-Final/reports/Loss-logging-impact--Vmlldzo5NzQzMjEy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a8f31-7fba-471d-b58c-4e74610171a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Deep-Q-Network Image Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c9d3c-f79f-4ed2-9b75-1d0555fbb148",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "First iteration of improvements:\n",
    "instead of ram implementation going to try grayscale then reduce image size \n",
    "add warm up steps\n",
    "add epsilon decay\n",
    "change Q Network to utilise images \n",
    "Frame stack to try and learn temporal information\n",
    "Skip frames \n",
    "Normalise frame values for "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad443a85-81ae-40d8-b819-d5dc2e48baf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:43:53.661471Z",
     "iopub.status.busy": "2024-10-14T11:43:53.661314Z",
     "iopub.status.idle": "2024-10-14T11:43:53.663653Z",
     "shell.execute_reply": "2024-10-14T11:43:53.663392Z",
     "shell.execute_reply.started": "2024-10-14T11:43:53.661459Z"
    }
   },
   "source": [
    "##### Q Network\n",
    "- Adapted to images and frame staking, hence the use of 2D convolutions. greyscale is a 2D array stack them and you have a 3D array.\n",
    "```python\n",
    "nn.Conv2d\n",
    "```\n",
    "- Also added in warm up steps\n",
    "```python\n",
    "def warmup(self):\n",
    "```\n",
    "- epsilon decay \n",
    "```python\n",
    "epsilon_threshold = self.epsilon_end[0] + (self.epsilon_start[0] - self.epsilon_end[0]) * math.exp(-1. * num_steps / self.epsilon_decay[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85e5d40d-cdcf-49f2-b51b-ad9d251083ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T14:41:49.131760Z",
     "iopub.status.busy": "2024-10-16T14:41:49.131459Z",
     "iopub.status.idle": "2024-10-16T14:41:49.136471Z",
     "shell.execute_reply": "2024-10-16T14:41:49.136190Z",
     "shell.execute_reply.started": "2024-10-16T14:41:49.131746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # training on GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class QNetworkImage(nn.Module):\n",
    "    def __init__(self, input_size, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        # create network layers\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Conv2d(4, 32, kernel_size=8, stride=4))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        #Layer2\n",
    "        layers.append(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        #Layer3\n",
    "        layers.append(nn.Conv2d(64,64, kernel_size=3, stride=1))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        #flatten for linear layes\n",
    "        layers.append(nn.Flatten(start_dim=1))\n",
    "    \n",
    "        # output layer\n",
    "        layers.append(nn.Linear(3136, 512)) #input size =64*7*7 =            \n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(512, output_size))\n",
    "\n",
    "        # combine layers into feed-forward network\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # select loss function and optimizer\n",
    "        # note: original paper uses modified MSE loss and RMSprop\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return output of Q-network for the input x\n",
    "        return self.net(x)\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        # update network weights for a minibatch of inputs and targets:\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.net(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def copy_from(self, qnetwork):\n",
    "        # copy weights from another Q-network\n",
    "        self.net.load_state_dict(qnetwork.net.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "caebe49d-2a21-452f-92f1-ec11cbe88613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T14:41:54.045767Z",
     "iopub.status.busy": "2024-10-16T14:41:54.045457Z",
     "iopub.status.idle": "2024-10-16T14:41:54.878127Z",
     "shell.execute_reply": "2024-10-16T14:41:54.877828Z",
     "shell.execute_reply.started": "2024-10-16T14:41:54.045754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241016_224154-8yabz8ol</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol' target=\"_blank\">DQN_Image</a></strong> to <a href='https://wandb.ai/tristancarlisle/DQN_Image' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/DQN_Image' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN_Image</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73824e471110>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array\",obs_type=\"grayscale\") # use grayscale images\n",
    "env = gym.wrappers.ResizeObservation(env, shape=(84, 84))    # Resize to 84x84 Gym inbuilt wrappers\n",
    "env = gym.wrappers.NormalizeObservation(env) #Normalize \n",
    "\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0001\n",
    "max_episodes = 100\n",
    "max_steps = 1000\n",
    "criterion_episodes = 5\n",
    "epsilon_start=1, #added epsilon start point\n",
    "epsilon_end=0.05, #added epsilon endpoint\n",
    "epsilon_decay=50000, #added epsilon decay\n",
    "warmupsteps=1000 #added warmupsteps\n",
    "replay_size=10000\n",
    "minibatch_size=32\n",
    "target_update=20\n",
    "\n",
    "wandb.init(project='DQN_Image', name='DQN_Image', config={\n",
    "    'gamma': gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon start': epsilon_start,\n",
    "    'epsilon end': epsilon_end,\n",
    "    'epsilon decay': epsilon_decay,\n",
    "    'replay_size': replay_size,\n",
    "    'minibatch_size': minibatch_size,\n",
    "    'target_update_freq': target_update,\n",
    "    'num_episodes': max_episodes,\n",
    "    'max_steps_per_episode': max_steps,\n",
    "    'criterion_episodes':criterion_episodes\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db02575-8575-467d-901c-edc85f85acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent class set up initialise and run train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7321e4c0-6baf-45df-89e0-e3d332f1ae67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:47:03.846519Z",
     "iopub.status.busy": "2024-10-17T09:47:03.846333Z",
     "iopub.status.idle": "2024-10-17T09:47:23.512509Z",
     "shell.execute_reply": "2024-10-17T09:47:23.512170Z",
     "shell.execute_reply.started": "2024-10-17T09:47:03.846505Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x738243f84910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -3.0\n",
      "Evaluation run: 0 Total Reward = -3.0\n",
      "Reward: 3.0\n",
      "Evaluation run: 1 Total Reward = 3.0\n",
      "Reward: 1.0\n",
      "Evaluation run: 2 Total Reward = 1.0\n",
      "Reward: -3.0\n",
      "Evaluation run: 3 Total Reward = -3.0\n",
      "Reward: 2.0\n",
      "Evaluation run: 4 Total Reward = 2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.872 MB of 0.872 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Evaluation Reward</td><td>▁█▆▁▇</td></tr><tr><td>Evaluation Run</td><td>▁▃▅▆█</td></tr><tr><td>episode</td><td>▁▁▂▂▂▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▄▇▆▆▅▆▆▆▇▆▆▆█▄▅▇▃▅▆▆▆▅▁▃▃▅▇▃▃▁▆▅▅▆▇▅▆▄▅▄</td></tr><tr><td>epsilon threshold</td><td>▇▆▆▆█▇▇▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>steps</td><td>▁▁▁▁▁▂▂▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Evaluation Reward</td><td>2</td></tr><tr><td>Evaluation Run</td><td>4</td></tr><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-12</td></tr><tr><td>epsilon threshold</td><td>0.0767</td></tr><tr><td>steps</td><td>178600</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN_Image</strong> at: <a href='https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN_Image/runs/8yabz8ol</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/DQN_Image' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN_Image</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241016_224154-8yabz8ol/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%wandb\n",
    "class AgentDQNImage():\n",
    "    def __init__(self, env, gamma,\n",
    "                 #hidden_sizes=(128, 128,128), #Increased Depth and size\n",
    "                 learning_rate=0.0001, #decrease learning rate\n",
    "                 epsilon_start=1, #added epsilon start point\n",
    "                 epsilon_end=0.05, #added epsilon endpoint\n",
    "                 epsilon_decay=50000, #added epsilon decay\n",
    "                 warmupsteps=1000, #added warmupsteps\n",
    "                 replay_size=10000,\n",
    "                 minibatch_size=32,\n",
    "                 target_update=20):\n",
    "        # check if the state space has correct type\n",
    "        #continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1  removed as using an image now \n",
    "        #assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "\n",
    "        # check if the action space has correct type\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        # create Q-networks for action-value function\n",
    "        self.qnet = QNetworkImage(self.state_dims,self.num_actions, learning_rate).to(device) #moved to GPU\n",
    "        self.target_qnet = QNetworkImage(self.state_dims,self.num_actions, learning_rate).to(device)#moved to GPU\n",
    "\n",
    "        # copy weights from Q-network to target Q-network\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "        # initialise replay buffer\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end= epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.target_update = target_update\n",
    "        self.target_update_idx = 0\n",
    "        self.warmupsteps=warmupsteps\n",
    "###############################################################################################################################################################################################################\n",
    "    def behaviour(self, state, num_steps): \n",
    "        #set  epsilon threshold to global variable to keep track\n",
    "        global epsilon_threshold\n",
    "        # exploratory behaviour policy\n",
    "        epsilon_threshold = self.epsilon_end[0] + (self.epsilon_start[0] - self.epsilon_end[0]) * math.exp(-1. * num_steps / self.epsilon_decay[0]) #epsilon value calculation for incorporation of decay\n",
    "\n",
    "        if rng.uniform() >= epsilon_threshold:\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.tensor(state, dtype=torch.float).to(device)\n",
    "\n",
    "            # exploitation with probability 1-epsilon; break ties randomly\n",
    "            q = self.qnet(state).detach()\n",
    "            action = q.argmax().item() #changed to take the highest q value \n",
    "            return torch.tensor([[action]], dtype=torch.long).to(device) #changed for usage on GPU\n",
    "        else:\n",
    "            # exploration with probability epsilon\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long).to(device) #changed to usage on GPU\n",
    "###############################################################################################################################################################################################################\n",
    "    def policy(self, state):\n",
    "        # convert state to torch format\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "\n",
    "        # greedy policy\n",
    "        q = self.qnet(state).detach()\n",
    "        return q.argmax().item()\n",
    "###############################################################################################################################################################################################################\n",
    "    def update(self):\n",
    "        # update Q-network if there is enough experience\n",
    "        if len(self.replay_buffer) >= self.minibatch_size:\n",
    "            # select mini-batch of experiences uniformly at random without replacement                                \n",
    "            batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, replace=False)\n",
    "\n",
    "            # calculate inputs and targets for the transitions in the mini-batch\n",
    "            inputs = torch.zeros((self.minibatch_size, 4, 84, 84), device=device)\n",
    "            targets = torch.zeros((self.minibatch_size, self.num_actions), device=device)\n",
    "\n",
    "            for n, index in enumerate(batch):\n",
    "                state, action, reward, next_state, terminated = self.replay_buffer[index]\n",
    "                inputs[n] = state.squeeze(0) # had to change to account for frame stacking removes batch dimension\n",
    "\n",
    "                # targets are TD targets\n",
    "                targets[n, :] = self.target_qnet(state).detach()\n",
    "\n",
    "                if terminated:\n",
    "                    targets[n, action] = reward\n",
    "                else:\n",
    "                    targets[n, action] = reward + self.gamma*self.target_qnet(next_state).detach().max()\n",
    "            \n",
    "            # train Q-network on the mini-batch\n",
    "            self.qnet.update(inputs, targets)\n",
    "\n",
    "        # periodically copy weights from Q-network to target Q-network\n",
    "        self.target_update_idx += 1\n",
    "        if self.target_update_idx % self.target_update == 0:\n",
    "            self.target_qnet.copy_from(self.qnet)\n",
    "###############################################################################################################################################################################################################\n",
    "    def warmup(self):\n",
    "            print('Warming up')\n",
    "            warmupstep = 0\n",
    "            # Warmup loop\n",
    "            while warmupstep < self.warmupsteps:\n",
    "                state, _ = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "                state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "                terminated = False\n",
    "                truncated = False\n",
    "                while not (terminated or truncated):\n",
    "                    action = torch.tensor([[self.env.action_space.sample()]]).to(device)# Random action selection during warm-up\n",
    "                    # Step in environment\n",
    "                    next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "                    next_state = torch.stack((next_state,state[0][0],state[0][1],state[0][2])).unsqueeze(0)\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "                    term = torch.tensor([terminated or truncated], device=device)\n",
    "                    self.replay_buffer.append((state, action, reward, next_state, term))# Store the transition in memory\n",
    "                    state = next_state\n",
    "                    warmupstep += 1\n",
    "                    if warmupstep >= self.warmupsteps:\n",
    "                        break\n",
    "    \n",
    "            print(f'Warm-up finito: {self.warmupsteps} steps stored.')\n",
    "###############################################################################################################################################################################################################    \n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # train the agent for a number of episodes\n",
    "        rewards = []\n",
    "        num_steps = 0\n",
    "        for episode in range(max_episodes):\n",
    "            state, _ = env.reset()\n",
    "            # convert state to torch format\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            rewards.append(0)\n",
    "            while not (terminated or truncated):\n",
    "                # select action by following behaviour policy\n",
    "                action = self.behaviour(state , num_steps).to(device)\n",
    "\n",
    "                # send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # convert next state to torch format and add experience to replay buffer\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "                next_state = torch.stack((next_state,state[0][0],state[0][1],state[0][2])).unsqueeze(0)\n",
    "                reward=torch.tensor(reward, dtype=torch.float).to(device)\n",
    "                term = torch.tensor([terminated or  truncated], device=device)\n",
    "                self.replay_buffer.append((state, action, reward, next_state, term))\n",
    "                \n",
    "                # update Q-network\n",
    "                self.update()\n",
    "\n",
    "                state = next_state\n",
    "                rewards[-1] += reward\n",
    "                num_steps += 1\n",
    "\n",
    "                #all to tensors\n",
    "                \n",
    "                \n",
    "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, rewards = {rewards[episode]}     ', end='')\n",
    "\n",
    "            if episode >= criterion_episodes-1 and stop_criterion(rewards[-criterion_episodes:]):\n",
    "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
    "                break\n",
    "\n",
    "        # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                    'episode': episode,\n",
    "                    'episode_reward': rewards[episode],\n",
    "                    'epsilon threshold': epsilon_threshold,\n",
    "                    'steps': num_steps,\n",
    "                })\n",
    "\n",
    "        # plot rewards received during training\n",
    "        #plt.figure(dpi=100)\n",
    "        #plt.plot(range(1, len(rewards)+1), rewards, label=f'Rewards')\n",
    "\n",
    "        #plt.xlabel('Episodes')\n",
    "        #plt.ylabel('Rewards per episode')\n",
    "        #plt.legend(loc='lower right')\n",
    "        #plt.grid()\n",
    "        #plt.show()\n",
    "###############################################################################################################################################################################################################\n",
    "    def evaluate(self, Run):\n",
    "        state, _ = self.env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        steps=0\n",
    "        frames=[]\n",
    "        while not (terminated or truncated or steps > max_steps*2):\n",
    "            # take action based on policy\n",
    "            \n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "            action = agent.policy(state)\n",
    "        \n",
    "            # environment receives the action and returns:\n",
    "            # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            frame = env.render()\n",
    "            frames.append(frame)\n",
    "        print(f'Reward: {total_reward}')\n",
    "        v= VideoRecorder('DQN') # None ram version of videorecorder\n",
    "        for frame in frames:\n",
    "            v.record(frame)\n",
    "        vfilename = f'DQN__Image_run_{Run}.mp4'\n",
    "        v.save(vfilename)\n",
    "        video_path=\"DQN/\" + vfilename\n",
    "        # Save video of the evaluation episode\n",
    "        #video = np.stack(frames)\n",
    "        #video_path = f\"evaluation_episode_{episode_num}.mp4\"\n",
    "        #mpy_clip = mpy.ImageSequenceClip(list(video), fps=30)\n",
    "        #mpy_clip.write_videofile(video_path, codec=\"libx264\")\n",
    "\n",
    "        # Log evaluation results and video to W&B\n",
    "        wandb.log({\n",
    "            'Evaluation Run': Run,\n",
    "            'Evaluation Reward': total_reward,\n",
    "            'Evaluation Video': wandb.Video(video_path, fps=30, format=\"mp4\")\n",
    "        })\n",
    "\n",
    "        # Logging evaluation result\n",
    "        print(f\"Evaluation run: {Run} Total Reward = {total_reward}\")\n",
    "        env.close()\n",
    "###############################################################################################################################################################################################################       \n",
    "    def save(self, path):\n",
    "        # save network weights to a file\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "###############################################################################################################################################################################################################\n",
    "    def load(self, path):\n",
    "        # load network weights from a file\n",
    "        self.qnet.load_state_dict(torch.load(path))\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "###############################################################################################################################################################################################################\n",
    "\n",
    "#initialise Agent\n",
    "agent = AgentDQNImage(env,\n",
    "                 gamma=gamma,\n",
    "                 learning_rate=learning_rate,\n",
    "                 replay_size=replay_size,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay=epsilon_decay,\n",
    "                 minibatch_size=minibatch_size,\n",
    "                 target_update=target_update)\n",
    "\n",
    "\n",
    "#train \n",
    "#agent.train(max_episodes, lambda x : min(x) >= 100, criterion_episodes)\n",
    "\n",
    "#evaluate\n",
    "for x in range(5):\n",
    "    agent.evaluate(Run=x)\n",
    "wandb.finish()\n",
    "\n",
    "#agent.load('acrobot.128x128.DQN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "769fc9a5-3b5b-4758-96d5-9c7a7c09633c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T16:37:17.907704Z",
     "iopub.status.busy": "2024-10-16T16:37:17.907593Z",
     "iopub.status.idle": "2024-10-16T16:37:17.920141Z",
     "shell.execute_reply": "2024-10-16T16:37:17.919864Z",
     "shell.execute_reply.started": "2024-10-16T16:37:17.907693Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.save('Boxing.DQN.Image.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872931e1-ba0d-4dfe-aff6-ad95ffeb425e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Hyperparameter Sweep Deep-Q-Network (RAM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affd07c-26a3-4f7c-b027-6c09a188c184",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Param sweep on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6af7d-294a-4132-9b24-3592a14959db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define the Q-Network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        # Create network layers\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # Combine layers into a feed-forward network\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # Select loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Return output of Q-network for the input x\n",
    "        return self.net(x)\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        # Update network weights for a minibatch of inputs and targets\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.net(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def copy_from(self, qnetwork):\n",
    "        # Copy weights from another Q-network\n",
    "        self.net.load_state_dict(qnetwork.net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a556f38-9f06-4d84-95d3-4bd25224259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network (DQN) Agent\n",
    "class AgentDQN():\n",
    "    def __init__(self, env, config):\n",
    "        # Check if the state space has the correct type\n",
    "        continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "\n",
    "        # Check if the action space has the correct type\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        # Parse hidden_sizes from string to tuple\n",
    "        if isinstance(config.hidden_sizes, str):\n",
    "            hidden_sizes = ast.literal_eval(config.hidden_sizes)\n",
    "        else:\n",
    "            hidden_sizes = config.hidden_sizes\n",
    "\n",
    "        # Create Q-networks for action-value function\n",
    "        self.qnet = QNetwork(self.state_dims, hidden_sizes, self.num_actions, config.learning_rate).to(device)\n",
    "        self.target_qnet = QNetwork(self.state_dims, hidden_sizes, self.num_actions, config.learning_rate).to(device)\n",
    "\n",
    "        # Copy weights from Q-network to target Q-network\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        self.replay_buffer = deque(maxlen=config.replay_size)\n",
    "\n",
    "        # Initialize hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = config.gamma\n",
    "        self.epsilon = config.epsilon\n",
    "        self.minibatch_size = config.minibatch_size\n",
    "        self.target_update = config.target_update\n",
    "        self.target_update_idx = 0\n",
    "###############################################################################################################################################################################################################\n",
    "    def behaviour(self, state):\n",
    "        # Exploratory behaviour policy\n",
    "        if random.uniform(0, 1) >= self.epsilon:\n",
    "            # Convert state to torch format\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Exploitation: select the action with highest Q-value\n",
    "            with torch.no_grad():\n",
    "                q_values = self.qnet(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            # Exploration: select a random action\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "###############################################################################################################################################################################################################\n",
    "    def policy(self, state):\n",
    "        # Greedy policy: select the action with highest Q-value\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.qnet(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "        return action\n",
    "###############################################################################################################################################################################################################\n",
    "    def update(self):\n",
    "        # Update Q-network if there is enough experience\n",
    "        if len(self.replay_buffer) >= self.minibatch_size:\n",
    "            # Sample a minibatch of experiences uniformly at random\n",
    "            batch = random.sample(self.replay_buffer, self.minibatch_size)\n",
    "\n",
    "            # Extract components of the batch\n",
    "            states, actions, rewards, next_states, terminateds = zip(*batch)\n",
    "\n",
    "            # Convert to tensors\n",
    "            states = torch.stack(states).to(device)  # Shape: (batch_size, state_dims)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)  # Shape: (batch_size, 1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)  # Shape: (batch_size, 1)\n",
    "            next_states = torch.stack(next_states).to(device)  # Shape: (batch_size, state_dims)\n",
    "            terminateds = torch.tensor(terminateds, dtype=torch.float32).unsqueeze(1).to(device)  # Shape: (batch_size, 1)\n",
    "\n",
    "            # Compute current Q-values\n",
    "            current_q = self.qnet(states).gather(1, actions)  # Shape: (batch_size, 1)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            with torch.no_grad():\n",
    "                max_next_q = self.target_qnet(next_states).max(1)[0].unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "                target_q = rewards + (1 - terminateds) * self.gamma * max_next_q  # Shape: (batch_size, 1)\n",
    "\n",
    "            # Update Q-network\n",
    "            self.qnet.update(states, target_q)\n",
    "\n",
    "        # Periodically copy weights from Q-network to target Q-network\n",
    "        self.target_update_idx += 1\n",
    "        if self.target_update_idx % self.target_update == 0:\n",
    "            self.target_qnet.copy_from(self.qnet)\n",
    "###############################################################################################################################################################################################################\n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # Train the agent for a number of episodes\n",
    "        rewards = []\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not (terminated or truncated):\n",
    "                # Select action by following behaviour policy\n",
    "                action = self.behaviour(state)\n",
    "\n",
    "                # Send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Convert states to torch tensors\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "                next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Add experience to replay buffer\n",
    "                self.replay_buffer.append((state_tensor, action, reward, next_state_tensor, terminated))\n",
    "\n",
    "                # Update Q-network\n",
    "                self.update()\n",
    "\n",
    "                # Update state and cumulative reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            # Append the reward for this episode\n",
    "            rewards.append(episode_reward)\n",
    "\n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                'episode': episode,\n",
    "                'episode_reward': episode_reward,\n",
    "                'epsilon': self.epsilon,\n",
    "                'steps': episode  # Alternatively, track actual steps if desired\n",
    "            })\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Total Steps = {episode}\")\n",
    "\n",
    "            # Check stopping criterion\n",
    "            if episode >= criterion_episodes and stop_criterion(rewards[-criterion_episodes:]):\n",
    "                print(f\"\\nStopping criterion satisfied after {episode} episodes\")\n",
    "                break\n",
    "\n",
    "        # Plot rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(rewards)+1), rewards, label='Rewards per Episode')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "###############################################################################################################################################################################################################\n",
    "    def save(self, path):\n",
    "        # Save network weights to a file\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "###############################################################################################################################################################################################################\n",
    "    def load(self, path):\n",
    "        # Load network weights from a file\n",
    "        self.qnet.load_state_dict(torch.load(path, map_location=device))\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dddb7c6-68e7-422b-bad6-f4e3256812b1",
   "metadata": {},
   "source": [
    "##### Sweep setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7904ee28-5cbe-4f74-a36a-9f5a52087566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sweep Agent Function\n",
    "def sweep_agent():\n",
    "    try:\n",
    "        # Initialize a new wandb run\n",
    "        with wandb.init() as run:\n",
    "            config = wandb.config\n",
    "\n",
    "            # Create the environment with RAM observation type\n",
    "            env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "            print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "\n",
    "            # Ensure the observation is a 128-length vector\n",
    "            assert env.observation_space.shape == (128,), \"Observation space must be a 128-length vector.\"\n",
    "\n",
    "            # Instantiate AgentDQN with current wandb config\n",
    "            agent = AgentDQN(env=env, config=config)\n",
    "\n",
    "            # Define stopping criterion (optional)\n",
    "            def stopping_criterion(rewards):\n",
    "                # Example: stop if average reward over last 5 episodes >= 100\n",
    "                return np.mean(rewards) >= 100\n",
    "\n",
    "            # Start training\n",
    "            agent.train(\n",
    "                max_episodes=config.num_episodes,\n",
    "                stop_criterion=stopping_criterion,\n",
    "                criterion_episodes=config.criterion_episodes\n",
    "            )\n",
    "\n",
    "            # Close the environment after training\n",
    "            env.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error to wandb\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Ensure the environment is closed in case of an error\n",
    "        env.close()\n",
    "\n",
    "# Define the Sweep Configuration\n",
    "# Random Sweep Configuration with Limited Runs\n",
    "sweep_configuration = {\n",
    "    \"method\": \"random\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"episode_reward\",  # The metric to optimize\n",
    "        \"goal\": \"maximize\"         # Whether to \"minimize\" or \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {  # Learning rate for the Q-network\n",
    "            \"distribution\": \"log_uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"epsilon\": {  # Exploration rate\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 0.3\n",
    "        },\n",
    "        \"gamma\": {  # Discount factor for future rewards\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.95,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"replay_size\": {  # Replay buffer size\n",
    "            \"values\": [5000, 10000, 20000]  # Three discrete values\n",
    "        },\n",
    "        \"minibatch_size\": {  # Mini-batch size for updates\n",
    "            \"values\": [32, 64, 128]  # Three discrete values\n",
    "        },\n",
    "        # Fixed Hyperparameters\n",
    "        \"hidden_sizes\": {  # Architecture of hidden layers\n",
    "            \"values\": [\"(64,64)\",\"(128,128)\",\"(64,64,64)\",\"(128,128,128)\"]  \n",
    "        },\n",
    "        \"target_update\": {  # Frequency of target network updates\n",
    "            \"values\": [20]  # Fixed to 20\n",
    "        },\n",
    "        \"num_episodes\": {  # Total number of training episodes\n",
    "            \"values\": [500]  # Fixed to 500\n",
    "        },\n",
    "        \"criterion_episodes\": {  # Number of episodes for stopping criterion\n",
    "            \"values\": [5]  # Fixed to 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "     \n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='DQN')\n",
    "\n",
    "# Launch the Sweep with Limited Runs\n",
    "\n",
    "wandb.agent(sweep_id, function=sweep_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e643a88b-5a63-4aff-84ea-5757dd2961ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T05:22:22.571720Z",
     "iopub.status.busy": "2024-10-16T05:22:22.571325Z",
     "iopub.status.idle": "2024-10-16T05:22:23.260908Z",
     "shell.execute_reply": "2024-10-16T05:22:23.260585Z",
     "shell.execute_reply.started": "2024-10-16T05:22:22.571704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/DQN/sweeps/vk9fl1fi?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x76561013ddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb tristancarlisle/DQN/sweeps/vk9fl1fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23bfc25-a093-4638-9fb1-d98d8aefb01d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Dueling Double DQN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e3cbe-bbd7-4fca-a769-6a93b5298934",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Dueling Double DQN RAM Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb4598-4462-444d-9064-df7921a8d589",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Dueling Double DQN Image Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8942e06-a363-4575-9fc8-9e55fd90544e",
   "metadata": {},
   "source": [
    "##### Network for dueling \n",
    "big point is the presence of the advantage and value streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ab1db02-eca6-4f71-8aa1-fe53c0ef1e65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:51:54.844567Z",
     "iopub.status.busy": "2024-10-17T09:51:54.844128Z",
     "iopub.status.idle": "2024-10-17T09:51:54.850447Z",
     "shell.execute_reply": "2024-10-17T09:51:54.850103Z",
     "shell.execute_reply.started": "2024-10-17T09:51:54.844550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # training on GPU\n",
    "print(f\"Using device: {device}\")\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "class DDQNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        # create network layers (gave up trying to get the format in the practicals to work this is just how I always do it)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(input_size, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.netAx = nn.Sequential(\n",
    "            nn.Linear(3136, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        \n",
    "        # Value stream\n",
    "        self.netVx = nn.Sequential(\n",
    "            nn.Linear(3136, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.net.parameters()) + list(self.netAx.parameters()) + list(self.netVx.parameters()),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        Ax = self.netAx(x) #key difference wiith network is the calculations here\n",
    "        Vx = self.netVx(x)\n",
    "        return Vx + (Ax - Ax.mean(dim=1, keepdim=True))\n",
    "\n",
    "    def update(self, inputs, targets):\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def copy_from(self, qnetwork):\n",
    "        # Correctly copy the entire state_dict\n",
    "        self.load_state_dict(qnetwork.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f066ffe-2962-49a6-972d-15fd05c18eba",
   "metadata": {},
   "source": [
    "###### Env set up for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "81266c41-98a1-4768-873f-dc478efb5ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:51:59.031812Z",
     "iopub.status.busy": "2024-10-17T09:51:59.031566Z",
     "iopub.status.idle": "2024-10-17T09:51:59.748681Z",
     "shell.execute_reply": "2024-10-17T09:51:59.748407Z",
     "shell.execute_reply.started": "2024-10-17T09:51:59.031793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241017_175159-danpovfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/DDQN%20dueling_Image/runs/danpovfr' target=\"_blank\">DDQN_dueling_image_1</a></strong> to <a href='https://wandb.ai/tristancarlisle/DDQN%20dueling_Image' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/DDQN%20dueling_Image' target=\"_blank\">https://wandb.ai/tristancarlisle/DDQN%20dueling_Image</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/DDQN%20dueling_Image/runs/danpovfr' target=\"_blank\">https://wandb.ai/tristancarlisle/DDQN%20dueling_Image/runs/danpovfr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tristancarlisle/DDQN%20dueling_Image/runs/danpovfr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73823653e0d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array\",obs_type=\"grayscale\")\n",
    "env = gym.wrappers.ResizeObservation(env, shape=(84, 84))    # Resize to 84x84\n",
    "env = gym.wrappers.NormalizeObservation(env)\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "learning_rate = 0.0001\n",
    "max_episodes = 100\n",
    "max_steps = 1000\n",
    "criterion_episodes = 5\n",
    "epsilon_start=1 #added epsilon start point\n",
    "epsilon_end=0.05 #added epsilon endpoint\n",
    "epsilon_decay=50000 #added epsilon decay\n",
    "warmupsteps=1000 #added warmupsteps\n",
    "replay_size=10000\n",
    "minibatch_size=32\n",
    "target_update=20\n",
    "rep_omega=0.2\n",
    "\n",
    "wandb.init(project='DDQN dueling_Image', name='DDQN_dueling_image_1', config={\n",
    "    'gamma': gamma,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon_start': epsilon_start,\n",
    "    'epsilon_end': epsilon_end,\n",
    "    'epsilon_decay': epsilon_decay,\n",
    "    'replay_size': replay_size,\n",
    "    'minibatch_size': minibatch_size,\n",
    "    'target_update_freq': target_update,\n",
    "    'num_episodes': max_episodes,\n",
    "    'max_steps_per_episode': max_steps,\n",
    "    'rep_omega':rep_omega,\n",
    "    'criterion_episodes':criterion_episodes\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd17aafa-cb3d-4ed0-9fd2-60b55b806b54",
   "metadata": {},
   "source": [
    "##### Agent class set up initialisation and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd0a18b-f952-4afc-b150-b12f1b5c8ba2",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-17T09:52:25.713180Z",
     "iopub.status.busy": "2024-10-17T09:52:25.712954Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 done: steps = 35720, rewards = -4.0     "
     ]
    }
   ],
   "source": [
    "# Deep Q-network (DQN)\n",
    "class AgentDuelingDDQNImage():\n",
    "    def __init__(self, env, gamma,\n",
    "                 #hidden_sizes=(128, 128,128), #Increased Depth and size\n",
    "                 learning_rate=0.0001, #decrease learning rate\n",
    "                 epsilon_start=1, #added epsilon start point\n",
    "                 epsilon_end=0.05, #added epsilon endpoint\n",
    "                 epsilon_decay=50000, #added epsilon decay\n",
    "                 warmupsteps=1000, #added warmupsteps\n",
    "                 replay_size=10000,\n",
    "                 minibatch_size=32,\n",
    "                 rep_omega=0.2,\n",
    "                 target_update=20):\n",
    "        # check if the state space has correct type\n",
    "        #continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1  removed as using an image now \n",
    "        #assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "\n",
    "        # check if the action space has correct type\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.qnet = DDQNetwork(input_size=4, output_size=self.num_actions, learning_rate=learning_rate).to(device) #moved to GPU\n",
    "        self.target_qnet = DDQNetwork(input_size=4, output_size=self.num_actions, learning_rate=learning_rate).to(device)#moved to GPU\n",
    "\n",
    "\n",
    "        # copy weights from Q-network to target Q-network\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "        # initialise replay buffer\n",
    "        self.replay_buffer = deque(maxlen=replay_size)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end= epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.target_update = target_update\n",
    "        self.rep_omega=rep_omega\n",
    "        self.target_update_idx = 0\n",
    "        self.warmupsteps=warmupsteps\n",
    "##########################################################################################################################################################################################################\n",
    "    def behaviour(self, state, num_steps): \n",
    "        #set  epsilon threshold to global variable to keep track\n",
    "        global epsilon_threshold\n",
    "        # exploratory behaviour policy\n",
    "        epsilon_threshold = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-1. * num_steps / self.epsilon_decay)#epsilon value calculation for incorporation of decay\n",
    "\n",
    "        if rng.uniform() >= epsilon_threshold:\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "            # exploitation with probability 1-epsilon; break ties randomly\n",
    "            q = self.qnet(state).detach()\n",
    "            j = rng.permutation(self.num_actions)\n",
    "            #print(f\"Q-values shape: {q.shape}, J: {j.shape}\")\n",
    "            action = j[q[0][j].argmax().item()]\n",
    "            return torch.tensor([[action]], dtype=torch.long).to(device) #changed for usage on GPU\n",
    "        else:\n",
    "            # exploration with probability epsilon\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long).to(device) #changed to usage on GPU\n",
    "##########################################################################################################################################################################################################\n",
    "    def policy(self, state):\n",
    "        # convert state to torch format\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # greedy policy\n",
    "        q = self.qnet(state).detach()\n",
    "        return q.argmax().item()\n",
    "##########################################################################################################################################################################################################\n",
    "    def td_error(self, state, action, reward, next_state, terminated):\n",
    "        # calculate td error for prioritised experience replay\n",
    "        q = self.qnet(state).detach()\n",
    "        action_index = action.item()#added cause I kept getting an indexing error and for some reason I cannot count \n",
    "        if action_index < 0 or action_index >= self.num_actions:\n",
    "            raise ValueError(f\"Action index {action_index} is out of bounds for the action space of size {self.num_actions}.\")\n",
    "        if terminated:\n",
    "            #have to convert back to CPU for numpy :(\n",
    "            return np.abs(reward.cpu().item() - q[0][action_index].cpu().item()) ** self.rep_omega\n",
    "        else:\n",
    "            next_action = self.qnet(next_state).detach().argmax().cpu()\n",
    "            next_q = self.target_qnet(next_state).detach()\n",
    "            #print(f\"Q-values shape: {q[0].shape}, Action: {action.item()}\")\n",
    "            #print(f\"Next Q-values shape: {next_q[0].shape}, Next Action: {next_action}\")\n",
    "            #also needs to be converted back to CPU for numpy\n",
    "            return np.abs(reward.cpu().item() + self.gamma * next_q[0][next_action].cpu().item() - q[0][action_index].cpu().item()) ** self.rep_omega\n",
    "            \n",
    "##########################################################################################################################################################################################################\n",
    "    def update(self):\n",
    "        # update Q-network if there is enough experience\n",
    "        if len(self.replay_buffer) >= self.minibatch_size:\n",
    "            # select mini-batch of experiences uniformly at random without replacement                                \n",
    "            p_rep = np.array([self.replay_buffer[i][-1].cpu() for i in range(len(self.replay_buffer))])\n",
    "            p_rep = p_rep/p_rep.sum()\n",
    "            batch = rng.choice(len(self.replay_buffer), size=self.minibatch_size, p=p_rep, replace=False)\n",
    "\n",
    "            # calculate inputs and targets for the transitions in the mini-batch\n",
    "            inputs = torch.zeros((self.minibatch_size, 4, 84, 84), device=device)\n",
    "            targets = torch.zeros((self.minibatch_size, self.num_actions), device=device)\n",
    "\n",
    "            for n, index in enumerate(batch):\n",
    "                state, action, reward, next_state, terminated, per = self.replay_buffer[index]\n",
    "                inputs[n] = state.squeeze(0) # had to change to account for frame stacking removes batch dimension\n",
    "\n",
    "                # targets are TD targets\n",
    "                target_q = self.target_qnet(state).detach()\n",
    "                targets[n, :] = target_q\n",
    "\n",
    "                if terminated:\n",
    "                    targets[n, action] = reward\n",
    "                else:\n",
    "                    next_action = self.qnet(next_state).detach().argmax().item() #primary network for action selection \n",
    "                    next_q = self.target_qnet(next_state).detach()#target network for action evaluation\n",
    "                    targets[n, action.item()] = reward + self.gamma*next_q[0][next_action]\n",
    "            \n",
    "            # train Q-network on the mini-batch\n",
    "            self.qnet.update(inputs, targets)\n",
    "\n",
    "        # periodically copy weights from Q-network to target Q-network\n",
    "        self.target_update_idx += 1\n",
    "        if self.target_update_idx % self.target_update == 0:\n",
    "            self.target_qnet.copy_from(self.qnet)\n",
    "##########################################################################################################################################################################################################\n",
    "    def warmup(self):\n",
    "            print('Warming up')\n",
    "            warmupstep = 0\n",
    "            # Warmup loop\n",
    "            while warmupstep < self.warmupsteps:\n",
    "                state, _ = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "                state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "                terminated = False\n",
    "                truncated = False\n",
    "                while not (terminated or truncated):\n",
    "                    action = torch.tensor([[self.env.action_space.sample()]]).to(device)# Random action selection during warm-up\n",
    "                    # Step in environment\n",
    "                    next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                    next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "                    next_state = torch.stack((next_state,state[0][0],state[0][1],state[0][2])).unsqueeze(0)\n",
    "                    reward = torch.tensor([reward], device=device)\n",
    "                    term = torch.tensor([terminated or truncated], device=device)\n",
    "                    per = self.td_error(state, action, reward, next_state, term)\n",
    "                    self.replay_buffer.append((state, action, reward, next_state, term,per))# Store the transition in memory\n",
    "                    state = next_state\n",
    "                    warmupstep += 1\n",
    "                    if warmupstep >= self.warmupsteps:\n",
    "                        break\n",
    "    \n",
    "            print(f'Warm-up finito: {self.warmupsteps} steps stored.')\n",
    "\n",
    "\n",
    "##########################################################################################################################################################################################################\n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # train the agent for a number of episodes\n",
    "        rewards = []\n",
    "        num_steps = 0\n",
    "        for episode in range(max_episodes):\n",
    "            state, _ = env.reset()\n",
    "            # convert state to torch format\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            rewards.append(0)\n",
    "            while not (terminated or truncated):\n",
    "                # select action by following behaviour policy\n",
    "                action = self.behaviour(state , num_steps).to(device)\n",
    "\n",
    "                # send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "                # convert next state to torch format and add experience to replay buffer\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "                next_state = torch.stack((next_state,state[0][0],state[0][1],state[0][2])).unsqueeze(0)\n",
    "                reward=torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "                term = torch.tensor([terminated or  truncated], device=device)\n",
    "                per = torch.tensor(self.td_error(state, action, reward, next_state, term), dtype=torch.float32).to(device)\n",
    "                \n",
    "                self.replay_buffer.append((state, action, reward, next_state, term,per))\n",
    "                \n",
    "                # update Q-network\n",
    "                self.update()\n",
    "\n",
    "                state = next_state\n",
    "                rewards[-1] += reward\n",
    "                num_steps += 1\n",
    "\n",
    "                #all to tensors\n",
    "            wandb.log({\n",
    "                    'episode': episode,\n",
    "                    'episode_reward': rewards[episode],\n",
    "                    'epsilon threshold': epsilon_threshold,\n",
    "                    'steps': num_steps,\n",
    "                })     \n",
    "                \n",
    "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, rewards = {rewards[episode]}     ', end='')\n",
    "\n",
    "            if episode >= criterion_episodes-1 and stop_criterion(rewards[-criterion_episodes:]):\n",
    "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
    "                break\n",
    "\n",
    "        # Log metrics to W&B\n",
    "           \n",
    "\n",
    "        # plot rewards received during training\n",
    "        #plt.figure(dpi=100)\n",
    "        #plt.plot(range(1, len(rewards)+1), rewards, label=f'Rewards')\n",
    "\n",
    "        #plt.xlabel('Episodes')\n",
    "        #plt.ylabel('Rewards per episode')\n",
    "        #plt.legend(loc='lower right')\n",
    "        #plt.grid()\n",
    "        #plt.show()\n",
    "\n",
    "##########################################################################################################################################################################################################\n",
    "        def evaluate(self, Run):\n",
    "            state, _ = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            total_reward = 0\n",
    "            steps=0\n",
    "            frames=[]\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "            while not (terminated or truncated):\n",
    "                # take action based on policy\n",
    "\n",
    "                state = torch.stack((state,state,state,state)).unsqueeze(0) #(1,4,84,84)\n",
    "                action = AgentDuelingDDQNImage.policy(state)\n",
    "            \n",
    "                # environment receives the action and returns:\n",
    "                # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
    "                state, reward, terminated, truncated, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                frame = env.render()\n",
    "                frames.append(frame)\n",
    "            print(f'Reward: {total_reward}')\n",
    "            v= VideoRecorder('DQN') # None ram version of videorecorder\n",
    "            for frame in frames:\n",
    "                v.record(frame)\n",
    "            vfilename = f'DDDQN__Image_run_{Run}.mp4'\n",
    "            v.save(vfilename)\n",
    "            video_path=\"DDDQN/\" + vfilename\n",
    "            # Save video of the evaluation episode\n",
    "            #video = np.stack(frames)\n",
    "            #video_path = f\"evaluation_episode_{episode_num}.mp4\"\n",
    "            #mpy_clip = mpy.ImageSequenceClip(list(video), fps=30)\n",
    "            #mpy_clip.write_videofile(video_path, codec=\"libx264\")\n",
    "    \n",
    "            # Log evaluation results and video to W&B\n",
    "            wandb.log({\n",
    "                'Evaluation Run': Run,\n",
    "                'Evaluation Reward': total_reward,\n",
    "                'Evaluation Video': wandb.Video(video_path, fps=30, format=\"mp4\")\n",
    "            })\n",
    "    \n",
    "            # Logging evaluation result\n",
    "            print(f\"Evaluation run: {Run} Total Reward = {total_reward}\")\n",
    "            env.close()\n",
    "##########################################################################################################################################################################################################            \n",
    "    def save(self, path):\n",
    "        # save network weights to a file\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "##########################################################################################################################################################################################################\n",
    "    def load(self, path):\n",
    "        # load network weights from a file\n",
    "        self.qnet.load_state_dict(torch.load(path))\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#initiaise\n",
    "AgentDuelingDDQNImage = AgentDuelingDDQNImage(env,\n",
    "                 gamma=gamma,\n",
    "                 learning_rate=learning_rate,\n",
    "                 replay_size=replay_size,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 rep_omega=rep_omega,\n",
    "                 epsilon_decay=epsilon_decay,\n",
    "                 minibatch_size=minibatch_size,\n",
    "                 target_update=target_update)\n",
    "\n",
    "#agent.load('acrobot.128x128.DQN.pt')\n",
    "AgentDuelingDDQNImage.train(max_episodes, lambda x : min(x) >= 100, criterion_episodes)\n",
    "\n",
    "\n",
    "##########################################################################################################################################################################################################\n",
    "#Evaluate\n",
    "for x in range(5):\n",
    "    AgentDuelingDDQNImage.evaluate(Run=x)\n",
    "wandb.finish()\n",
    "\n",
    "# create and play video clip using the frames and given fps\n",
    "#clip = mpy.ImageSequenceClip(frames, fps=15)\n",
    "#clip.ipython_display(rd_kwargs=dict(logger=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d8352-6ebd-4af6-93be-56f8ef2630d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentDQN.save('Boxing.DuelingDQN.greyscale.agent1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed6671-b0df-4fed-8a08-c765123e6702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%wandb tristancarlisle/DDQN dueling_Image/DDQN_dueling_image_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea591195-2f87-47d1-b7c8-e759856b6ff3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Hyperparameter Sweep Dueling Double DQN (RAM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5c240-f388-49b8-88a2-8a460985228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define the Dueling Q-Network\n",
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "        # Shared network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        self.shared_net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_net = nn.Sequential(\n",
    "            nn.Linear(last_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)\n",
    "        )\n",
    "        \n",
    "        # Value stream\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(last_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.shared_net(x)  # Shape: [batch_size, last_hidden_size]\n",
    "        advantage = self.advantage_net(shared)  # Shape: [batch_size, output_size]\n",
    "        value = self.value_net(shared)  # Shape: [batch_size, 1]\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_values  # Shape: [batch_size, output_size]\n",
    "    \n",
    "    def update_network(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def copy_from(self, source_network):\n",
    "        self.load_state_dict(source_network.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cca27-0be9-43a2-9040-e7a40fc99160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dueling Double DQN Agent\n",
    "class AgentDuelingDoubleDQN:\n",
    "    def __init__(self, env, config, device):\n",
    "        # Validate observation space\n",
    "        continuous = isinstance(env.observation_space, gym.spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "    \n",
    "        # Validate action space\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "    \n",
    "        # Parse hidden_sizes from string to tuple if necessary\n",
    "        if isinstance(config.hidden_sizes, str):\n",
    "            hidden_sizes = ast.literal_eval(config.hidden_sizes)\n",
    "        elif isinstance(config.hidden_sizes, list):\n",
    "            hidden_sizes = tuple(config.hidden_sizes)\n",
    "        else:\n",
    "            hidden_sizes = config.hidden_sizes\n",
    "    \n",
    "       # print(f\"Using hidden_sizes: {hidden_sizes}\")\n",
    "    \n",
    "        # Initialize Q-networks\n",
    "        self.qnet = DuelingQNetwork(\n",
    "            input_size=self.state_dims, \n",
    "            hidden_sizes=hidden_sizes, \n",
    "            output_size=self.num_actions, \n",
    "            learning_rate=config.learning_rate\n",
    "        ).to(device)\n",
    "    \n",
    "        self.target_qnet = DuelingQNetwork(\n",
    "            input_size=self.state_dims, \n",
    "            hidden_sizes=hidden_sizes, \n",
    "            output_size=self.num_actions, \n",
    "            learning_rate=config.learning_rate\n",
    "        ).to(device)\n",
    "    \n",
    "        # Copy weights from Q-network to target Q-network\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "    \n",
    "        # Initialize replay buffer\n",
    "        self.replay_buffer = deque(maxlen=config.replay_size)\n",
    "    \n",
    "        # Initialize hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = config.gamma\n",
    "        self.epsilon = config.epsilon\n",
    "        self.minibatch_size = config.minibatch_size\n",
    "        self.target_update = config.target_update\n",
    "        self.target_update_idx = 0\n",
    "    \n",
    "    def behaviour(self, state):\n",
    "        # Exploratory behaviour policy\n",
    "        if random.uniform(0, 1) >= self.epsilon:\n",
    "            # Convert state to torch tensor\n",
    "            if not torch.is_tensor(state):\n",
    "                state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            state = state.unsqueeze(0)  # Shape: [1, state_dims]\n",
    "    \n",
    "            # Exploitation: select the action with highest Q-value\n",
    "            with torch.no_grad():\n",
    "                q_values = self.qnet(state)  # Shape: [1, num_actions]\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "        else:\n",
    "            # Exploration: select a random action\n",
    "            action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def policy(self, state):\n",
    "        # Greedy policy: select the action with highest Q-value\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        state = state.unsqueeze(0)  # Shape: [1, state_dims]\n",
    "        with torch.no_grad():\n",
    "            q_values = self.qnet(state)  # Shape: [1, num_actions]\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "        return action\n",
    "    \n",
    "    def update(self):\n",
    "        # Update Q-network if there is enough experience\n",
    "        if len(self.replay_buffer) >= self.minibatch_size:\n",
    "            # Sample a minibatch of experiences uniformly at random\n",
    "            batch = random.sample(self.replay_buffer, self.minibatch_size)\n",
    "    \n",
    "            # Extract components of the batch\n",
    "            states, actions, rewards, next_states, terminateds = zip(*batch)\n",
    "    \n",
    "            # Convert to tensors\n",
    "            states = torch.stack(states).to(device)  # Shape: [batch_size, state_dims]\n",
    "            actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1).to(device)  # Shape: [batch_size, 1]\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)  # Shape: [batch_size, 1]\n",
    "            next_states = torch.stack(next_states).to(device)  # Shape: [batch_size, state_dims]\n",
    "            terminateds = torch.tensor(terminateds, dtype=torch.float32).unsqueeze(1).to(device)  # Shape: [batch_size, 1]\n",
    "    \n",
    "            # Debugging: Print tensor shapes\n",
    "            #print(f\"Update - states shape: {states.shape}\")  # [batch_size, state_dims]\n",
    "            #print(f\"Update - actions shape: {actions.shape}\")  # [batch_size, 1]\n",
    "            #print(f\"Update - rewards shape: {rewards.shape}\")  # [batch_size, 1]\n",
    "            #print(f\"Update - next_states shape: {next_states.shape}\")  # [batch_size, state_dims]\n",
    "            #print(f\"Update - terminateds shape: {terminateds.shape}\")  # [batch_size, 1]\n",
    "    \n",
    "            # Validate actions\n",
    "            if (actions < 0).any() or (actions >= self.num_actions).any():\n",
    "                invalid_actions = actions[(actions < 0) | (actions >= self.num_actions)]\n",
    "                print(f\"Invalid actions in batch: {invalid_actions}\")\n",
    "                raise ValueError(\"Found invalid action indices in batch.\")\n",
    "    \n",
    "            # Compute current Q-values for the actions taken\n",
    "            current_q = self.qnet(states).gather(1, actions)  # Shape: [batch_size, 1]\n",
    "            #print(f\"Update - current_q shape: {current_q.shape}\")  # [batch_size, 1]\n",
    "    \n",
    "            # Double DQN: Use the main network to select the best action, and the target network to evaluate it\n",
    "            with torch.no_grad():\n",
    "                # Select the best action using the main Q-network\n",
    "                next_q_main = self.qnet(next_states)  # Shape: [batch_size, num_actions]\n",
    "                best_next_actions = torch.argmax(next_q_main, dim=1).unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "                #print(f\"Update - best_next_actions shape: {best_next_actions.shape}\")  # [batch_size, 1]\n",
    "    \n",
    "                # Evaluate the best actions using the target Q-network\n",
    "                next_q_target = self.target_qnet(next_states).gather(1, best_next_actions).squeeze(1)  # Shape: [batch_size]\n",
    "               # print(f\"Update - next_q_target shape: {next_q_target.shape}\")  # [batch_size]\n",
    "    \n",
    "                # Compute target Q-values\n",
    "                target_q = rewards.squeeze(1) + (1 - terminateds.squeeze(1)) * self.gamma * next_q_target  # Shape: [batch_size]\n",
    "               # print(f\"Update - target_q shape: {target_q.shape}\")  # [batch_size]\n",
    "    \n",
    "            # Compute loss\n",
    "            loss = self.qnet.criterion(current_q.squeeze(1), target_q)\n",
    "            #print(f\"Update - loss: {loss.item()}\")\n",
    "    \n",
    "            # Update Q-network\n",
    "            self.qnet.update_network(loss)\n",
    "    \n",
    "            # Periodically copy weights from Q-network to target Q-network\n",
    "            self.target_update_idx += 1\n",
    "            if self.target_update_idx % self.target_update == 0:\n",
    "                self.target_qnet.copy_from(self.qnet)\n",
    "                #print(\"Target network updated.\")\n",
    "    \n",
    "    def train_agent(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # Train the agent for a number of episodes\n",
    "        rewards = []\n",
    "        num_steps = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_reward = 0\n",
    "    \n",
    "            while not (terminated or truncated):\n",
    "                # Select action by following behaviour policy\n",
    "                action = self.behaviour(state)\n",
    "    \n",
    "                # Send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "    \n",
    "                # Convert states to torch tensors\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).to(device)  # Shape: [state_dims]\n",
    "                next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(device)  # Shape: [state_dims]\n",
    "    \n",
    "                # Add experience to replay buffer\n",
    "                self.replay_buffer.append((state_tensor, action, reward, next_state_tensor, terminated))\n",
    "    \n",
    "                # Update Q-network\n",
    "                try:\n",
    "                    self.update()\n",
    "                except ValueError as ve:\n",
    "                    print(f\"ValueError during update: {ve}\")\n",
    "                    raise ve\n",
    "                except RuntimeError as re:\n",
    "                    print(f\"RuntimeError during update: {re}\")\n",
    "                    print(\"Consider running on CPU for detailed error messages.\")\n",
    "                    raise re\n",
    "    \n",
    "                # Update state and cumulative reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                num_steps += 1\n",
    "    \n",
    "            # Append the reward for this episode\n",
    "            rewards.append(episode_reward)\n",
    "    \n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                'episode': episode,\n",
    "                'episode_reward': episode_reward,\n",
    "                'epsilon': self.epsilon,\n",
    "                'steps': num_steps\n",
    "            })\n",
    "    \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Total Steps = {num_steps}\")\n",
    "    \n",
    "            # Check stopping criterion\n",
    "            if episode >= criterion_episodes and stop_criterion(rewards[-criterion_episodes:]):\n",
    "                print(f\"\\nStopping criterion satisfied after {episode} episodes\")\n",
    "                break\n",
    "    \n",
    "        # Plot rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(rewards)+1), rewards, label='Rewards per Episode')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        # Save network weights to a file\n",
    "        torch.save(self.qnet.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        # Load network weights from a file\n",
    "        self.qnet.load_state_dict(torch.load(path, map_location=device))\n",
    "        self.target_qnet.copy_from(self.qnet)\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213860a-0595-4ca4-9bae-9c78650c20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sweep Agent Function\n",
    "def sweep_agent():\n",
    "    try:\n",
    "        # Initialize a new wandb run\n",
    "        with wandb.init() as run:\n",
    "            config = wandb.config\n",
    "    \n",
    "            # Create the environment with RAM observation type\n",
    "            env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "            print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "    \n",
    "            # Ensure the observation is a 128-length vector\n",
    "            assert env.observation_space.shape == (128,), \"Observation space must be a 128-length vector.\"\n",
    "    \n",
    "            # Instantiate AgentDuelingDoubleDQN with current wandb config\n",
    "            agent = AgentDuelingDoubleDQN(env=env, config=config, device=device)\n",
    "    \n",
    "            # Define stopping criterion (optional)\n",
    "            def stopping_criterion(rewards):\n",
    "                # Example: stop if average reward over last 5 episodes >= 100\n",
    "                return np.mean(rewards) >= 100\n",
    "    \n",
    "            # Start training\n",
    "            agent.train_agent(\n",
    "                max_episodes=config.num_episodes,\n",
    "                stop_criterion=stopping_criterion,\n",
    "                criterion_episodes=config.criterion_episodes\n",
    "            )\n",
    "    \n",
    "            # Close the environment after training\n",
    "            env.close()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Log the error to wandb\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Ensure the environment is closed in case of an error\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446cd609-4f92-49d5-9bac-e7a25b03b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sweep Configuration\n",
    "# Random Sweep Configuration with Limited Runs\n",
    "sweep_configuration = {\n",
    "    \"method\": \"random\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"episode_reward\",  # The metric to optimize\n",
    "        \"goal\": \"maximize\"         # Whether to \"minimize\" or \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {  # Learning rate for the Q-network\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-7,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"epsilon\": {  # Exploration rate\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 0.3\n",
    "        },\n",
    "        \"gamma\": {  # Discount factor for future rewards\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.95,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"replay_size\": {  # Replay buffer size\n",
    "            \"values\": [5000, 10000, 20000]  # Three discrete values\n",
    "        },\n",
    "        \"minibatch_size\": {  # Mini-batch size for updates\n",
    "            \"values\": [32, 64, 128]  # Three discrete values\n",
    "        },\n",
    "        # Fixed Hyperparameters\n",
    "        \"hidden_sizes\": {  # Architecture of hidden layers\n",
    "            \"values\": [\"(64,64)\", \"(128,128)\", \"(64,64,64)\", \"(128,128,128)\"]  \n",
    "        },\n",
    "        \"target_update\": {  # Frequency of target network updates\n",
    "            \"values\": [20]  # Fixed to 20\n",
    "        },\n",
    "        \"num_episodes\": {  # Total number of training episodes\n",
    "            \"values\": [500]  # Fixed to 500\n",
    "        },\n",
    "        \"criterion_episodes\": {  # Number of episodes for stopping criterion\n",
    "            \"values\": [5]  # Fixed to 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='DuelingDoubleDQN_RAM')\n",
    "\n",
    "# Launch the Sweep with Limited Runs\n",
    "wandb.agent(sweep_id, function=sweep_agent, count=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ba3835ea-95c9-47aa-b1d0-a96e2aa809ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T09:31:35.413959Z",
     "iopub.status.busy": "2024-10-17T09:31:35.413615Z",
     "iopub.status.idle": "2024-10-17T09:31:36.066291Z",
     "shell.execute_reply": "2024-10-17T09:31:36.065911Z",
     "shell.execute_reply.started": "2024-10-17T09:31:35.413946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/DuelingDoubleDQN_RAM/sweeps/14vtcl50?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x73824f13d950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb tristancarlisle/DuelingDoubleDQN_RAM/sweeps/14vtcl50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fddfd9-7d05-4355-b038-f09617d18b0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Soft Actor Critic (SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5adf7c-f807-4d6a-9676-ab55a2662b37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Actor Critic Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564860a-b366-4f54-9890-9f2c91551215",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Actor Critic Image Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccf4873-059b-4ec5-81b4-65f1901a511c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Explanation of the Current Implementation\n",
    "The code below implements a basic Actor-Critic algorithm for the Atari Boxing environment using PyTorch.\n",
    "\n",
    "Overview of Actor-Critic Algorithm\n",
    "The Actor-Critic (AC) method is a type of policy gradient algorithm that combines both value-based and policy-based methods. It consists of two main components:\n",
    "    Actor (Policy Network): Learns the policy function 𝜋(𝑎∣𝑠), which tells the agent which action to take in a given state.\n",
    "    Critic (Value Network): Learns the value function 𝑉(𝑠), which estimates the expected return from a state.\n",
    "    The actor updates the policy in the direction suggested by the critic, using the estimated advantage (how good an action is compared to the average).\n",
    "\n",
    "On-Policy Learning: The agent learns from the data collected by its current policy.\n",
    "Single-Step Updates: Uses one-step TD targets for updates.\n",
    "No Entropy Regularization: The policy updates do not include an entropy term to encourage exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b480390-3938-4a65-a8d6-7c2d41170c8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Policy network - deep neural network approximation of policy function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c5017-2cb7-4302-b68c-574f38686c9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Policy Network (PolicyNetwork)\n",
    "Architecture:\n",
    "    Convolutional Layers: Extract features from the image input (Atari frames).\n",
    "    Fully Connected Layers: Process the flattened features to produce logits for each possible action.\n",
    "Functionality:\n",
    "    Forward Pass: Processes input states and outputs action logits.\n",
    "    Update Method: Computes the loss using the negative log probability of the taken action multiplied by the advantage (delta), and updates the network       weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ef5f667-7bd2-4a10-96fe-5bfac9686f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Policy network for approximating policy function\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        #2D CNN layers for boxing atari image \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # Input channels = 3 (RGB), Output channels = 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        #Input size post CNN for fully connected layers\n",
    "        self.flattened_size = self._get_flattened_size(input_size)\n",
    "        # create network layers\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(self.flattened_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # output layer (preferences/logits/unnormalised log-probabilities)\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "\n",
    "        # combine layers into feed-forward network\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # select optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "##########################################################################################################################################################################################################        \n",
    "    def _get_flattened_size(self, input_size):\n",
    "        #just a function to help me with dynamically changing the CNN structure\n",
    "       with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, input_size[0], input_size[1])  # Batch size 1, C, H, W\n",
    "            conv_out = self.conv_net(dummy_input)\n",
    "            return conv_out.view(1, -1).size(1)\n",
    "##########################################################################################################################################################################################################\n",
    "    def forward(self, x):\n",
    "        #permuting input from(H,W,C) to (C,H,W) for CNN: (210, 160, 3) to (3, 210, 160)\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # return output of policy network\n",
    "        return self.net(x)\n",
    "##########################################################################################################################################################################################################\n",
    "    def update(self, state, action, advantage):\n",
    "        # update network weights for given input(s) and target(s)\n",
    "        self.optimizer.zero_grad()\n",
    "        if state.dim() == 3:\n",
    "            state = state.unsqueeze(0)\n",
    "        # State is (batch_size, H, W, C)\n",
    "        state = state.permute(0, 3, 1, 2)  # (batch_size, C, H, W)\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob * advantage\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd0cdd-8b5b-4f5b-a477-09678e301844",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Value Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc680953-3534-4bb7-82a1-299e0c96d536",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Value Network (ValueNetwork)\n",
    "Architecture:\n",
    "Similar to the policy network but outputs a single scalar value representing the state value 𝑉(𝑠).\n",
    "Functionality:\n",
    "    Forward Pass: Processes input states and outputs the estimated value.\n",
    "    Update Method: Uses mean squared error (MSE) loss between the predicted value and the TD target, and updates the network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "afa028c7-5d1b-4bed-b28d-2140273b703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, learning_rate):\n",
    "        super().__init__()\n",
    "        #2D CNN layers for boxing atari image \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # Input channels = 3 (RGB), Output channels = 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        #Input size post CNN for fully connected layers\n",
    "        self.flattened_size = self._get_flattened_size(input_size)\n",
    "        \n",
    "        # create network layers\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        layers.append(nn.Linear(self.flattened_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # hidden layers\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # output layer (there is only one unit representing state value)\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "\n",
    "        # combine layers into feed-forward network\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # select loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "##########################################################################################################################################################################################################\n",
    "    def _get_flattened_size(self, input_size):\n",
    "        #just a function to help me with dynamically changing the CNN structure\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, input_size[0], input_size[1])  # Batch size 1, C, H, W\n",
    "            conv_out = self.conv_net(dummy_input)\n",
    "            return conv_out.view(1, -1).size(1)\n",
    "##########################################################################################################################################################################################################\n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # return output of value network for the input x\n",
    "        return self.net(x)\n",
    "##########################################################################################################################################################################################################\n",
    "    def update(self, state, target):\n",
    "        # update network weights \n",
    "        self.optimizer.zero_grad()\n",
    "        if state.dim() == 3:\n",
    "            state = state.unsqueeze(0)\n",
    "        # State is (batch_size, H, W, C)\n",
    "        state = state.permute(0, 3, 1, 2)  # (batch_size, C, H, W)\n",
    "        value = self.forward(state)\n",
    "        loss = self.criterion(value, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d2fe7-4b6f-4091-912e-69b152762615",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Actor-critic class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f8512-7f3e-44a9-acda-2b7d1450447f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Training Loop (ActorCritic Class)\n",
    "Initialization:\n",
    "    Creates instances of the policy and value networks.\n",
    "    Sets up the environment and hyperparameters.\n",
    "Policy Method:\n",
    "    Chooses an action based on the current policy, either stochastically (sampling) or deterministically (argmax).\n",
    "Update Method:\n",
    "    Computes the TD target and advantage.\n",
    "    Updates both the policy and value networks using the computed loss functions.\n",
    "Training Method:\n",
    "    Runs episodes where the agent interacts with the environment.\n",
    "    After each step, updates the networks.\n",
    "    Tracks and plots the rewards over episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5547b4e8-a149-48ac-b4db-6aff153cefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic algorithm with one-step TD target\n",
    "class ActorCritic():\n",
    "    def __init__(self, env, gamma, hidden_sizes=(32, 32), lr_policy=0.001, lr_value=0.001):\n",
    "        # check if the state space has correct type\n",
    "        #continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        #assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        #Variation to accept 2D image using CNN as opposed to 1D continous statespace\n",
    "\n",
    "        self.state_dims = env.observation_space.shape\n",
    "\n",
    "        # check if the action space has correct type\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        # create policy network\n",
    "        self.policynet = PolicyNetwork(self.state_dims, hidden_sizes, self.num_actions, lr_policy)\n",
    "\n",
    "        # create value network\n",
    "        self.valuenet = ValueNetwork(self.state_dims, hidden_sizes, lr_value)\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "##########################################################################################################################################################################################################\n",
    "    def prepare_state(self, state):\n",
    "        #New introduced function to practical code it converts state to tensor with batch dimension and proper shape, just to assist policy function\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float)\n",
    "        if state.dim() == 3:\n",
    "            state = state.unsqueeze(0)  # (1, H, W, C)\n",
    "        return state\n",
    "##########################################################################################################################################################################################################\n",
    "    def policy(self, state, stochastic=True):\n",
    "        state = self.prepare_state(state)\n",
    "        state = state.permute(0, 3, 1, 2)  # (batch_size, C, H, W)\n",
    "        logits = self.policynet(state).detach()\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if stochastic:\n",
    "            # sample action using action probabilities\n",
    "            return dist.sample().item()\n",
    "        else:\n",
    "            # select action with the highest probability\n",
    "            # note: we ignore breaking ties randomly (low chance of happening)\n",
    "            return dist.probs.argmax().item()\n",
    "##########################################################################################################################################################################################################\n",
    "    def update(self, state, action, reward, next_state, terminated):\n",
    "        state = self.prepare_state(state)\n",
    "        next_state = self.prepare_state(next_state)\n",
    "        state_perm = state.permute(0, 3, 1, 2)\n",
    "        next_state_perm = next_state.permute(0, 3, 1, 2)\n",
    "        # calculate TD target for value network update\n",
    "        with torch.no_grad():\n",
    "            if terminated:\n",
    "                target = torch.tensor([[reward]], dtype=torch.float) #updated hasd issues doing it below for some reason but works here (convert target to torch format)\n",
    "            else:\n",
    "                next_state_value = self.valuenet(next_state_perm) # changed as well (convert target to torch format(\n",
    "                target = torch.tensor([[reward]], dtype=torch.float) + self.gamma * next_state_value\n",
    "                #target = reward + self.gamma*self.valuenet(next_state).detach()\n",
    "\n",
    "        # calculate TD error for policy network update (equal to the action advantage)\n",
    "        value = self.valuenet(state_perm) #broke it up to help make it easier to follow for myself and anyone reading\n",
    "        delta = target - value\n",
    "\n",
    "\n",
    "        # update networks\n",
    "        action = torch.tensor([action]) # batch dimension so slightly different to in practical\n",
    "        self.policynet.update(state, action, delta.detach()) #also detached here instead was having issues just doing it in the delta function\n",
    "        self.valuenet.update(state, target)\n",
    "##########################################################################################################################################################################################################\n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # train the agent for a number of episodes\n",
    "        num_steps = 0\n",
    "        episode_rewards = []\n",
    "        for episode in range(max_episodes):\n",
    "            state, _ = env.reset()\n",
    "\n",
    "\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_rewards.append(0)\n",
    "            while not (terminated or truncated):\n",
    "                # select action by following policy\n",
    "                action = self.policy(state)\n",
    "\n",
    "                # send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                episode_rewards[-1] += reward\n",
    "\n",
    "                # update policy and value networks\n",
    "                self.update(state, action, reward, next_state, terminated)\n",
    "\n",
    "                state = next_state\n",
    "                num_steps += 1\n",
    "\n",
    "            print(f'\\rEpisode {episode+1} done: steps = {num_steps}, '\n",
    "                  f'rewards = {episode_rewards[episode]}     ', end='')\n",
    "\n",
    "            if episode >= criterion_episodes-1 and stop_criterion(episode_rewards[-criterion_episodes:]):\n",
    "                print(f'\\nStopping criterion satisfied after {episode} episodes')\n",
    "                break\n",
    "\n",
    "        # plot rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(episode_rewards)+1), episode_rewards, label=f'Rewards')\n",
    "\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards per episode')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "##########################################################################################################################################################################################################\n",
    "    def save(self, path):\n",
    "        # save network weights to a file\n",
    "        torch.save({'policy': self.policynet.state_dict(),\n",
    "                    'value': self.valuenet.state_dict()}, path)\n",
    "##########################################################################################################################################################################################################\n",
    "    def load(self, path):\n",
    "        # load network weights from a file\n",
    "        networks = torch.load(path)\n",
    "        self.policynet.load_state_dict(networks['policy'])\n",
    "        self.valuenet.load_state_dict(networks['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40ab9a-5043-4287-8e79-3c1a2bbddceb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Boxing ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fc608-3efc-4be6-95d1-09f37c757d77",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3452 done: steps = 6165272, rewards = -25.0     "
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\")\n",
    "\n",
    "gamma = 0.99\n",
    "hidden_sizes = (64, 64)\n",
    "lr_policy = 0.001\n",
    "lr_value = 0.005\n",
    "max_episodes = 20000\n",
    "max_steps = 2000\n",
    "criterion_episodes = 100\n",
    "\n",
    "agent = ActorCritic(env, gamma=gamma, hidden_sizes=hidden_sizes, lr_policy=lr_policy, lr_value=lr_value)\n",
    "\n",
    "#agent.load('cartpole.64x64.AC.pt')\n",
    "agent.train(max_episodes, lambda x : min(x) >= 400, criterion_episodes)\n",
    "\n",
    "# visualise one episode\n",
    "state, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "while not (terminated or truncated or steps > max_steps):\n",
    "    # take action based on policy\n",
    "    action = agent.policy(state, stochastic=False)\n",
    "\n",
    "    # environment receives the action and returns:\n",
    "    # next observation, reward, terminated, truncated, and additional information (if applicable)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "\n",
    "print(f'Reward: {total_reward}')\n",
    "\n",
    "# store RGB frames for the entire episode\n",
    "frames = env.render()\n",
    "\n",
    "# close the environment\n",
    "env.close()\n",
    "\n",
    "# create and play video clip using the frames and given fps\n",
    "clip = mpy.ImageSequenceClip(frames, fps=50)\n",
    "clip.ipython_display(rd_kwargs=dict(logger=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bff5e37-1fc3-4b72-b548-437e9830c786",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Soft Actor-Critic (SAC) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910071c9-b92d-4191-9693-d464fe93689b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Soft Actor-Critic (SAC) is an off-policy actor-critic algorithm that aims to maximize a trade-off between expected return and entropy. It encourages exploration by adding an entropy term to the objective function, making the policy stochastic even in deterministic environments.\n",
    "\n",
    "Off-Policy Learning: Uses a replay buffer to store experiences and sample from it.\n",
    "Entropy Regularization: Encourages the policy to explore more by maximizing the expected entropy of the policy.\n",
    "Twin Q-Networks: Uses two Q-value networks to mitigate overestimation bias.\n",
    "Target Networks: Uses target networks to stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f0396-2da7-4529-8272-7fcb0606ef22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Atari boxing is a discrete action spcae thus, need to make alterations for SAC which is designed for continuous action spaces.\n",
    "Alterations:\n",
    "Implement rwo Q-networks: mitigate overestimation bias.\n",
    "Modify the policy network: tooutput a categorical distribution and include entropy in the loss function.\n",
    "Use a replay buffer: for off-policy learning.\n",
    "Add target networks: Ffor both Q-networks.\n",
    "Include entropy coefficient (𝛼): fixed or learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb2747-41ed-4479-adf3-56f0a0ec1144",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Soft Actor-Critic (SAC) RAM Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9854189e-232b-4be6-9455-0b073fef4552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T12:07:02.206039Z",
     "iopub.status.busy": "2024-10-15T12:07:02.205812Z",
     "iopub.status.idle": "2024-10-15T12:07:02.209966Z",
     "shell.execute_reply": "2024-10-15T12:07:02.209650Z",
     "shell.execute_reply.started": "2024-10-15T12:07:02.206020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9fbaf67f-aca7-4313-ae9b-d7a46684192d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T12:45:02.553442Z",
     "iopub.status.busy": "2024-10-15T12:45:02.553284Z",
     "iopub.status.idle": "2024-10-15T12:45:02.557370Z",
     "shell.execute_reply": "2024-10-15T12:45:02.557082Z",
     "shell.execute_reply.started": "2024-10-15T12:45:02.553430Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNetworkRAM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        # Define network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        # Output layer for logits\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output logits for Categorical distribution\n",
    "        return self.net(x)\n",
    "    \n",
    "    def update_policy(self, states, actions, q_values):\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.forward(states)  # [batch_size, num_actions]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions.squeeze(1))  # [batch_size]\n",
    "        entropy = dist.entropy()  # [batch_size]\n",
    "        \n",
    "        # Ensure q_values require gradients\n",
    "        q_values = q_values.detach()\n",
    "        \n",
    "        # Policy loss with entropy regularization\n",
    "        policy_loss = (self.alpha * log_probs - q_values).mean()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Calculate gradient norms for logging\n",
    "        total_norm = 0\n",
    "        for p in self.net.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = math.sqrt(total_norm)\n",
    "        \n",
    "        return policy_loss.item(), total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9911c11b-fcb5-4550-b137-0c681a3e001b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T12:45:03.885230Z",
     "iopub.status.busy": "2024-10-15T12:45:03.884879Z",
     "iopub.status.idle": "2024-10-15T12:45:03.888792Z",
     "shell.execute_reply": "2024-10-15T12:45:03.888476Z",
     "shell.execute_reply.started": "2024-10-15T12:45:03.885217Z"
    }
   },
   "outputs": [],
   "source": [
    "class QNetworkRAM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_actions, learning_rate):\n",
    "        super().__init__()\n",
    "        # Define network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        # Output layer for Q-values\n",
    "        layers.append(nn.Linear(last_size, num_actions))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output Q-values for each action\n",
    "        return self.net(x)  # [batch_size, num_actions]\n",
    "    \n",
    "    def update_q_network(self, parameters, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters, max_norm=1.0)\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04050921-41fb-4bb4-9bb9-768721f04a1f",
   "metadata": {},
   "source": [
    "Added replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91a5747d-b9f5-4c8f-9bc0-5363cb7c2ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T12:44:00.191110Z",
     "iopub.status.busy": "2024-10-15T12:44:00.190824Z",
     "iopub.status.idle": "2024-10-15T12:44:00.194975Z",
     "shell.execute_reply": "2024-10-15T12:44:00.194703Z",
     "shell.execute_reply.started": "2024-10-15T12:44:00.191097Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = zip(*samples)\n",
    "        \n",
    "        # Convert lists to single numpy arrays for efficiency\n",
    "        state_batch = np.array(state_batch)\n",
    "        action_batch = np.array(action_batch)\n",
    "        reward_batch = np.array(reward_batch)\n",
    "        next_state_batch = np.array(next_state_batch)\n",
    "        terminated_batch = np.array(terminated_batch)\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)  # [batch_size, state_dims]\n",
    "        action_batch = torch.LongTensor(action_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)  # [batch_size, state_dims]\n",
    "        terminated_batch = torch.FloatTensor(terminated_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "508e96d5-6da9-4d66-b5f6-0554e963f370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T12:45:40.036392Z",
     "iopub.status.busy": "2024-10-15T12:45:40.036236Z",
     "iopub.status.idle": "2024-10-15T12:45:40.051209Z",
     "shell.execute_reply": "2024-10-15T12:45:40.050913Z",
     "shell.execute_reply.started": "2024-10-15T12:45:40.036380Z"
    }
   },
   "outputs": [],
   "source": [
    "class SACAgentRAM:\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.2, hidden_sizes=(256, 256),\n",
    "                 lr_policy=1e-4, lr_q=1e-3, replay_size=100000, batch_size=64,\n",
    "                 target_update_interval=1, tau=0.005, automatic_alpha=True):\n",
    "        # Validate observation space\n",
    "        continuous = isinstance(env.observation_space, spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "        #print(f\"State dimensions: {self.state_dims}\")\n",
    "    \n",
    "        # Validate action space\n",
    "        assert isinstance(env.action_space, spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "        #print(f\"Number of actions: {self.num_actions}\")\n",
    "        \n",
    "        # Initialize Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        \n",
    "        # Initialize Policy Network\n",
    "        self.policy_net = PolicyNetworkRAM(self.state_dims, hidden_sizes, self.num_actions, lr_policy, alpha).to(device)\n",
    "        \n",
    "        # Initialize Q-Networks\n",
    "        self.q_net1 = QNetworkRAM(self.state_dims, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        self.q_net2 = QNetworkRAM(self.state_dims, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        \n",
    "        # Initialize Target Q-Networks\n",
    "        self.target_q_net1 = QNetworkRAM(self.state_dims, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        self.target_q_net2 = QNetworkRAM(self.state_dims, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        \n",
    "        # Synchronize target networks with main Q-networks\n",
    "        self.target_q_net1.load_state_dict(self.q_net1.state_dict())\n",
    "        self.target_q_net2.load_state_dict(self.q_net2.state_dict())\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Entropy temperature\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau  # Target smoothing coefficient\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.automatic_alpha = automatic_alpha\n",
    "        self.warm_up(initial_steps=1000)\n",
    "        if self.automatic_alpha:\n",
    "            # target entropy\n",
    "            self.target_entropy = -math.log(1.0 / self.num_actions) * 0.98  # Slightly lower than maximum\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "    \n",
    "            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr_policy)\n",
    "            #print(\"Automatic alpha tuning enabled.\")\n",
    "##########################################################################################################################################################################################################    \n",
    "    def warm_up(self, initial_steps=1000):\n",
    "        state, _ = self.env.reset()\n",
    "        for _ in range(initial_steps):\n",
    "            action = self.env.action_space.sample()  # Take random action\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            reward = np.clip(reward, -1, 1)  # Clip rewards\n",
    "            self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "                \n",
    "  ##########################################################################################################################################################################################################  \n",
    "    def policy(self, state, evaluate=False):\n",
    "        #state = self.normalize_state(state)\n",
    "        logits = self.policy_net(state)  # [1, num_actions]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if evaluate:\n",
    "            action = dist.probs.argmax(dim=1).item()\n",
    "        else:\n",
    "            action = dist.sample().item()\n",
    "        # Validate action\n",
    "        if not (0 <= action < self.num_actions):\n",
    "            raise ValueError(f\"Sampled action {action} is out of bounds.\")\n",
    "        \n",
    "        # Log action probabilities for debugging\n",
    "        action_probs = dist.probs.detach().cpu().numpy()[0]\n",
    "        wandb.log({\n",
    "            'action_probabilities': wandb.Histogram(action_probs),\n",
    "            'selected_action': action\n",
    "        })\n",
    "        \n",
    "        return action\n",
    " ##########################################################################################################################################################################################################   \n",
    "    def update(self, updates):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None, None, None, None  #  none for all values when not updating\n",
    "    \n",
    "        # samples a minibatch from replay buffer\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = self.replay_buffer.sample(self.batch_size)\n",
    "    \n",
    "    \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            # Get logits and probabilities from the policy network for next states\n",
    "            next_logits = self.policy_net(next_state_batch)\n",
    "            next_probs = F.softmax(next_logits, dim=1)  # [batch_size, num_actions]\n",
    "            next_log_probs = F.log_softmax(next_logits, dim=1)  # [batch_size, num_actions]\n",
    "    \n",
    "            # Get target q-values from target networks\n",
    "            target_q1_values = self.target_q_net1(next_state_batch)  # [batch_size, num_actions]\n",
    "            target_q2_values = self.target_q_net2(next_state_batch)  # [batch_size, num_actions]\n",
    "            target_q_values = torch.min(target_q1_values, target_q2_values)  # [batch_size, num_actions]\n",
    "    \n",
    "            # Compute expected q-values for next states\n",
    "            expected_q = (next_probs * (target_q_values - self.alpha * next_log_probs)).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "    \n",
    "            # Compute target q\n",
    "            q_target = reward_batch + (1 - terminated_batch) * self.gamma * expected_q  # [batch_size, 1]\n",
    "    \n",
    "        # Compute current Q-values from both Q-networks\n",
    "        current_q1 = self.q_net1(state_batch).gather(1, action_batch)  # [batch_size, 1]\n",
    "        current_q2 = self.q_net2(state_batch).gather(1, action_batch)  # [batch_size, 1]\n",
    "\n",
    "\n",
    "        current_q1 = torch.clamp(current_q1, min=-100, max=100)\n",
    "        current_q2 = torch.clamp(current_q2, min=-100, max=100)\n",
    "    \n",
    "        # Compute q-network losses\n",
    "        q1_loss = self.q_net1.criterion(current_q1, q_target)\n",
    "        q2_loss = self.q_net2.criterion(current_q2, q_target)\n",
    "    \n",
    "        # Update q-networks\n",
    "        q1_loss_value = self.q_net1.update_q_network(self.q_net1.parameters(), q1_loss)\n",
    "        q2_loss_value = self.q_net2.update_q_network(self.q_net2.parameters(), q2_loss)\n",
    "        policy_loss_value, grad_norm = self.policy_net.update_policy(state_batch, action_batch, torch.min(self.q_net1(state_batch), self.q_net2(state_batch)).gather(1, action_batch).squeeze(1))\n",
    "        # Compute policy loss\n",
    "        with torch.no_grad():\n",
    "            entropy = F.softmax(self.policy_net(state_batch), dim=1) * F.log_softmax(self.policy_net(state_batch), dim=1)\n",
    "            entropy = -entropy.sum(dim=1).mean().item()\n",
    "\n",
    "        #  Update alpha for entropy temperature added because my first implementation sucked\n",
    "        if self.automatic_alpha:\n",
    "            #log_probs for entropy adjustment\n",
    "            logits = self.policy_net(state_batch)  # [batch_size, num_actions]\n",
    "            log_probs = F.log_softmax(logits, dim=1)  # [batch_size, num_actions]\n",
    "            entropy = - (log_probs * F.softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "            alpha_loss = -(self.log_alpha * (entropy - self.target_entropy)).mean()\n",
    "\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "\n",
    "            # clamp alpha to prevent it from becoming too small\n",
    "            self.alpha = torch.clamp(self.log_alpha.exp(), min=0.01).detach()\n",
    "            print(f\"Updated alpha: {self.alpha.item():.4f}, Alpha Loss: {alpha_loss.item():.4f}\")\n",
    "\n",
    "        # update target networks\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            self.soft_update(self.q_net1, self.target_q_net1)\n",
    "            self.soft_update(self.q_net2, self.target_q_net2)\n",
    "            print(\"Target networks updated.\")\n",
    "\n",
    "        return q1_loss_value, q2_loss_value, policy_loss_value, entropy\n",
    "\n",
    "##########################################################################################################################################################################################################\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "##########################################################################################################################################################################################################    \n",
    "    def train_agent(self, num_episodes, stop_criterion, criterion_episodes):\n",
    "        total_rewards = []\n",
    "        updates = 0\n",
    "        best_avg_reward = -float('inf')\n",
    "\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            while not (terminated or truncated) and steps < 10000:  # infinite loops preventing\n",
    "                action = self.policy(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Store transition in replay buffer\n",
    "                self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "                \n",
    "                # Update state\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                # Update networks\n",
    "                update_results = self.update(updates)\n",
    "                if update_results[0] is not None:\n",
    "                    q1_loss, q2_loss, policy_loss, entropy = update_results\n",
    "                    updates += 1\n",
    "                else:\n",
    "                    q1_loss, q2_loss, policy_loss, entropy = (None, None, None, None)\n",
    "\n",
    "                \n",
    "            # Append reward for this episode\n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode}: Total Reward = {episode_reward}, Steps = {steps}\")\n",
    "            q1_loss, q2_loss, policy_loss, entropy = self.update(updates)\n",
    "            # Log metrics to W&B\n",
    "            log_dict = {\n",
    "                'episode': episode,\n",
    "                'episode_reward': episode_reward,\n",
    "                'steps': steps,\n",
    "                'updates': updates,\n",
    "                'alpha': self.alpha.item() if self.automatic_alpha else self.alpha,\n",
    "            }\n",
    "            if q1_loss is not None:\n",
    "                log_dict.update({\n",
    "                    'q1_loss': q1_loss,\n",
    "                    'q2_loss': q2_loss,\n",
    "                    'policy_loss': policy_loss,\n",
    "                    'entropy': entropy,\n",
    "                })\n",
    "\n",
    "            # Log metrics to W&B\n",
    "            wandb.log(log_dict)\n",
    "            \n",
    "            # Check stopping criterion\n",
    "            if episode >= criterion_episodes:\n",
    "                recent_rewards = total_rewards[-criterion_episodes:]\n",
    "                if stop_criterion(recent_rewards):\n",
    "                    print(f\"\\nStopping criterion satisfied after {episode} episodes\")\n",
    "                    break\n",
    "        \n",
    "        # Plot rewards\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(total_rewards)+1), total_rewards, label='Total Rewards')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model parameters.\n",
    "        Args:\n",
    "            path (str): File path to save the model.\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'q_net1': self.q_net1.state_dict(),\n",
    "            'q_net2': self.q_net2.state_dict(),\n",
    "            'target_q_net1': self.target_q_net1.state_dict(),\n",
    "            'target_q_net2': self.target_q_net2.state_dict(),\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Load the model parameters.\n",
    "        Args:\n",
    "            path (str): File path to load the model from.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.q_net1.load_state_dict(checkpoint['q_net1'])\n",
    "        self.q_net2.load_state_dict(checkpoint['q_net2'])\n",
    "        self.target_q_net1.load_state_dict(checkpoint['target_q_net1'])\n",
    "        self.target_q_net2.load_state_dict(checkpoint['target_q_net2'])\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a5499ce-e853-4354-933a-55ea9f791c9a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-10-15T12:45:40.441276Z",
     "iopub.status.busy": "2024-10-15T12:45:40.441112Z",
     "iopub.status.idle": "2024-10-15T13:13:22.387026Z",
     "shell.execute_reply": "2024-10-15T13:13:22.386682Z",
     "shell.execute_reply.started": "2024-10-15T12:45:40.441263Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Shape: (128,)\n",
      "Action Space: Discrete(18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6zejlson) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.137 MB of 0.137 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>▁</td></tr><tr><td>entropy</td><td>▁</td></tr><tr><td>episode</td><td>▁</td></tr><tr><td>episode_reward</td><td>▁</td></tr><tr><td>policy_loss</td><td>▁</td></tr><tr><td>q1_loss</td><td>▁</td></tr><tr><td>q2_loss</td><td>▁</td></tr><tr><td>steps</td><td>▁</td></tr><tr><td>updates</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>0.8415</td></tr><tr><td>entropy</td><td>0</td></tr><tr><td>episode</td><td>1</td></tr><tr><td>episode_reward</td><td>-22</td></tr><tr><td>policy_loss</td><td>-11292.90625</td></tr><tr><td>q1_loss</td><td>0.76257</td></tr><tr><td>q2_loss</td><td>0.04202</td></tr><tr><td>steps</td><td>1786</td></tr><tr><td>updates</td><td>1723</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SAC_Discrete_RAM_Run</strong> at: <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/6zejlson' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/6zejlson</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241015_204511-6zejlson/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6zejlson). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241015_204540-2cds1q84</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/2cds1q84' target=\"_blank\">SAC_Discrete_RAM_Run</a></strong> to <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/2cds1q84' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/2cds1q84</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -54.0, Steps = 1786\n",
      "Episode 2: Total Reward = -54.0, Steps = 1786\n",
      "Episode 3: Total Reward = -54.0, Steps = 1786\n",
      "Episode 4: Total Reward = -54.0, Steps = 1786\n",
      "Episode 5: Total Reward = -54.0, Steps = 1786\n",
      "Episode 6: Total Reward = -54.0, Steps = 1786\n",
      "Episode 7: Total Reward = -54.0, Steps = 1786\n",
      "Episode 8: Total Reward = -54.0, Steps = 1786\n",
      "Episode 9: Total Reward = -54.0, Steps = 1786\n",
      "Episode 10: Total Reward = -54.0, Steps = 1786\n",
      "Episode 11: Total Reward = -54.0, Steps = 1786\n",
      "Episode 12: Total Reward = -54.0, Steps = 1786\n",
      "Episode 13: Total Reward = -54.0, Steps = 1786\n",
      "Episode 14: Total Reward = -54.0, Steps = 1786\n",
      "Episode 15: Total Reward = -54.0, Steps = 1786\n",
      "Episode 16: Total Reward = -54.0, Steps = 1786\n",
      "Episode 17: Total Reward = -54.0, Steps = 1786\n",
      "Episode 18: Total Reward = -54.0, Steps = 1786\n",
      "Episode 19: Total Reward = -54.0, Steps = 1786\n",
      "Episode 20: Total Reward = -54.0, Steps = 1786\n",
      "Episode 21: Total Reward = -54.0, Steps = 1786\n",
      "Episode 22: Total Reward = -54.0, Steps = 1786\n",
      "Episode 23: Total Reward = -54.0, Steps = 1786\n",
      "Episode 24: Total Reward = -54.0, Steps = 1786\n",
      "Episode 25: Total Reward = -54.0, Steps = 1786\n",
      "Episode 26: Total Reward = -54.0, Steps = 1786\n",
      "Episode 27: Total Reward = -54.0, Steps = 1786\n",
      "Episode 28: Total Reward = -54.0, Steps = 1786\n",
      "Episode 29: Total Reward = -54.0, Steps = 1786\n",
      "Episode 30: Total Reward = -54.0, Steps = 1786\n",
      "Episode 31: Total Reward = -54.0, Steps = 1786\n",
      "Episode 32: Total Reward = -54.0, Steps = 1786\n",
      "Episode 33: Total Reward = -54.0, Steps = 1786\n",
      "Episode 34: Total Reward = -54.0, Steps = 1786\n",
      "Episode 35: Total Reward = -54.0, Steps = 1786\n",
      "Episode 36: Total Reward = -54.0, Steps = 1786\n",
      "Episode 37: Total Reward = -54.0, Steps = 1786\n",
      "Episode 38: Total Reward = -54.0, Steps = 1786\n",
      "Episode 39: Total Reward = -54.0, Steps = 1786\n",
      "Episode 40: Total Reward = -54.0, Steps = 1786\n",
      "Episode 41: Total Reward = -54.0, Steps = 1786\n",
      "Episode 42: Total Reward = -54.0, Steps = 1786\n",
      "Episode 43: Total Reward = -54.0, Steps = 1786\n",
      "Episode 44: Total Reward = -54.0, Steps = 1786\n",
      "Episode 45: Total Reward = -54.0, Steps = 1786\n",
      "Episode 46: Total Reward = -54.0, Steps = 1786\n",
      "Episode 47: Total Reward = -54.0, Steps = 1786\n",
      "Episode 48: Total Reward = -54.0, Steps = 1786\n",
      "Episode 49: Total Reward = -54.0, Steps = 1786\n",
      "Episode 50: Total Reward = -54.0, Steps = 1786\n",
      "Episode 51: Total Reward = -54.0, Steps = 1786\n",
      "Episode 52: Total Reward = -54.0, Steps = 1786\n",
      "Episode 53: Total Reward = -54.0, Steps = 1786\n",
      "Episode 54: Total Reward = -54.0, Steps = 1786\n",
      "Episode 55: Total Reward = -54.0, Steps = 1786\n",
      "Episode 56: Total Reward = -54.0, Steps = 1786\n",
      "Episode 57: Total Reward = -54.0, Steps = 1786\n",
      "Episode 58: Total Reward = -54.0, Steps = 1786\n",
      "Episode 59: Total Reward = -54.0, Steps = 1786\n",
      "Episode 60: Total Reward = -54.0, Steps = 1786\n",
      "Episode 61: Total Reward = -54.0, Steps = 1786\n",
      "Episode 62: Total Reward = -54.0, Steps = 1786\n",
      "Episode 63: Total Reward = -54.0, Steps = 1786\n",
      "Episode 64: Total Reward = -54.0, Steps = 1786\n",
      "Episode 65: Total Reward = -54.0, Steps = 1786\n",
      "Episode 66: Total Reward = -54.0, Steps = 1786\n",
      "Episode 67: Total Reward = -54.0, Steps = 1786\n",
      "Episode 68: Total Reward = -54.0, Steps = 1786\n",
      "Episode 69: Total Reward = -54.0, Steps = 1786\n",
      "Episode 70: Total Reward = -54.0, Steps = 1786\n",
      "Episode 71: Total Reward = -54.0, Steps = 1786\n",
      "Episode 72: Total Reward = -54.0, Steps = 1786\n",
      "Episode 73: Total Reward = -54.0, Steps = 1786\n",
      "Episode 74: Total Reward = -54.0, Steps = 1786\n",
      "Episode 75: Total Reward = -54.0, Steps = 1786\n",
      "Episode 76: Total Reward = -54.0, Steps = 1786\n",
      "Episode 77: Total Reward = -54.0, Steps = 1786\n",
      "Episode 78: Total Reward = -54.0, Steps = 1786\n",
      "Episode 79: Total Reward = -54.0, Steps = 1786\n",
      "Episode 80: Total Reward = -54.0, Steps = 1786\n",
      "Episode 81: Total Reward = -54.0, Steps = 1786\n",
      "Episode 82: Total Reward = -54.0, Steps = 1786\n",
      "Episode 83: Total Reward = -54.0, Steps = 1786\n",
      "Episode 84: Total Reward = -54.0, Steps = 1786\n",
      "Episode 85: Total Reward = -54.0, Steps = 1786\n",
      "Episode 86: Total Reward = -54.0, Steps = 1786\n",
      "Episode 87: Total Reward = -54.0, Steps = 1786\n",
      "Episode 88: Total Reward = -54.0, Steps = 1786\n",
      "Episode 89: Total Reward = -54.0, Steps = 1786\n",
      "Episode 90: Total Reward = -54.0, Steps = 1786\n",
      "Episode 91: Total Reward = -54.0, Steps = 1786\n",
      "Episode 92: Total Reward = -54.0, Steps = 1786\n",
      "Episode 93: Total Reward = -54.0, Steps = 1786\n",
      "Episode 94: Total Reward = -54.0, Steps = 1786\n",
      "Episode 95: Total Reward = -54.0, Steps = 1786\n",
      "Episode 96: Total Reward = -54.0, Steps = 1786\n",
      "Episode 97: Total Reward = -54.0, Steps = 1786\n",
      "Episode 98: Total Reward = -54.0, Steps = 1786\n",
      "Episode 99: Total Reward = -54.0, Steps = 1786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_878795/236495212.py:230: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Total Reward = -54.0, Steps = 1786\n",
      "Model saved to sac_discrete_boxing_ram.pth\n",
      "Reward: -54.0, Steps: 1786\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "print(\"Action Space:\", env.action_space)\n",
    "env = NormalizeObservation(env)\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "hidden_sizes = (256, 256)\n",
    "lr_policy = 1e-4\n",
    "lr_q = 1e-4\n",
    "replay_size = 100000\n",
    "batch_size = 64\n",
    "target_update_interval = 1\n",
    "tau = 0.005\n",
    "num_episodes = 100\n",
    "criterion_episodes = 5\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project='SAC_Discrete_RAM', name='SAC_Discrete_RAM_Run', config={\n",
    "    'gamma': gamma,\n",
    "    'lr_q': lr_q,\n",
    "    'lr_policy': lr_policy,\n",
    "    'alpha': alpha,\n",
    "    'tau': tau,\n",
    "    'replay_size': replay_size,\n",
    "    'batch_size': batch_size,\n",
    "    'target_update_interval': target_update_interval,\n",
    "    'num_episodes': num_episodes,\n",
    "    'hidden_sizes': hidden_sizes,\n",
    "    'criterion_episodes': criterion_episodes\n",
    "})\n",
    "\n",
    "# Initialize the SAC agent\n",
    "agent = SACAgentRAM(env, gamma, alpha, hidden_sizes, lr_policy, lr_q, replay_size, batch_size,\n",
    "                   target_update_interval, tau, automatic_alpha=True)\n",
    "\n",
    "# Start training\n",
    "agent.train_agent(\n",
    "    num_episodes=num_episodes,\n",
    "    stop_criterion=lambda rewards: np.mean(rewards) >= 100,\n",
    "    criterion_episodes=criterion_episodes\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "agent.save_model('sac_discrete_boxing_ram.pth')\n",
    "\n",
    "# Visualization: Run one episode and record frames\n",
    "state, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not (terminated or truncated) and steps < 10000:\n",
    "    # Select action based on policy\n",
    "    action = agent.policy(state, evaluate=True)\n",
    "    \n",
    "    # Take action in the environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    frames.append(env.render())\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    state = next_state\n",
    "\n",
    "print(f'Reward: {total_reward}, Steps: {steps}')\n",
    "\n",
    "\n",
    "# store RGB frames for the entire episode\n",
    "#frames = env.render()\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6f668-ae41-4f62-ad96-8737a40dd871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T13:29:25.350951Z",
     "iopub.status.busy": "2024-10-15T13:29:25.350678Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to render progress bar, see the user log for details\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated alpha: 0.8542, Alpha Loss: -0.4456An error occurred: Socket operation on non-socket\n",
      "Using device: cuda\n",
      "Create sweep with ID: tc9vx463\n",
      "Sweep URL: https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hd3x4ms8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.37577847603438974\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tautomatic_alpha: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion_episodes: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9590821558092576\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_sizes: (256,256)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_policy: 0.0002718167665946118\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_q: 0.00025261350637357247\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_episodes: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_size: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_update_interval: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttau: 0.004966244006524133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mhearty-sweep-1\u001b[0m at: \u001b[34mhttps://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/mickt0ee\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241015_212814-mickt0ee/logs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241015_212927-hd3x4ms8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/hd3x4ms8' target=\"_blank\">graceful-sweep-1</a></strong> to <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/hd3x4ms8' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/hd3x4ms8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Shape: (128,)\n",
      "Action Space: Discrete(18)\n",
      "State dimensions: 128\n",
      "Number of actions: 18\n",
      "Automatic alpha tuning enabled.\n",
      "Episode 1: Reward = -49.0, Steps = 1521\n",
      "Episode 2: Reward = -50.0, Steps = 1437\n",
      "Episode 3: Reward = -50.0, Steps = 1437\n",
      "Episode 4: Reward = -50.0, Steps = 1437\n",
      "Episode 5: Reward = -50.0, Steps = 1437\n",
      "Model saved to sac_discrete_boxing_ram_best_episode_5.pth\n",
      "New best average reward: -49.80. Model saved.\n",
      "Episode 6: Reward = -50.0, Steps = 1437\n",
      "Episode 7: Reward = -50.0, Steps = 1437\n",
      "Episode 8: Reward = -50.0, Steps = 1437\n",
      "Episode 9: Reward = -50.0, Steps = 1437\n",
      "Episode 10: Reward = -50.0, Steps = 1437\n",
      "Episode 11: Reward = -50.0, Steps = 1437\n",
      "Episode 12: Reward = -50.0, Steps = 1437\n",
      "Episode 13: Reward = -50.0, Steps = 1437\n",
      "Episode 14: Reward = -50.0, Steps = 1437\n",
      "Episode 15: Reward = -50.0, Steps = 1437\n",
      "Episode 16: Reward = -50.0, Steps = 1436\n",
      "Episode 17: Reward = -50.0, Steps = 1437\n",
      "Episode 18: Reward = -50.0, Steps = 1437\n",
      "Episode 19: Reward = -50.0, Steps = 1437\n",
      "Episode 20: Reward = -50.0, Steps = 1437\n",
      "Episode 21: Reward = -50.0, Steps = 1437\n",
      "Episode 22: Reward = -50.0, Steps = 1437\n",
      "Episode 23: Reward = -50.0, Steps = 1437\n",
      "Episode 24: Reward = -50.0, Steps = 1437\n",
      "Episode 25: Reward = -50.0, Steps = 1437\n",
      "Episode 26: Reward = -50.0, Steps = 1437\n",
      "Episode 27: Reward = -50.0, Steps = 1437\n",
      "Episode 28: Reward = -50.0, Steps = 1436\n",
      "Episode 29: Reward = -50.0, Steps = 1437\n",
      "Episode 30: Reward = -50.0, Steps = 1437\n",
      "Episode 31: Reward = -50.0, Steps = 1437\n",
      "Episode 32: Reward = -50.0, Steps = 1437\n",
      "Episode 33: Reward = -50.0, Steps = 1437\n",
      "Episode 34: Reward = -50.0, Steps = 1437\n",
      "Episode 35: Reward = -50.0, Steps = 1437\n",
      "Episode 36: Reward = -50.0, Steps = 1437\n",
      "Episode 37: Reward = -50.0, Steps = 1437\n",
      "Episode 38: Reward = -50.0, Steps = 1437\n",
      "Episode 39: Reward = -50.0, Steps = 1437\n",
      "Episode 40: Reward = -50.0, Steps = 1437\n",
      "Episode 41: Reward = -50.0, Steps = 1436\n",
      "Episode 42: Reward = -50.0, Steps = 1437\n",
      "Episode 43: Reward = -50.0, Steps = 1437\n",
      "Episode 44: Reward = -50.0, Steps = 1437\n",
      "Episode 45: Reward = -50.0, Steps = 1437\n",
      "Episode 46: Reward = -50.0, Steps = 1437\n",
      "Episode 47: Reward = -50.0, Steps = 1437\n",
      "Episode 48: Reward = -50.0, Steps = 1437\n",
      "Episode 49: Reward = -50.0, Steps = 1437\n",
      "Episode 50: Reward = -50.0, Steps = 1437\n",
      "Episode 51: Reward = -50.0, Steps = 1436\n",
      "Episode 52: Reward = -50.0, Steps = 1437\n",
      "Episode 53: Reward = -50.0, Steps = 1437\n",
      "Episode 54: Reward = -50.0, Steps = 1436\n",
      "Episode 55: Reward = -50.0, Steps = 1437\n",
      "Episode 56: Reward = -50.0, Steps = 1437\n",
      "Episode 57: Reward = -50.0, Steps = 1437\n",
      "Episode 58: Reward = -50.0, Steps = 1437\n",
      "Episode 59: Reward = -50.0, Steps = 1437\n",
      "Episode 60: Reward = -50.0, Steps = 1437\n",
      "Episode 61: Reward = -50.0, Steps = 1436\n",
      "Episode 62: Reward = -50.0, Steps = 1437\n",
      "Episode 63: Reward = -50.0, Steps = 1437\n",
      "Episode 64: Reward = -50.0, Steps = 1437\n",
      "Episode 65: Reward = -50.0, Steps = 1437\n",
      "Episode 66: Reward = -50.0, Steps = 1437\n",
      "Episode 67: Reward = -50.0, Steps = 1437\n",
      "Episode 68: Reward = -50.0, Steps = 1437\n",
      "Episode 69: Reward = -50.0, Steps = 1437\n",
      "Episode 70: Reward = -50.0, Steps = 1437\n",
      "Episode 71: Reward = -50.0, Steps = 1437\n",
      "Episode 72: Reward = -50.0, Steps = 1437\n",
      "Episode 73: Reward = -50.0, Steps = 1437\n",
      "Episode 74: Reward = -50.0, Steps = 1437\n",
      "Episode 75: Reward = -50.0, Steps = 1437\n",
      "Episode 76: Reward = -50.0, Steps = 1437\n",
      "Episode 77: Reward = -50.0, Steps = 1437\n",
      "Episode 78: Reward = -50.0, Steps = 1436\n",
      "Episode 79: Reward = -50.0, Steps = 1437\n",
      "Episode 80: Reward = -50.0, Steps = 1437\n",
      "Episode 81: Reward = -50.0, Steps = 1437\n",
      "Episode 82: Reward = -50.0, Steps = 1437\n",
      "Episode 83: Reward = -50.0, Steps = 1437\n",
      "Episode 84: Reward = -50.0, Steps = 1437\n",
      "Episode 85: Reward = -50.0, Steps = 1437\n",
      "Episode 86: Reward = -50.0, Steps = 1437\n",
      "Episode 87: Reward = -50.0, Steps = 1437\n",
      "Episode 88: Reward = -50.0, Steps = 1436\n",
      "Episode 89: Reward = -50.0, Steps = 1437\n",
      "Episode 90: Reward = -50.0, Steps = 1437\n",
      "Episode 91: Reward = -50.0, Steps = 1437\n",
      "Episode 92: Reward = -50.0, Steps = 1437\n",
      "Episode 93: Reward = -50.0, Steps = 1437\n",
      "Episode 94: Reward = -50.0, Steps = 1437\n",
      "Episode 95: Reward = -50.0, Steps = 1437\n",
      "Episode 96: Reward = -50.0, Steps = 1437\n",
      "Episode 97: Reward = -50.0, Steps = 1437\n",
      "Episode 98: Reward = -50.0, Steps = 1437\n",
      "Episode 99: Reward = -50.0, Steps = 1437\n",
      "Episode 100: Reward = -50.0, Steps = 1437\n",
      "Episode 101: Reward = -50.0, Steps = 1437\n",
      "Episode 102: Reward = -50.0, Steps = 1437\n",
      "Episode 103: Reward = -50.0, Steps = 1437\n",
      "Episode 104: Reward = -50.0, Steps = 1437\n",
      "Episode 105: Reward = -50.0, Steps = 1437\n",
      "Episode 106: Reward = -50.0, Steps = 1437\n",
      "Episode 107: Reward = -50.0, Steps = 1436\n",
      "Episode 108: Reward = -50.0, Steps = 1437\n",
      "Episode 109: Reward = -50.0, Steps = 1437\n",
      "Episode 110: Reward = -50.0, Steps = 1437\n",
      "Episode 111: Reward = -50.0, Steps = 1437\n",
      "Episode 112: Reward = -50.0, Steps = 1436\n",
      "Episode 113: Reward = -50.0, Steps = 1437\n",
      "Episode 114: Reward = -50.0, Steps = 1437\n",
      "Episode 115: Reward = -50.0, Steps = 1437\n",
      "Episode 116: Reward = -50.0, Steps = 1437\n",
      "Episode 117: Reward = -50.0, Steps = 1437\n",
      "Episode 118: Reward = -50.0, Steps = 1437\n",
      "Episode 119: Reward = -50.0, Steps = 1437\n",
      "Episode 120: Reward = -50.0, Steps = 1437\n",
      "Episode 121: Reward = -50.0, Steps = 1437\n",
      "Episode 122: Reward = -50.0, Steps = 1437\n",
      "Episode 123: Reward = -50.0, Steps = 1437\n",
      "Episode 124: Reward = -50.0, Steps = 1436\n",
      "Episode 125: Reward = -50.0, Steps = 1437\n",
      "Episode 126: Reward = -50.0, Steps = 1436\n",
      "Episode 127: Reward = -50.0, Steps = 1436\n",
      "Episode 128: Reward = -50.0, Steps = 1437\n",
      "Episode 129: Reward = -50.0, Steps = 1437\n",
      "Episode 130: Reward = -50.0, Steps = 1437\n",
      "Episode 131: Reward = -50.0, Steps = 1437\n",
      "Episode 132: Reward = -50.0, Steps = 1437\n",
      "Episode 133: Reward = -50.0, Steps = 1437\n",
      "Episode 134: Reward = -50.0, Steps = 1437\n",
      "Episode 135: Reward = -50.0, Steps = 1437\n",
      "Episode 136: Reward = -50.0, Steps = 1437\n",
      "Episode 137: Reward = -50.0, Steps = 1437\n",
      "Episode 138: Reward = -50.0, Steps = 1436\n",
      "Episode 139: Reward = -50.0, Steps = 1437\n",
      "Episode 140: Reward = -50.0, Steps = 1437\n",
      "Episode 141: Reward = -50.0, Steps = 1437\n",
      "Episode 142: Reward = -50.0, Steps = 1437\n",
      "Episode 143: Reward = -50.0, Steps = 1437\n",
      "Episode 144: Reward = -50.0, Steps = 1437\n",
      "Episode 145: Reward = -50.0, Steps = 1437\n",
      "Episode 146: Reward = -50.0, Steps = 1437\n",
      "Episode 147: Reward = -50.0, Steps = 1437\n",
      "Episode 148: Reward = -50.0, Steps = 1437\n",
      "Episode 149: Reward = -50.0, Steps = 1437\n",
      "Episode 150: Reward = -50.0, Steps = 1437\n",
      "Episode 151: Reward = -50.0, Steps = 1437\n",
      "Episode 152: Reward = -50.0, Steps = 1437\n",
      "Episode 153: Reward = -50.0, Steps = 1436\n",
      "Episode 154: Reward = -50.0, Steps = 1437\n",
      "Episode 155: Reward = -50.0, Steps = 1437\n",
      "Episode 156: Reward = -50.0, Steps = 1437\n",
      "Episode 157: Reward = -50.0, Steps = 1437\n",
      "Episode 158: Reward = -50.0, Steps = 1437\n",
      "Episode 159: Reward = -50.0, Steps = 1437\n",
      "Episode 160: Reward = -50.0, Steps = 1437\n",
      "Episode 161: Reward = -50.0, Steps = 1437\n",
      "Episode 162: Reward = -50.0, Steps = 1437\n",
      "Episode 163: Reward = -50.0, Steps = 1437\n",
      "Episode 164: Reward = -50.0, Steps = 1437\n",
      "Episode 165: Reward = -50.0, Steps = 1437\n",
      "Episode 166: Reward = -50.0, Steps = 1437\n",
      "Episode 167: Reward = -50.0, Steps = 1437\n",
      "Episode 168: Reward = -50.0, Steps = 1437\n",
      "Episode 169: Reward = -50.0, Steps = 1437\n",
      "Episode 170: Reward = -50.0, Steps = 1437\n",
      "Episode 171: Reward = -50.0, Steps = 1437\n",
      "Episode 172: Reward = -50.0, Steps = 1437\n",
      "Episode 173: Reward = -50.0, Steps = 1437\n",
      "Episode 174: Reward = -50.0, Steps = 1437\n",
      "Episode 175: Reward = -50.0, Steps = 1437\n",
      "Episode 176: Reward = -50.0, Steps = 1437\n",
      "Episode 177: Reward = -50.0, Steps = 1437\n",
      "Episode 178: Reward = -50.0, Steps = 1437\n",
      "Episode 179: Reward = -50.0, Steps = 1437\n",
      "Episode 180: Reward = -50.0, Steps = 1437\n",
      "Episode 181: Reward = -50.0, Steps = 1437\n",
      "Episode 182: Reward = -50.0, Steps = 1437\n",
      "Episode 183: Reward = -50.0, Steps = 1437\n",
      "Episode 184: Reward = -50.0, Steps = 1437\n",
      "Episode 185: Reward = -50.0, Steps = 1437\n",
      "Episode 186: Reward = -50.0, Steps = 1437\n",
      "Episode 187: Reward = -50.0, Steps = 1437\n",
      "Episode 188: Reward = -50.0, Steps = 1437\n",
      "Episode 189: Reward = -50.0, Steps = 1437\n",
      "Episode 190: Reward = -50.0, Steps = 1437\n",
      "Episode 191: Reward = -50.0, Steps = 1437\n",
      "Episode 192: Reward = -50.0, Steps = 1437\n",
      "Episode 193: Reward = -50.0, Steps = 1437\n",
      "Episode 194: Reward = -50.0, Steps = 1437\n",
      "Episode 195: Reward = -50.0, Steps = 1437\n",
      "Episode 196: Reward = -50.0, Steps = 1437\n",
      "Episode 197: Reward = -50.0, Steps = 1437\n",
      "Episode 198: Reward = -50.0, Steps = 1437\n",
      "Episode 199: Reward = -50.0, Steps = 1437\n",
      "Episode 200: Reward = -50.0, Steps = 1437\n",
      "Episode 201: Reward = -50.0, Steps = 1437\n",
      "Episode 202: Reward = -50.0, Steps = 1437\n",
      "Episode 203: Reward = -50.0, Steps = 1437\n",
      "Episode 204: Reward = -50.0, Steps = 1437\n",
      "Episode 205: Reward = -50.0, Steps = 1437\n",
      "Episode 206: Reward = -50.0, Steps = 1437\n",
      "Episode 207: Reward = -50.0, Steps = 1437\n",
      "Episode 208: Reward = -50.0, Steps = 1437\n",
      "Episode 209: Reward = -50.0, Steps = 1437\n",
      "Episode 210: Reward = -50.0, Steps = 1437\n",
      "Episode 211: Reward = -50.0, Steps = 1437\n",
      "Episode 212: Reward = -50.0, Steps = 1437\n",
      "Episode 213: Reward = -50.0, Steps = 1437\n",
      "Episode 214: Reward = -50.0, Steps = 1437\n",
      "Episode 215: Reward = -50.0, Steps = 1437\n",
      "Episode 216: Reward = -50.0, Steps = 1437\n",
      "Episode 217: Reward = -50.0, Steps = 1437\n",
      "Episode 218: Reward = -50.0, Steps = 1437\n",
      "Episode 219: Reward = -50.0, Steps = 1437\n",
      "Episode 220: Reward = -50.0, Steps = 1437\n",
      "Episode 221: Reward = -50.0, Steps = 1437\n",
      "Episode 222: Reward = -50.0, Steps = 1437\n",
      "Episode 223: Reward = -50.0, Steps = 1437\n",
      "Episode 224: Reward = -50.0, Steps = 1437\n",
      "Episode 225: Reward = -50.0, Steps = 1437\n",
      "Episode 226: Reward = -50.0, Steps = 1437\n",
      "Episode 227: Reward = -50.0, Steps = 1437\n",
      "Episode 228: Reward = -50.0, Steps = 1437\n",
      "Episode 229: Reward = -50.0, Steps = 1436\n",
      "Episode 230: Reward = -50.0, Steps = 1437\n",
      "Episode 231: Reward = -50.0, Steps = 1437\n",
      "Episode 232: Reward = -50.0, Steps = 1437\n",
      "Episode 233: Reward = -50.0, Steps = 1437\n",
      "Episode 234: Reward = -50.0, Steps = 1437\n",
      "Episode 235: Reward = -50.0, Steps = 1437\n",
      "Episode 236: Reward = -50.0, Steps = 1437\n",
      "Episode 237: Reward = -50.0, Steps = 1437\n",
      "Episode 238: Reward = -50.0, Steps = 1437\n",
      "Episode 239: Reward = -50.0, Steps = 1436\n",
      "Episode 240: Reward = -50.0, Steps = 1437\n",
      "Episode 241: Reward = -50.0, Steps = 1437\n",
      "Episode 242: Reward = -50.0, Steps = 1437\n",
      "Episode 243: Reward = -50.0, Steps = 1437\n",
      "Episode 244: Reward = -50.0, Steps = 1437\n",
      "Episode 245: Reward = -50.0, Steps = 1437\n",
      "Episode 246: Reward = -50.0, Steps = 1437\n",
      "Episode 247: Reward = -50.0, Steps = 1437\n",
      "Episode 248: Reward = -50.0, Steps = 1437\n",
      "Episode 249: Reward = -50.0, Steps = 1437\n",
      "Episode 250: Reward = -50.0, Steps = 1437\n",
      "Episode 251: Reward = -50.0, Steps = 1437\n",
      "Episode 252: Reward = -50.0, Steps = 1437\n",
      "Episode 253: Reward = -50.0, Steps = 1437\n",
      "Episode 254: Reward = -50.0, Steps = 1437\n",
      "Episode 255: Reward = -50.0, Steps = 1437\n",
      "Episode 256: Reward = -50.0, Steps = 1437\n",
      "Episode 257: Reward = -50.0, Steps = 1437\n",
      "Episode 258: Reward = -50.0, Steps = 1437\n",
      "Episode 259: Reward = -50.0, Steps = 1436\n",
      "Episode 260: Reward = -50.0, Steps = 1437\n",
      "Episode 261: Reward = -50.0, Steps = 1437\n",
      "Episode 262: Reward = -50.0, Steps = 1437\n",
      "Episode 263: Reward = -50.0, Steps = 1437\n",
      "Episode 264: Reward = -50.0, Steps = 1437\n",
      "Episode 265: Reward = -50.0, Steps = 1437\n",
      "Episode 266: Reward = -50.0, Steps = 1437\n",
      "Episode 267: Reward = -50.0, Steps = 1437\n",
      "Episode 268: Reward = -50.0, Steps = 1437\n",
      "Episode 269: Reward = -50.0, Steps = 1437\n",
      "Episode 270: Reward = -50.0, Steps = 1437\n",
      "Episode 271: Reward = -50.0, Steps = 1437\n",
      "Episode 272: Reward = -50.0, Steps = 1437\n",
      "Episode 273: Reward = -50.0, Steps = 1437\n",
      "Episode 274: Reward = -50.0, Steps = 1437\n",
      "Episode 275: Reward = -50.0, Steps = 1437\n",
      "Episode 276: Reward = -50.0, Steps = 1437\n",
      "Episode 277: Reward = -50.0, Steps = 1437\n",
      "Episode 278: Reward = -50.0, Steps = 1437\n",
      "Episode 279: Reward = -50.0, Steps = 1437\n",
      "Episode 280: Reward = -50.0, Steps = 1437\n",
      "Episode 281: Reward = -50.0, Steps = 1437\n",
      "Episode 282: Reward = -50.0, Steps = 1437\n",
      "Episode 283: Reward = -50.0, Steps = 1437\n",
      "Episode 284: Reward = -50.0, Steps = 1437\n",
      "Episode 285: Reward = -50.0, Steps = 1437\n",
      "Episode 286: Reward = -50.0, Steps = 1436\n",
      "Episode 287: Reward = -50.0, Steps = 1437\n",
      "Episode 288: Reward = -50.0, Steps = 1437\n",
      "Episode 289: Reward = -50.0, Steps = 1437\n",
      "Episode 290: Reward = -50.0, Steps = 1437\n",
      "Episode 291: Reward = -50.0, Steps = 1437\n",
      "Episode 292: Reward = -50.0, Steps = 1437\n",
      "Episode 293: Reward = -50.0, Steps = 1437\n",
      "Episode 294: Reward = -50.0, Steps = 1437\n",
      "Episode 295: Reward = -50.0, Steps = 1437\n",
      "Episode 296: Reward = -50.0, Steps = 1437\n",
      "Episode 297: Reward = -50.0, Steps = 1437\n",
      "Episode 298: Reward = -50.0, Steps = 1437\n",
      "Episode 299: Reward = -50.0, Steps = 1437\n",
      "Episode 300: Reward = -50.0, Steps = 1437\n",
      "Episode 301: Reward = -50.0, Steps = 1437\n",
      "Episode 302: Reward = -50.0, Steps = 1437\n",
      "Episode 303: Reward = -50.0, Steps = 1437\n",
      "Episode 304: Reward = -50.0, Steps = 1437\n",
      "Episode 305: Reward = -50.0, Steps = 1437\n",
      "Episode 306: Reward = -50.0, Steps = 1437\n",
      "Episode 307: Reward = -50.0, Steps = 1437\n",
      "Episode 308: Reward = -50.0, Steps = 1437\n",
      "Episode 309: Reward = -50.0, Steps = 1436\n",
      "Episode 310: Reward = -50.0, Steps = 1437\n",
      "Episode 311: Reward = -50.0, Steps = 1437\n",
      "Episode 312: Reward = -50.0, Steps = 1437\n",
      "Episode 313: Reward = -50.0, Steps = 1437\n",
      "Episode 314: Reward = -50.0, Steps = 1437\n",
      "Episode 315: Reward = -50.0, Steps = 1437\n",
      "Episode 316: Reward = -50.0, Steps = 1437\n",
      "Episode 317: Reward = -50.0, Steps = 1437\n",
      "Episode 318: Reward = -50.0, Steps = 1437\n",
      "Episode 319: Reward = -50.0, Steps = 1437\n",
      "Episode 320: Reward = -50.0, Steps = 1436\n",
      "Episode 321: Reward = -50.0, Steps = 1437\n",
      "Episode 322: Reward = -50.0, Steps = 1437\n",
      "Episode 323: Reward = -50.0, Steps = 1437\n",
      "Episode 324: Reward = -50.0, Steps = 1436\n",
      "Episode 325: Reward = -50.0, Steps = 1437\n",
      "Episode 326: Reward = -50.0, Steps = 1437\n",
      "Episode 327: Reward = -50.0, Steps = 1437\n",
      "Episode 328: Reward = -50.0, Steps = 1437\n",
      "Episode 329: Reward = -50.0, Steps = 1437\n",
      "Episode 330: Reward = -50.0, Steps = 1437\n",
      "Episode 331: Reward = -50.0, Steps = 1436\n",
      "Episode 332: Reward = -50.0, Steps = 1437\n",
      "Episode 333: Reward = -50.0, Steps = 1437\n",
      "Episode 334: Reward = -50.0, Steps = 1437\n",
      "Episode 335: Reward = -50.0, Steps = 1437\n",
      "Episode 336: Reward = -50.0, Steps = 1437\n",
      "Episode 337: Reward = -50.0, Steps = 1437\n",
      "Episode 338: Reward = -50.0, Steps = 1437\n",
      "Episode 339: Reward = -50.0, Steps = 1437\n",
      "Episode 340: Reward = -50.0, Steps = 1437\n",
      "Episode 341: Reward = -50.0, Steps = 1437\n",
      "Episode 342: Reward = -50.0, Steps = 1437\n",
      "Episode 343: Reward = -50.0, Steps = 1437\n",
      "Episode 344: Reward = -50.0, Steps = 1437\n",
      "Episode 345: Reward = -50.0, Steps = 1436\n",
      "Episode 346: Reward = -50.0, Steps = 1437\n",
      "Episode 347: Reward = -50.0, Steps = 1437\n",
      "Episode 348: Reward = -50.0, Steps = 1437\n",
      "Episode 349: Reward = -50.0, Steps = 1437\n",
      "Episode 350: Reward = -50.0, Steps = 1437\n",
      "Episode 351: Reward = -50.0, Steps = 1437\n",
      "Episode 352: Reward = -50.0, Steps = 1437\n",
      "Episode 353: Reward = -50.0, Steps = 1437\n",
      "Episode 354: Reward = -50.0, Steps = 1437\n",
      "Episode 355: Reward = -50.0, Steps = 1437\n",
      "Episode 356: Reward = -50.0, Steps = 1437\n",
      "Episode 357: Reward = -50.0, Steps = 1437\n",
      "Episode 358: Reward = -50.0, Steps = 1437\n",
      "Episode 359: Reward = -50.0, Steps = 1437\n",
      "Episode 360: Reward = -50.0, Steps = 1437\n",
      "Episode 361: Reward = -50.0, Steps = 1437\n",
      "Episode 362: Reward = -50.0, Steps = 1437\n",
      "Episode 363: Reward = -50.0, Steps = 1437\n",
      "Episode 364: Reward = -50.0, Steps = 1437\n",
      "Episode 365: Reward = -50.0, Steps = 1437\n",
      "Episode 366: Reward = -50.0, Steps = 1437\n",
      "Episode 367: Reward = -50.0, Steps = 1437\n",
      "Episode 368: Reward = -50.0, Steps = 1437\n",
      "Episode 369: Reward = -50.0, Steps = 1437\n",
      "Episode 370: Reward = -50.0, Steps = 1437\n",
      "Episode 371: Reward = -50.0, Steps = 1437\n",
      "Episode 372: Reward = -50.0, Steps = 1437\n",
      "Episode 373: Reward = -50.0, Steps = 1437\n",
      "Episode 374: Reward = -50.0, Steps = 1437\n",
      "Episode 375: Reward = -50.0, Steps = 1437\n",
      "Episode 376: Reward = -50.0, Steps = 1437\n",
      "Episode 377: Reward = -50.0, Steps = 1437\n",
      "Episode 378: Reward = -50.0, Steps = 1437\n",
      "Episode 379: Reward = -50.0, Steps = 1437\n",
      "Episode 380: Reward = -50.0, Steps = 1437\n",
      "Episode 381: Reward = -50.0, Steps = 1437\n",
      "Episode 382: Reward = -50.0, Steps = 1437\n",
      "Episode 383: Reward = -50.0, Steps = 1437\n",
      "Episode 384: Reward = -50.0, Steps = 1437\n",
      "Episode 385: Reward = -50.0, Steps = 1437\n",
      "Episode 386: Reward = -50.0, Steps = 1437\n",
      "Episode 387: Reward = -50.0, Steps = 1437\n",
      "Episode 388: Reward = -50.0, Steps = 1437\n",
      "Episode 389: Reward = -50.0, Steps = 1437\n",
      "Episode 390: Reward = -50.0, Steps = 1437\n",
      "Episode 391: Reward = -50.0, Steps = 1437\n",
      "Episode 392: Reward = -50.0, Steps = 1437\n",
      "Episode 393: Reward = -50.0, Steps = 1437\n",
      "Episode 394: Reward = -50.0, Steps = 1437\n",
      "Episode 395: Reward = -50.0, Steps = 1437\n",
      "Episode 396: Reward = -50.0, Steps = 1436\n",
      "Episode 397: Reward = -50.0, Steps = 1437\n",
      "Episode 398: Reward = -50.0, Steps = 1437\n",
      "Episode 399: Reward = -50.0, Steps = 1437\n",
      "Episode 400: Reward = -50.0, Steps = 1437\n",
      "Episode 401: Reward = -50.0, Steps = 1437\n",
      "Episode 402: Reward = -50.0, Steps = 1437\n",
      "Episode 403: Reward = -50.0, Steps = 1437\n",
      "Episode 404: Reward = -50.0, Steps = 1437\n",
      "Episode 405: Reward = -50.0, Steps = 1437\n",
      "Episode 406: Reward = -50.0, Steps = 1437\n",
      "Episode 407: Reward = -50.0, Steps = 1436\n",
      "Episode 408: Reward = -50.0, Steps = 1437\n",
      "Episode 409: Reward = -50.0, Steps = 1437\n",
      "Episode 410: Reward = -50.0, Steps = 1437\n",
      "Episode 411: Reward = -50.0, Steps = 1437\n",
      "Episode 412: Reward = -50.0, Steps = 1437\n",
      "Episode 413: Reward = -50.0, Steps = 1437\n",
      "Episode 414: Reward = -50.0, Steps = 1436\n",
      "Episode 415: Reward = -50.0, Steps = 1437\n",
      "Episode 416: Reward = -50.0, Steps = 1437\n",
      "Episode 417: Reward = -50.0, Steps = 1437\n",
      "Episode 418: Reward = -50.0, Steps = 1437\n",
      "Episode 419: Reward = -50.0, Steps = 1437\n",
      "Episode 420: Reward = -50.0, Steps = 1437\n",
      "Episode 421: Reward = -50.0, Steps = 1437\n",
      "Episode 422: Reward = -50.0, Steps = 1437\n",
      "Episode 423: Reward = -50.0, Steps = 1437\n",
      "Episode 424: Reward = -50.0, Steps = 1437\n",
      "Episode 425: Reward = -50.0, Steps = 1437\n",
      "Episode 426: Reward = -50.0, Steps = 1436\n",
      "Episode 427: Reward = -50.0, Steps = 1436\n",
      "Episode 428: Reward = -50.0, Steps = 1437\n",
      "Episode 429: Reward = -50.0, Steps = 1436\n",
      "Episode 430: Reward = -50.0, Steps = 1437\n",
      "Episode 431: Reward = -50.0, Steps = 1437\n",
      "Episode 432: Reward = -50.0, Steps = 1437\n",
      "Episode 433: Reward = -50.0, Steps = 1437\n",
      "Episode 434: Reward = -50.0, Steps = 1437\n",
      "Episode 435: Reward = -50.0, Steps = 1437\n",
      "Episode 436: Reward = -50.0, Steps = 1437\n",
      "Episode 437: Reward = -50.0, Steps = 1437\n",
      "Episode 438: Reward = -50.0, Steps = 1437\n",
      "Episode 439: Reward = -50.0, Steps = 1437\n",
      "Episode 440: Reward = -50.0, Steps = 1437\n",
      "Episode 441: Reward = -50.0, Steps = 1437\n",
      "Episode 442: Reward = -50.0, Steps = 1437\n",
      "Episode 443: Reward = -50.0, Steps = 1437\n",
      "Episode 444: Reward = -50.0, Steps = 1437\n",
      "Episode 445: Reward = -50.0, Steps = 1437\n",
      "Episode 446: Reward = -50.0, Steps = 1437\n",
      "Episode 447: Reward = -50.0, Steps = 1437\n",
      "Episode 448: Reward = -50.0, Steps = 1437\n",
      "Episode 449: Reward = -50.0, Steps = 1437\n",
      "Episode 450: Reward = -50.0, Steps = 1437\n",
      "Episode 451: Reward = -50.0, Steps = 1437\n",
      "Episode 452: Reward = -50.0, Steps = 1437\n",
      "Episode 453: Reward = -50.0, Steps = 1437\n",
      "Episode 454: Reward = -50.0, Steps = 1437\n",
      "Episode 455: Reward = -50.0, Steps = 1437\n",
      "Episode 456: Reward = -50.0, Steps = 1437\n",
      "Episode 457: Reward = -50.0, Steps = 1437\n",
      "Episode 458: Reward = -50.0, Steps = 1437\n",
      "Episode 459: Reward = -50.0, Steps = 1436\n",
      "Episode 460: Reward = -50.0, Steps = 1437\n",
      "Episode 461: Reward = -50.0, Steps = 1437\n",
      "Episode 462: Reward = -50.0, Steps = 1437\n",
      "Episode 463: Reward = -50.0, Steps = 1437\n",
      "Episode 464: Reward = -50.0, Steps = 1437\n",
      "Episode 465: Reward = -50.0, Steps = 1437\n",
      "Episode 466: Reward = -50.0, Steps = 1437\n",
      "Episode 467: Reward = -50.0, Steps = 1437\n",
      "Episode 468: Reward = -50.0, Steps = 1437\n",
      "Episode 469: Reward = -50.0, Steps = 1436\n",
      "Episode 470: Reward = -50.0, Steps = 1437\n",
      "Episode 471: Reward = -50.0, Steps = 1437\n",
      "Episode 472: Reward = -50.0, Steps = 1437\n",
      "Episode 473: Reward = -50.0, Steps = 1437\n",
      "Episode 474: Reward = -50.0, Steps = 1437\n",
      "Episode 475: Reward = -50.0, Steps = 1437\n",
      "Episode 476: Reward = -50.0, Steps = 1437\n",
      "Episode 477: Reward = -50.0, Steps = 1437\n",
      "Episode 478: Reward = -50.0, Steps = 1437\n",
      "Episode 479: Reward = -50.0, Steps = 1437\n",
      "Episode 480: Reward = -50.0, Steps = 1437\n",
      "Episode 481: Reward = -50.0, Steps = 1437\n",
      "Episode 482: Reward = -50.0, Steps = 1437\n",
      "Episode 483: Reward = -50.0, Steps = 1437\n",
      "Episode 484: Reward = -50.0, Steps = 1437\n",
      "Episode 485: Reward = -50.0, Steps = 1437\n",
      "Episode 486: Reward = -50.0, Steps = 1437\n",
      "Episode 487: Reward = -50.0, Steps = 1437\n",
      "Episode 488: Reward = -50.0, Steps = 1437\n",
      "Episode 489: Reward = -50.0, Steps = 1437\n",
      "Episode 490: Reward = -50.0, Steps = 1437\n",
      "Episode 491: Reward = -50.0, Steps = 1437\n",
      "Episode 492: Reward = -50.0, Steps = 1437\n",
      "Episode 493: Reward = -50.0, Steps = 1437\n",
      "Episode 494: Reward = -50.0, Steps = 1437\n",
      "Episode 495: Reward = -50.0, Steps = 1437\n",
      "Episode 496: Reward = -50.0, Steps = 1437\n",
      "Episode 497: Reward = -50.0, Steps = 1437\n",
      "Episode 498: Reward = -50.0, Steps = 1437\n",
      "Episode 499: Reward = -50.0, Steps = 1437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_878795/601979139.py:496: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500: Reward = -50.0, Steps = 1436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_878795/601979139.py\", line 583, in sweep_agent\n",
      "    agent.save_model('sac_discrete_boxing_ram.pth')\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'AgentSAC' object has no attribute 'save_model'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>policy_loss</td><td>▇▁██████████████████████████████████████</td></tr><tr><td>q1_loss</td><td>█▄▄▄▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>q2_loss</td><td>█▇▆▄▅▄▃▂▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>selected_action</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>steps</td><td>███▁████▁██████████████████████████▁████</td></tr><tr><td>updates</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>alpha</td><td>0.01</td></tr><tr><td>entropy</td><td>0</td></tr><tr><td>episode</td><td>500</td></tr><tr><td>episode_reward</td><td>-50</td></tr><tr><td>policy_loss</td><td>0.83623</td></tr><tr><td>q1_loss</td><td>0.00032</td></tr><tr><td>q2_loss</td><td>0.00014</td></tr><tr><td>selected_action</td><td>6</td></tr><tr><td>steps</td><td>1436</td></tr><tr><td>updates</td><td>718551</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-1</strong> at: <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/hd3x4ms8' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/hd3x4ms8</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241015_212927-hd3x4ms8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hd3x4ms8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_878795/601979139.py\", line 583, in sweep_agent\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     agent.save_model('sac_discrete_boxing_ram.pth')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AttributeError: 'AgentSAC' object has no attribute 'save_model'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_878795/601979139.py\", line 590, in sweep_agent\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({\"error\": str(e)})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py\", line 36, in preinit_wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise wandb.Error(f\"You must call wandb.init() before {name}()\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m wandb.errors.errors.Error: You must call wandb.init() before wandb.log()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: huyzxg8n with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 0.4243911024820056\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tautomatic_alpha: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion_episodes: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9660990324836776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_sizes: (512,512)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_policy: 2.579533072455188e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_q: 0.0002999300399583993\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_episodes: 500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \treplay_size: 50000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_update_interval: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttau: 0.006101050578685132\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241016_045124-huyzxg8n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/huyzxg8n' target=\"_blank\">hopeful-sweep-2</a></strong> to <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/huyzxg8n' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/runs/huyzxg8n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Shape: (128,)\n",
      "Action Space: Discrete(18)\n",
      "State dimensions: 128\n",
      "Number of actions: 18\n",
      "Automatic alpha tuning enabled.\n",
      "Episode 1: Reward = -48.0, Steps = 1786\n",
      "Episode 2: Reward = -34.0, Steps = 1786\n",
      "Episode 3: Reward = -34.0, Steps = 1786\n",
      "Episode 4: Reward = -34.0, Steps = 1786\n",
      "Episode 5: Reward = -34.0, Steps = 1786\n",
      "Model saved to sac_discrete_boxing_ram_best_episode_5.pth\n",
      "New best average reward: -36.80. Model saved.\n",
      "Episode 6: Reward = -34.0, Steps = 1786\n",
      "Model saved to sac_discrete_boxing_ram_best_episode_6.pth\n",
      "New best average reward: -34.00. Model saved.\n",
      "Episode 7: Reward = -34.0, Steps = 1786\n",
      "Episode 8: Reward = -34.0, Steps = 1786\n",
      "Episode 9: Reward = -34.0, Steps = 1786\n",
      "Episode 10: Reward = -34.0, Steps = 1786\n",
      "Episode 11: Reward = -34.0, Steps = 1786\n",
      "Episode 12: Reward = -34.0, Steps = 1786\n",
      "Episode 13: Reward = -34.0, Steps = 1786\n",
      "Episode 14: Reward = -34.0, Steps = 1786\n",
      "Episode 15: Reward = -34.0, Steps = 1786\n",
      "Episode 16: Reward = -34.0, Steps = 1786\n",
      "Episode 17: Reward = -34.0, Steps = 1786\n",
      "Episode 18: Reward = -34.0, Steps = 1786\n",
      "Episode 19: Reward = -34.0, Steps = 1786\n",
      "Episode 20: Reward = -34.0, Steps = 1786\n",
      "Episode 21: Reward = -34.0, Steps = 1786\n",
      "Episode 22: Reward = -34.0, Steps = 1786\n",
      "Episode 23: Reward = -34.0, Steps = 1786\n",
      "Episode 24: Reward = -34.0, Steps = 1786\n",
      "Episode 25: Reward = -34.0, Steps = 1786\n",
      "Episode 26: Reward = -34.0, Steps = 1786\n",
      "Episode 27: Reward = -34.0, Steps = 1786\n",
      "Episode 28: Reward = -34.0, Steps = 1786\n",
      "Episode 29: Reward = -34.0, Steps = 1786\n",
      "Episode 30: Reward = -34.0, Steps = 1786\n",
      "Episode 31: Reward = -34.0, Steps = 1786\n",
      "Episode 32: Reward = -34.0, Steps = 1786\n",
      "Episode 33: Reward = -34.0, Steps = 1786\n",
      "Episode 34: Reward = -34.0, Steps = 1786\n",
      "Episode 35: Reward = -34.0, Steps = 1786\n",
      "Episode 36: Reward = -34.0, Steps = 1786\n",
      "Episode 37: Reward = -34.0, Steps = 1786\n",
      "Episode 38: Reward = -34.0, Steps = 1786\n",
      "Episode 39: Reward = -34.0, Steps = 1786\n",
      "Episode 40: Reward = -34.0, Steps = 1786\n",
      "Episode 41: Reward = -34.0, Steps = 1786\n",
      "Episode 42: Reward = -34.0, Steps = 1786\n",
      "Episode 43: Reward = -34.0, Steps = 1786\n",
      "Episode 44: Reward = -34.0, Steps = 1786\n",
      "Episode 45: Reward = -34.0, Steps = 1786\n",
      "Episode 46: Reward = -34.0, Steps = 1786\n",
      "Episode 47: Reward = -34.0, Steps = 1786\n",
      "Episode 48: Reward = -34.0, Steps = 1786\n",
      "Episode 49: Reward = -34.0, Steps = 1786\n",
      "Episode 50: Reward = -34.0, Steps = 1786\n",
      "Episode 51: Reward = -34.0, Steps = 1786\n",
      "Episode 52: Reward = -34.0, Steps = 1786\n",
      "Episode 53: Reward = -34.0, Steps = 1786\n",
      "Episode 54: Reward = -34.0, Steps = 1786\n",
      "Episode 55: Reward = -34.0, Steps = 1786\n",
      "Episode 56: Reward = -34.0, Steps = 1786\n",
      "Episode 57: Reward = -34.0, Steps = 1786\n",
      "Episode 58: Reward = -34.0, Steps = 1786\n",
      "Episode 59: Reward = -34.0, Steps = 1786\n",
      "Episode 60: Reward = -34.0, Steps = 1786\n",
      "Episode 61: Reward = -34.0, Steps = 1786\n",
      "Episode 62: Reward = -34.0, Steps = 1786\n",
      "Episode 63: Reward = -34.0, Steps = 1786\n",
      "Episode 64: Reward = -34.0, Steps = 1786\n",
      "Episode 65: Reward = -34.0, Steps = 1786\n",
      "Episode 66: Reward = -34.0, Steps = 1786\n",
      "Episode 67: Reward = -34.0, Steps = 1786\n",
      "Episode 68: Reward = -34.0, Steps = 1786\n",
      "Episode 69: Reward = -34.0, Steps = 1786\n",
      "Episode 70: Reward = -34.0, Steps = 1786\n",
      "Episode 71: Reward = -34.0, Steps = 1786\n",
      "Episode 72: Reward = -34.0, Steps = 1786\n",
      "Episode 73: Reward = -34.0, Steps = 1786\n",
      "Episode 74: Reward = -34.0, Steps = 1786\n",
      "Episode 75: Reward = -34.0, Steps = 1786\n",
      "Episode 76: Reward = -34.0, Steps = 1786\n",
      "Episode 77: Reward = -34.0, Steps = 1786\n",
      "Episode 78: Reward = -34.0, Steps = 1786\n",
      "Episode 79: Reward = -34.0, Steps = 1786\n",
      "Episode 80: Reward = -34.0, Steps = 1786\n",
      "Episode 81: Reward = -34.0, Steps = 1786\n",
      "Episode 82: Reward = -34.0, Steps = 1786\n",
      "Episode 83: Reward = -34.0, Steps = 1786\n",
      "Episode 84: Reward = -34.0, Steps = 1786\n",
      "Episode 85: Reward = -34.0, Steps = 1786\n",
      "Episode 86: Reward = -34.0, Steps = 1786\n",
      "Episode 87: Reward = -34.0, Steps = 1786\n",
      "Episode 88: Reward = -34.0, Steps = 1786\n",
      "Episode 89: Reward = -34.0, Steps = 1786\n",
      "Episode 90: Reward = -34.0, Steps = 1786\n",
      "Episode 91: Reward = -34.0, Steps = 1786\n",
      "Episode 92: Reward = -34.0, Steps = 1786\n",
      "Episode 93: Reward = -34.0, Steps = 1786\n",
      "Episode 94: Reward = -34.0, Steps = 1786\n",
      "Episode 95: Reward = -34.0, Steps = 1786\n",
      "Episode 96: Reward = -34.0, Steps = 1786\n",
      "Episode 97: Reward = -34.0, Steps = 1786\n",
      "Episode 98: Reward = -34.0, Steps = 1786\n",
      "Episode 99: Reward = -34.0, Steps = 1786\n",
      "Episode 100: Reward = -34.0, Steps = 1786\n",
      "Episode 101: Reward = -34.0, Steps = 1786\n",
      "Episode 102: Reward = -34.0, Steps = 1786\n",
      "Episode 103: Reward = -34.0, Steps = 1786\n",
      "Episode 104: Reward = -34.0, Steps = 1786\n",
      "Episode 105: Reward = -34.0, Steps = 1786\n",
      "Episode 106: Reward = -34.0, Steps = 1786\n",
      "Episode 107: Reward = -34.0, Steps = 1786\n",
      "Episode 108: Reward = -34.0, Steps = 1786\n",
      "Episode 109: Reward = -34.0, Steps = 1786\n",
      "Episode 110: Reward = -34.0, Steps = 1786\n",
      "Episode 111: Reward = -34.0, Steps = 1786\n",
      "Episode 112: Reward = -34.0, Steps = 1786\n",
      "Episode 113: Reward = -34.0, Steps = 1786\n",
      "Episode 114: Reward = -34.0, Steps = 1786\n",
      "Episode 115: Reward = -34.0, Steps = 1786\n",
      "Episode 116: Reward = -34.0, Steps = 1786\n",
      "Episode 117: Reward = -34.0, Steps = 1786\n",
      "Episode 118: Reward = -34.0, Steps = 1786\n",
      "Episode 119: Reward = -34.0, Steps = 1786\n",
      "Episode 120: Reward = -34.0, Steps = 1786\n",
      "Episode 121: Reward = -34.0, Steps = 1786\n",
      "Episode 122: Reward = -34.0, Steps = 1786\n",
      "Episode 123: Reward = -34.0, Steps = 1786\n",
      "Episode 124: Reward = -34.0, Steps = 1786\n",
      "Episode 125: Reward = -34.0, Steps = 1786\n",
      "Episode 126: Reward = -34.0, Steps = 1786\n",
      "Episode 127: Reward = -34.0, Steps = 1786\n",
      "Episode 128: Reward = -34.0, Steps = 1786\n",
      "Episode 129: Reward = -34.0, Steps = 1786\n",
      "Episode 130: Reward = -34.0, Steps = 1786\n",
      "Episode 131: Reward = -34.0, Steps = 1786\n",
      "Episode 132: Reward = -34.0, Steps = 1786\n",
      "Episode 133: Reward = -34.0, Steps = 1786\n",
      "Episode 134: Reward = -34.0, Steps = 1786\n",
      "Episode 135: Reward = -34.0, Steps = 1786\n",
      "Episode 136: Reward = -34.0, Steps = 1786\n",
      "Episode 137: Reward = -34.0, Steps = 1786\n",
      "Episode 138: Reward = -34.0, Steps = 1786\n",
      "Episode 139: Reward = -34.0, Steps = 1786\n",
      "Episode 140: Reward = -34.0, Steps = 1786\n",
      "Episode 141: Reward = -34.0, Steps = 1786\n",
      "Episode 142: Reward = -34.0, Steps = 1786\n",
      "Episode 143: Reward = -34.0, Steps = 1786\n",
      "Episode 144: Reward = -34.0, Steps = 1786\n",
      "Episode 145: Reward = -34.0, Steps = 1786\n",
      "Episode 146: Reward = -34.0, Steps = 1786\n",
      "Episode 147: Reward = -34.0, Steps = 1786\n",
      "Episode 148: Reward = -34.0, Steps = 1786\n",
      "Episode 149: Reward = -34.0, Steps = 1786\n",
      "Episode 150: Reward = -34.0, Steps = 1786\n",
      "Episode 151: Reward = -34.0, Steps = 1786\n",
      "Episode 152: Reward = -34.0, Steps = 1786\n",
      "Episode 153: Reward = -34.0, Steps = 1786\n",
      "Episode 154: Reward = -34.0, Steps = 1786\n",
      "Episode 155: Reward = -34.0, Steps = 1786\n",
      "Episode 156: Reward = -34.0, Steps = 1786\n",
      "Episode 157: Reward = -34.0, Steps = 1786\n",
      "Episode 158: Reward = -34.0, Steps = 1786\n",
      "Episode 159: Reward = -34.0, Steps = 1786\n",
      "Episode 160: Reward = -34.0, Steps = 1786\n",
      "Episode 161: Reward = -34.0, Steps = 1786\n",
      "Episode 162: Reward = -34.0, Steps = 1786\n",
      "Episode 163: Reward = -34.0, Steps = 1786\n",
      "Episode 164: Reward = -34.0, Steps = 1786\n",
      "Episode 165: Reward = -34.0, Steps = 1786\n",
      "Episode 166: Reward = -34.0, Steps = 1786\n",
      "Episode 167: Reward = -34.0, Steps = 1786\n",
      "Episode 168: Reward = -34.0, Steps = 1786\n",
      "Episode 169: Reward = -34.0, Steps = 1786\n",
      "Episode 170: Reward = -34.0, Steps = 1786\n",
      "Episode 171: Reward = -34.0, Steps = 1786\n",
      "Episode 172: Reward = -34.0, Steps = 1786\n",
      "Episode 173: Reward = -34.0, Steps = 1786\n",
      "Episode 174: Reward = -34.0, Steps = 1786\n",
      "Episode 175: Reward = -34.0, Steps = 1786\n",
      "Episode 176: Reward = -34.0, Steps = 1786\n",
      "Episode 177: Reward = -34.0, Steps = 1786\n",
      "Episode 178: Reward = -34.0, Steps = 1786\n",
      "Episode 179: Reward = -34.0, Steps = 1786\n",
      "Episode 180: Reward = -34.0, Steps = 1786\n",
      "Episode 181: Reward = -34.0, Steps = 1786\n",
      "Episode 182: Reward = -34.0, Steps = 1786\n",
      "Episode 183: Reward = -34.0, Steps = 1786\n",
      "Episode 184: Reward = -34.0, Steps = 1786\n",
      "Episode 185: Reward = -34.0, Steps = 1786\n",
      "Episode 186: Reward = -34.0, Steps = 1786\n",
      "Episode 187: Reward = -34.0, Steps = 1786\n",
      "Episode 188: Reward = -34.0, Steps = 1786\n",
      "Episode 189: Reward = -34.0, Steps = 1786\n",
      "Episode 190: Reward = -34.0, Steps = 1786\n",
      "Episode 191: Reward = -34.0, Steps = 1786\n",
      "Episode 192: Reward = -34.0, Steps = 1786\n",
      "Episode 193: Reward = -34.0, Steps = 1786\n",
      "Episode 194: Reward = -34.0, Steps = 1786\n",
      "Episode 195: Reward = -34.0, Steps = 1786\n",
      "Episode 196: Reward = -34.0, Steps = 1786\n",
      "Episode 197: Reward = -34.0, Steps = 1786\n",
      "Episode 198: Reward = -34.0, Steps = 1786\n",
      "Episode 199: Reward = -34.0, Steps = 1786\n",
      "Episode 200: Reward = -34.0, Steps = 1786\n",
      "Episode 201: Reward = -34.0, Steps = 1786\n",
      "Episode 202: Reward = -34.0, Steps = 1786\n",
      "Episode 203: Reward = -34.0, Steps = 1786\n",
      "Episode 204: Reward = -34.0, Steps = 1786\n",
      "Episode 205: Reward = -34.0, Steps = 1786\n",
      "Episode 206: Reward = -34.0, Steps = 1786\n",
      "Episode 207: Reward = -34.0, Steps = 1786\n",
      "Episode 208: Reward = -34.0, Steps = 1786\n",
      "Episode 209: Reward = -34.0, Steps = 1786\n",
      "Episode 210: Reward = -34.0, Steps = 1786\n",
      "Episode 211: Reward = -34.0, Steps = 1786\n",
      "Episode 212: Reward = -34.0, Steps = 1786\n",
      "Episode 213: Reward = -34.0, Steps = 1786\n",
      "Episode 214: Reward = -34.0, Steps = 1786\n",
      "Episode 215: Reward = -34.0, Steps = 1786\n",
      "Episode 216: Reward = -34.0, Steps = 1786\n",
      "Episode 217: Reward = -34.0, Steps = 1786\n",
      "Episode 218: Reward = -34.0, Steps = 1786\n",
      "Episode 219: Reward = -34.0, Steps = 1786\n",
      "Episode 220: Reward = -34.0, Steps = 1786\n",
      "Episode 221: Reward = -34.0, Steps = 1786\n",
      "Episode 222: Reward = -34.0, Steps = 1786\n",
      "Episode 223: Reward = -34.0, Steps = 1786\n",
      "Episode 224: Reward = -34.0, Steps = 1786\n",
      "Episode 225: Reward = -34.0, Steps = 1786\n",
      "Episode 226: Reward = -34.0, Steps = 1786\n",
      "Episode 227: Reward = -34.0, Steps = 1786\n",
      "Episode 228: Reward = -34.0, Steps = 1786\n",
      "Episode 229: Reward = -34.0, Steps = 1786\n",
      "Episode 230: Reward = -34.0, Steps = 1786\n",
      "Episode 231: Reward = -34.0, Steps = 1786\n",
      "Episode 232: Reward = -34.0, Steps = 1786\n",
      "Episode 233: Reward = -34.0, Steps = 1786\n",
      "Episode 234: Reward = -34.0, Steps = 1786\n",
      "Episode 235: Reward = -34.0, Steps = 1786\n",
      "Episode 236: Reward = -34.0, Steps = 1786\n",
      "Episode 237: Reward = -34.0, Steps = 1786\n",
      "Episode 238: Reward = -34.0, Steps = 1786\n",
      "Episode 239: Reward = -34.0, Steps = 1786\n",
      "Episode 240: Reward = -34.0, Steps = 1786\n",
      "Episode 241: Reward = -34.0, Steps = 1786\n",
      "Episode 242: Reward = -34.0, Steps = 1786\n",
      "Episode 243: Reward = -34.0, Steps = 1786\n",
      "Episode 244: Reward = -34.0, Steps = 1786\n",
      "Episode 245: Reward = -34.0, Steps = 1786\n",
      "Episode 246: Reward = -34.0, Steps = 1786\n",
      "Episode 247: Reward = -34.0, Steps = 1786\n",
      "Episode 248: Reward = -34.0, Steps = 1786\n",
      "Episode 249: Reward = -34.0, Steps = 1786\n",
      "Episode 250: Reward = -34.0, Steps = 1786\n",
      "Episode 251: Reward = -34.0, Steps = 1786\n",
      "Episode 252: Reward = -34.0, Steps = 1786\n",
      "Episode 253: Reward = -34.0, Steps = 1786\n",
      "Episode 254: Reward = -34.0, Steps = 1786\n",
      "Episode 255: Reward = -34.0, Steps = 1786\n",
      "Episode 256: Reward = -34.0, Steps = 1786\n",
      "Episode 257: Reward = -34.0, Steps = 1786\n",
      "Episode 258: Reward = -34.0, Steps = 1786\n",
      "Episode 259: Reward = -34.0, Steps = 1786\n",
      "Episode 260: Reward = -34.0, Steps = 1786\n",
      "Episode 261: Reward = -34.0, Steps = 1786\n",
      "Episode 262: Reward = -34.0, Steps = 1786\n",
      "Episode 263: Reward = -34.0, Steps = 1786\n",
      "Episode 264: Reward = -34.0, Steps = 1786\n",
      "Episode 265: Reward = -34.0, Steps = 1786\n",
      "Episode 266: Reward = -34.0, Steps = 1786\n",
      "Episode 267: Reward = -34.0, Steps = 1786\n",
      "Episode 268: Reward = -34.0, Steps = 1786\n",
      "Episode 269: Reward = -34.0, Steps = 1786\n",
      "Episode 270: Reward = -34.0, Steps = 1786\n",
      "Episode 271: Reward = -34.0, Steps = 1786\n",
      "Episode 272: Reward = -34.0, Steps = 1786\n",
      "Episode 273: Reward = -34.0, Steps = 1786\n",
      "Episode 274: Reward = -34.0, Steps = 1786\n",
      "Episode 275: Reward = -34.0, Steps = 1786\n",
      "Episode 276: Reward = -34.0, Steps = 1786\n",
      "Episode 277: Reward = -34.0, Steps = 1786\n",
      "Episode 278: Reward = -34.0, Steps = 1786\n",
      "Episode 279: Reward = -34.0, Steps = 1786\n",
      "Episode 280: Reward = -34.0, Steps = 1786\n",
      "Episode 281: Reward = -34.0, Steps = 1786\n",
      "Episode 282: Reward = -34.0, Steps = 1786\n",
      "Episode 283: Reward = -34.0, Steps = 1786\n",
      "Episode 284: Reward = -34.0, Steps = 1786\n",
      "Episode 285: Reward = -34.0, Steps = 1786\n",
      "Episode 286: Reward = -34.0, Steps = 1786\n",
      "Episode 287: Reward = -34.0, Steps = 1786\n",
      "Episode 288: Reward = -34.0, Steps = 1786\n",
      "Episode 289: Reward = -34.0, Steps = 1786\n",
      "Episode 290: Reward = -34.0, Steps = 1786\n",
      "Episode 291: Reward = -34.0, Steps = 1786\n",
      "Episode 292: Reward = -34.0, Steps = 1786\n",
      "Episode 293: Reward = -34.0, Steps = 1786\n",
      "Episode 294: Reward = -34.0, Steps = 1786\n",
      "Episode 295: Reward = -34.0, Steps = 1786\n",
      "Episode 296: Reward = -34.0, Steps = 1786\n",
      "Episode 297: Reward = -34.0, Steps = 1786\n",
      "Episode 298: Reward = -34.0, Steps = 1786\n",
      "Episode 299: Reward = -34.0, Steps = 1786\n",
      "Episode 300: Reward = -34.0, Steps = 1786\n",
      "Episode 301: Reward = -34.0, Steps = 1786\n",
      "Episode 302: Reward = -34.0, Steps = 1786\n",
      "Episode 303: Reward = -34.0, Steps = 1786\n",
      "Episode 304: Reward = -34.0, Steps = 1786\n",
      "Episode 305: Reward = -34.0, Steps = 1786\n",
      "Episode 306: Reward = -34.0, Steps = 1786\n",
      "Episode 307: Reward = -34.0, Steps = 1786\n",
      "Episode 308: Reward = -34.0, Steps = 1786\n",
      "Episode 309: Reward = -34.0, Steps = 1786\n",
      "Episode 310: Reward = -34.0, Steps = 1786\n",
      "Episode 311: Reward = -34.0, Steps = 1786\n",
      "Episode 312: Reward = -34.0, Steps = 1786\n",
      "Episode 313: Reward = -34.0, Steps = 1786\n",
      "Episode 314: Reward = -34.0, Steps = 1786\n",
      "Episode 315: Reward = -34.0, Steps = 1786\n",
      "Episode 316: Reward = -34.0, Steps = 1786\n",
      "Episode 317: Reward = -34.0, Steps = 1786\n",
      "Episode 318: Reward = -34.0, Steps = 1786\n",
      "Episode 319: Reward = -34.0, Steps = 1786\n",
      "Episode 320: Reward = -34.0, Steps = 1786\n",
      "Episode 321: Reward = -34.0, Steps = 1786\n",
      "Episode 322: Reward = -34.0, Steps = 1786\n",
      "Episode 323: Reward = -34.0, Steps = 1786\n",
      "Episode 324: Reward = -34.0, Steps = 1786\n",
      "Episode 325: Reward = -34.0, Steps = 1786\n",
      "Episode 326: Reward = -34.0, Steps = 1786\n",
      "Episode 327: Reward = -34.0, Steps = 1786\n",
      "Episode 328: Reward = -34.0, Steps = 1786\n",
      "Episode 329: Reward = -34.0, Steps = 1786\n",
      "Episode 330: Reward = -34.0, Steps = 1786\n",
      "Episode 331: Reward = -34.0, Steps = 1786\n",
      "Episode 332: Reward = -34.0, Steps = 1786\n",
      "Episode 333: Reward = -34.0, Steps = 1786\n",
      "Episode 334: Reward = -34.0, Steps = 1786\n",
      "Episode 335: Reward = -34.0, Steps = 1786\n",
      "Episode 336: Reward = -34.0, Steps = 1786\n",
      "Episode 337: Reward = -34.0, Steps = 1786\n",
      "Episode 338: Reward = -34.0, Steps = 1786\n",
      "Episode 339: Reward = -34.0, Steps = 1786\n",
      "Episode 340: Reward = -34.0, Steps = 1786\n",
      "Episode 341: Reward = -34.0, Steps = 1786\n",
      "Episode 342: Reward = -34.0, Steps = 1786\n",
      "Episode 343: Reward = -34.0, Steps = 1786\n",
      "Episode 344: Reward = -34.0, Steps = 1786\n",
      "Episode 345: Reward = -34.0, Steps = 1786\n",
      "Episode 346: Reward = -34.0, Steps = 1786\n",
      "Episode 347: Reward = -34.0, Steps = 1786\n",
      "Episode 348: Reward = -34.0, Steps = 1786\n",
      "Episode 349: Reward = -34.0, Steps = 1786\n",
      "Episode 350: Reward = -34.0, Steps = 1786\n",
      "Episode 351: Reward = -34.0, Steps = 1786\n",
      "Episode 352: Reward = -34.0, Steps = 1786\n",
      "Episode 353: Reward = -34.0, Steps = 1786\n",
      "Episode 354: Reward = -34.0, Steps = 1786\n",
      "Episode 355: Reward = -34.0, Steps = 1786\n",
      "Episode 356: Reward = -34.0, Steps = 1786\n",
      "Episode 357: Reward = -34.0, Steps = 1786\n",
      "Episode 358: Reward = -34.0, Steps = 1786\n",
      "Episode 359: Reward = -34.0, Steps = 1786\n",
      "Episode 360: Reward = -34.0, Steps = 1786\n",
      "Episode 361: Reward = -34.0, Steps = 1786\n",
      "Episode 362: Reward = -34.0, Steps = 1786\n",
      "Episode 363: Reward = -34.0, Steps = 1786\n",
      "Episode 364: Reward = -34.0, Steps = 1786\n",
      "Episode 365: Reward = -34.0, Steps = 1786\n",
      "Episode 366: Reward = -34.0, Steps = 1786\n",
      "Episode 367: Reward = -34.0, Steps = 1786\n",
      "Episode 368: Reward = -34.0, Steps = 1786\n",
      "Episode 369: Reward = -34.0, Steps = 1786\n",
      "Episode 370: Reward = -34.0, Steps = 1786\n",
      "Episode 371: Reward = -34.0, Steps = 1786\n",
      "Episode 372: Reward = -34.0, Steps = 1786\n",
      "Episode 373: Reward = -34.0, Steps = 1786\n",
      "Episode 374: Reward = -34.0, Steps = 1786\n",
      "Episode 375: Reward = -34.0, Steps = 1786\n",
      "Episode 376: Reward = -34.0, Steps = 1786\n",
      "Episode 377: Reward = -34.0, Steps = 1786\n",
      "Episode 378: Reward = -34.0, Steps = 1786\n",
      "Episode 379: Reward = -34.0, Steps = 1786\n",
      "Episode 380: Reward = -34.0, Steps = 1786\n",
      "Episode 381: Reward = -34.0, Steps = 1786\n",
      "Episode 382: Reward = -34.0, Steps = 1786\n",
      "Episode 383: Reward = -34.0, Steps = 1786\n",
      "Episode 384: Reward = -34.0, Steps = 1786\n",
      "Episode 385: Reward = -34.0, Steps = 1786\n",
      "Episode 386: Reward = -34.0, Steps = 1786\n",
      "Episode 387: Reward = -34.0, Steps = 1786\n",
      "Episode 388: Reward = -34.0, Steps = 1786\n",
      "Episode 389: Reward = -34.0, Steps = 1786\n",
      "Episode 390: Reward = -34.0, Steps = 1786\n",
      "Episode 391: Reward = -34.0, Steps = 1786\n",
      "Episode 392: Reward = -34.0, Steps = 1786\n",
      "Episode 393: Reward = -34.0, Steps = 1786\n",
      "Episode 394: Reward = -34.0, Steps = 1786\n",
      "Episode 395: Reward = -34.0, Steps = 1786\n",
      "Episode 396: Reward = -34.0, Steps = 1786\n",
      "Episode 397: Reward = -34.0, Steps = 1786\n",
      "Episode 398: Reward = -34.0, Steps = 1786\n",
      "Episode 399: Reward = -34.0, Steps = 1786\n",
      "Episode 400: Reward = -34.0, Steps = 1786\n",
      "Episode 401: Reward = -34.0, Steps = 1786\n",
      "Episode 402: Reward = -34.0, Steps = 1786\n",
      "Episode 403: Reward = -34.0, Steps = 1786\n",
      "Episode 404: Reward = -34.0, Steps = 1786\n",
      "Episode 405: Reward = -34.0, Steps = 1786\n",
      "Episode 406: Reward = -34.0, Steps = 1786\n",
      "Episode 407: Reward = -34.0, Steps = 1786\n",
      "Episode 408: Reward = -34.0, Steps = 1786\n",
      "Episode 409: Reward = -34.0, Steps = 1786\n",
      "Episode 410: Reward = -34.0, Steps = 1786\n",
      "Episode 411: Reward = -34.0, Steps = 1786\n",
      "Episode 412: Reward = -34.0, Steps = 1786\n",
      "Episode 413: Reward = -34.0, Steps = 1786\n",
      "Episode 414: Reward = -34.0, Steps = 1786\n",
      "Episode 415: Reward = -34.0, Steps = 1786\n",
      "Episode 416: Reward = -34.0, Steps = 1786\n",
      "Episode 417: Reward = -34.0, Steps = 1786\n",
      "Episode 418: Reward = -34.0, Steps = 1786\n",
      "Episode 419: Reward = -34.0, Steps = 1786\n",
      "Episode 420: Reward = -34.0, Steps = 1786\n",
      "Episode 421: Reward = -34.0, Steps = 1786\n",
      "Episode 422: Reward = -34.0, Steps = 1786\n",
      "Episode 423: Reward = -34.0, Steps = 1786\n",
      "Episode 424: Reward = -34.0, Steps = 1786\n",
      "Episode 425: Reward = -34.0, Steps = 1786\n",
      "Episode 426: Reward = -34.0, Steps = 1786\n",
      "Episode 427: Reward = -34.0, Steps = 1786\n",
      "Episode 428: Reward = -34.0, Steps = 1786\n",
      "Episode 429: Reward = -34.0, Steps = 1786\n",
      "Episode 430: Reward = -34.0, Steps = 1786\n",
      "Episode 431: Reward = -34.0, Steps = 1786\n",
      "Episode 432: Reward = -34.0, Steps = 1786\n",
      "Episode 433: Reward = -34.0, Steps = 1786\n",
      "Episode 434: Reward = -34.0, Steps = 1786\n",
      "Episode 435: Reward = -34.0, Steps = 1786\n",
      "Episode 436: Reward = -34.0, Steps = 1786\n",
      "Episode 437: Reward = -34.0, Steps = 1786\n",
      "Episode 438: Reward = -34.0, Steps = 1786\n",
      "Episode 439: Reward = -34.0, Steps = 1786\n",
      "Episode 440: Reward = -34.0, Steps = 1786\n",
      "Episode 441: Reward = -34.0, Steps = 1786\n",
      "Episode 442: Reward = -34.0, Steps = 1786\n",
      "Episode 443: Reward = -34.0, Steps = 1786\n",
      "Episode 444: Reward = -34.0, Steps = 1786\n",
      "Episode 445: Reward = -34.0, Steps = 1786\n",
      "Episode 446: Reward = -34.0, Steps = 1786\n",
      "Episode 447: Reward = -34.0, Steps = 1786\n",
      "Episode 448: Reward = -34.0, Steps = 1786\n",
      "Episode 449: Reward = -34.0, Steps = 1786\n",
      "Episode 450: Reward = -34.0, Steps = 1786\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import math\n",
    "import imageio\n",
    "import ast\n",
    "\n",
    "# Enable anomaly detection for debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define the Policy Network for Discrete Actions\n",
    "class PolicyNetworkRAM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        # Define network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        # Output layer for logits\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output logits for Categorical distribution\n",
    "        return self.net(x)\n",
    "    \n",
    "    def update_policy(self, states, actions, q_values):\n",
    "        \"\"\"\n",
    "        Update the policy network.\n",
    "        Args:\n",
    "            states (Tensor): Current states [batch_size, state_dims]\n",
    "            actions (Tensor): Actions taken [batch_size, 1]\n",
    "            q_values (Tensor): Q-values corresponding to actions [batch_size]\n",
    "        Returns:\n",
    "            float: Policy loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.forward(states)  # [batch_size, num_actions]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions.squeeze(1))  # [batch_size]\n",
    "        entropy = dist.entropy()  # [batch_size]\n",
    "        \n",
    "        # Ensure q_values are detached to prevent gradients flowing back to Q-networks\n",
    "        q_values = q_values.detach()\n",
    "        \n",
    "        # Policy loss with entropy regularization\n",
    "        policy_loss = (self.alpha * log_probs - q_values).mean()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Gradient clipping (optional)\n",
    "        nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Calculate gradient norms for logging\n",
    "        total_norm = 0\n",
    "        for p in self.net.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = math.sqrt(total_norm)\n",
    "        \n",
    "        return policy_loss.item(), total_norm\n",
    "\n",
    "# Define the Q-Network\n",
    "class QNetworkRAM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_actions, learning_rate):\n",
    "        super().__init__()\n",
    "        # Define network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        # Output layer for Q-values\n",
    "        layers.append(nn.Linear(last_size, num_actions))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output Q-values for each action\n",
    "        return self.net(x)  # [batch_size, num_actions]\n",
    "    \n",
    "    def update_q_network(self, parameters, loss):\n",
    "        \"\"\"\n",
    "        Update Q-network parameters.\n",
    "        Args:\n",
    "            parameters (iterable): Network parameters to update.\n",
    "            loss (Tensor): Computed loss.\n",
    "        Returns:\n",
    "            float: Q-network loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters, max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# Define the Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = zip(*samples)\n",
    "        \n",
    "        # Convert lists to single NumPy arrays for efficiency\n",
    "        state_batch = np.array(state_batch)\n",
    "        action_batch = np.array(action_batch)\n",
    "        reward_batch = np.array(reward_batch)\n",
    "        next_state_batch = np.array(next_state_batch)\n",
    "        terminated_batch = np.array(terminated_batch)\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)  # [batch_size, state_dims]\n",
    "        action_batch = torch.LongTensor(action_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)  # [batch_size, state_dims]\n",
    "        terminated_batch = torch.FloatTensor(terminated_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define the SAC Agent for Discrete Actions\n",
    "class SACAgentRAM:\n",
    "    def __init__(self, env, config):\n",
    "        # Parse hyperparameters from config\n",
    "        self.gamma = config.gamma\n",
    "        self.alpha = config.alpha  # Entropy temperature\n",
    "        self.batch_size = config.batch_size\n",
    "        self.tau = config.tau  # Target smoothing coefficient\n",
    "        self.target_update_interval = config.target_update_interval\n",
    "        self.automatic_alpha = config.automatic_alpha\n",
    "        \n",
    "        # Validate observation space\n",
    "        continuous = isinstance(env.observation_space, gym.spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "        print(f\"State dimensions: {self.state_dims}\")\n",
    "    \n",
    "        # Validate action space\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "        print(f\"Number of actions: {self.num_actions}\")\n",
    "        \n",
    "        # Parse hidden_sizes from string to tuple if necessary\n",
    "        if isinstance(config.hidden_sizes, str):\n",
    "            hidden_sizes = ast.literal_eval(config.hidden_sizes)\n",
    "        else:\n",
    "            hidden_sizes = config.hidden_sizes\n",
    "        \n",
    "        # Initialize Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(config.replay_size)\n",
    "        \n",
    "        # Initialize Policy Network\n",
    "        self.policy_net = PolicyNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=self.num_actions,\n",
    "            learning_rate=config.lr_policy,\n",
    "            alpha=self.alpha\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize Q-Networks\n",
    "        self.q_net1 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        self.q_net2 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize Target Q-Networks\n",
    "        self.target_q_net1 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        self.target_q_net2 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        \n",
    "        # Synchronize target networks with main Q-networks\n",
    "        self.target_q_net1.load_state_dict(self.q_net1.state_dict())\n",
    "        self.target_q_net2.load_state_dict(self.q_net2.state_dict())\n",
    "        \n",
    "        # Automatic entropy tuning\n",
    "        if self.automatic_alpha:\n",
    "            self.target_entropy = -math.log(1.0 / self.num_actions) * 0.98  # Slightly lower than maximum\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.lr_policy)\n",
    "            print(\"Automatic alpha tuning enabled.\")\n",
    "        \n",
    "        # Store environment\n",
    "        self.env = env\n",
    "        \n",
    "        # Warm up the replay buffer\n",
    "        self.warm_up(initial_steps=1000)\n",
    "        \n",
    "    def normalize_state(self, state):\n",
    "        \"\"\"\n",
    "        Normalize the state observations.\n",
    "        Args:\n",
    "            state (array-like or Tensor): Raw state.\n",
    "        Returns:\n",
    "            Tensor: Normalized state tensor.\n",
    "        \"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            # Convert NumPy array to PyTorch tensor\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        elif isinstance(state, torch.Tensor):\n",
    "            # Ensure the tensor is on the correct device and of type float\n",
    "            state = state.float().to(device)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported state type: {type(state)}\")\n",
    "        \n",
    "        # Normalize the state\n",
    "        return state / 255.0\n",
    "\n",
    "    \n",
    "    def warm_up(self, initial_steps=1000):\n",
    "        \"\"\"\n",
    "        Populate the replay buffer with initial random experiences.\n",
    "        Args:\n",
    "            initial_steps (int): Number of initial random steps to perform.\n",
    "        \"\"\"\n",
    "        state, _ = self.env.reset()\n",
    "        for _ in range(initial_steps):\n",
    "            action = self.env.action_space.sample()  # Take random action\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            reward = np.clip(reward, -1, 1)  # Clip rewards\n",
    "            self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "            state = next_state\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "    \n",
    "    def policy(self, state, evaluate=False):\n",
    "        \"\"\"\n",
    "        Select action based on current policy.\n",
    "        Args:\n",
    "            state (array-like): Current state.\n",
    "            evaluate (bool): If True, select the best action deterministically.\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        state = self.normalize_state(state)\n",
    "        logits = self.policy_net(state)  # [1, num_actions]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if evaluate:\n",
    "            action = dist.probs.argmax(dim=1).item()\n",
    "        else:\n",
    "            action = dist.sample().item()\n",
    "        # Validate action\n",
    "        if not (0 <= action < self.num_actions):\n",
    "            raise ValueError(f\"Sampled action {action} is out of bounds.\")\n",
    "        \n",
    "        # Log action probabilities for debugging\n",
    "        action_probs = dist.probs.detach().cpu().numpy()[0]\n",
    "        wandb.log({\n",
    "            'action_probabilities': wandb.Histogram(action_probs),\n",
    "            'selected_action': action\n",
    "        })\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, updates):\n",
    "        \"\"\"\n",
    "        Update the SAC agent's networks.\n",
    "        Args:\n",
    "            updates (int): Current update step count.\n",
    "        Returns:\n",
    "            tuple: (q1_loss, q2_loss, policy_loss, entropy) or (None, None, None, None) if no update.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None, None, None, None  # Return None for all values when not updating\n",
    "\n",
    "        # Sample a minibatch from replay buffer\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        # Normalize states\n",
    "        state_batch = self.normalize_state(state_batch)\n",
    "        next_state_batch = self.normalize_state(next_state_batch)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            # Get logits and probabilities from the policy network for next states\n",
    "            next_logits = self.policy_net(next_state_batch)\n",
    "            next_probs = F.softmax(next_logits, dim=1)  # [batch_size, num_actions]\n",
    "            next_log_probs = F.log_softmax(next_logits, dim=1)  # [batch_size, num_actions]\n",
    "\n",
    "            # Get target Q-values from target networks\n",
    "            target_q1_values = self.target_q_net1(next_state_batch)  # [batch_size, num_actions]\n",
    "            target_q2_values = self.target_q_net2(next_state_batch)  # [batch_size, num_actions]\n",
    "            target_q_values = torch.min(target_q1_values, target_q2_values)  # [batch_size, num_actions]\n",
    "\n",
    "            # Compute expected Q-values for next states\n",
    "            expected_q = (next_probs * (target_q_values - self.alpha * next_log_probs)).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "\n",
    "            # Compute target Q\n",
    "            q_target = reward_batch + (1 - terminated_batch) * self.gamma * expected_q  # [batch_size, 1]\n",
    "\n",
    "        # Compute current Q-values from both Q-networks\n",
    "        current_q1 = self.q_net1(state_batch).gather(1, action_batch)  # [batch_size, 1]\n",
    "        current_q2 = self.q_net2(state_batch).gather(1, action_batch)  # [batch_size, 1]\n",
    "        \n",
    "        # Clip Q-values to prevent them from growing too large\n",
    "        current_q1 = torch.clamp(current_q1, min=-100, max=100)\n",
    "        current_q2 = torch.clamp(current_q2, min=-100, max=100)\n",
    "\n",
    "        # Compute Q-network losses\n",
    "        q1_loss = self.q_net1.criterion(current_q1, q_target)\n",
    "        q2_loss = self.q_net2.criterion(current_q2, q_target)\n",
    "\n",
    "        # Update Q-networks\n",
    "        q1_loss_value = self.q_net1.update_q_network(self.q_net1.parameters(), q1_loss)\n",
    "        q2_loss_value = self.q_net2.update_q_network(self.q_net2.parameters(), q2_loss)\n",
    "\n",
    "        # Compute policy loss and gradient norm\n",
    "        policy_loss_value, grad_norm = self.policy_net.update_policy(state_batch, action_batch, torch.min(self.q_net1(state_batch), self.q_net2(state_batch)).gather(1, action_batch).squeeze(1))\n",
    "        \n",
    "        # Compute entropy for logging\n",
    "        with torch.no_grad():\n",
    "            entropy = F.softmax(self.policy_net(state_batch), dim=1) * F.log_softmax(self.policy_net(state_batch), dim=1)\n",
    "            entropy = -entropy.sum(dim=1).mean().item()\n",
    "\n",
    "        # Optional: Update alpha for entropy temperature\n",
    "        if self.automatic_alpha:\n",
    "            # Recompute log_probs for entropy adjustment\n",
    "            logits = self.policy_net(state_batch)  # [batch_size, num_actions]\n",
    "            log_probs = F.log_softmax(logits, dim=1)  # [batch_size, num_actions]\n",
    "            entropy = - (log_probs * F.softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "            alpha_loss = -(self.log_alpha * (entropy - self.target_entropy)).mean()\n",
    "\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "\n",
    "            # Clamp alpha to prevent it from becoming too small\n",
    "            self.alpha = torch.clamp(self.log_alpha.exp(), min=0.01).detach()\n",
    "            #print(f\"Updated alpha: {self.alpha.item():.4f}, Alpha Loss: {alpha_loss.item():.4f}\")\n",
    "\n",
    "        # Update target networks\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            self.soft_update(self.q_net1, self.target_q_net1)\n",
    "            self.soft_update(self.q_net2, self.target_q_net2)\n",
    "            #print(\"Target networks updated.\")\n",
    "\n",
    "        return q1_loss_value, q2_loss_value, policy_loss_value, entropy\n",
    "    \n",
    "    def soft_update(self, net, target_net):\n",
    "        \"\"\"\n",
    "        Perform soft update of target network parameters.\n",
    "        Args:\n",
    "            net (nn.Module): Main network.\n",
    "            target_net (nn.Module): Target network.\n",
    "        \"\"\"\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "    \n",
    "    def train_agent(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        \"\"\"\n",
    "        Train the SAC agent.\n",
    "        Args:\n",
    "            max_episodes (int): Maximum number of episodes for training.\n",
    "            stop_criterion (function): Function to determine stopping condition.\n",
    "            criterion_episodes (int): Number of episodes to evaluate the stopping criterion.\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "        updates = 0\n",
    "        best_avg_reward = -float('inf')\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            while not (terminated or truncated) and steps < 10000:  # Prevent infinite loops\n",
    "                # Select action by following policy\n",
    "                action = self.policy(state)\n",
    "\n",
    "                # Send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Clip reward\n",
    "                reward = np.clip(reward, -1, 1)\n",
    "\n",
    "                # Add experience to replay buffer\n",
    "                self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                # Update Q-network and policy\n",
    "                update_results = self.update(updates)\n",
    "                if update_results[0] is not None:\n",
    "                    q1_loss, q2_loss, policy_loss, entropy = update_results\n",
    "                    updates += 1\n",
    "                else:\n",
    "                    q1_loss, q2_loss, policy_loss, entropy = (None, None, None, None)\n",
    "\n",
    "            # Append the reward for this episode\n",
    "            total_rewards.append(episode_reward)\n",
    "\n",
    "            # Prepare logging dictionary\n",
    "            log_dict = {\n",
    "                'episode': episode,\n",
    "                'episode_reward': episode_reward,\n",
    "                'steps': steps,\n",
    "                'updates': updates,\n",
    "                'alpha': self.alpha.item() if self.automatic_alpha else self.alpha,\n",
    "            }\n",
    "            if q1_loss is not None:\n",
    "                log_dict.update({\n",
    "                    'q1_loss': q1_loss,\n",
    "                    'q2_loss': q2_loss,\n",
    "                    'policy_loss': policy_loss,\n",
    "                    'entropy': entropy,\n",
    "                })\n",
    "\n",
    "            # Log metrics to W&B\n",
    "            wandb.log(log_dict)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Steps = {steps}\")\n",
    "\n",
    "            # Check stopping criterion\n",
    "            if episode >= criterion_episodes:\n",
    "                recent_rewards = total_rewards[-criterion_episodes:]\n",
    "                if stop_criterion(recent_rewards):\n",
    "                    print(f\"\\nStopping criterion satisfied after {episode} episodes\")\n",
    "                    break\n",
    "\n",
    "            # Check and save the best model\n",
    "            if len(total_rewards) >= criterion_episodes:\n",
    "                avg_reward = np.mean(total_rewards[-criterion_episodes:])\n",
    "                if avg_reward > best_avg_reward:\n",
    "                    best_avg_reward = avg_reward\n",
    "                    self.save_model(f'sac_discrete_boxing_ram_best_episode_{episode}.pth')\n",
    "                    print(f\"New best average reward: {best_avg_reward:.2f}. Model saved.\")\n",
    "\n",
    "        # Plot rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(total_rewards)+1), total_rewards, label='Rewards per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model parameters.\n",
    "        Args:\n",
    "            path (str): File path to save the model.\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'q_net1': self.q_net1.state_dict(),\n",
    "            'q_net2': self.q_net2.state_dict(),\n",
    "            'target_q_net1': self.target_q_net1.state_dict(),\n",
    "            'target_q_net2': self.target_q_net2.state_dict(),\n",
    "            'alpha': self.alpha,\n",
    "            'log_alpha': self.log_alpha if self.automatic_alpha else None\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Load the model parameters.\n",
    "        Args:\n",
    "            path (str): File path to load the model from.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.q_net1.load_state_dict(checkpoint['q_net1'])\n",
    "        self.q_net2.load_state_dict(checkpoint['q_net2'])\n",
    "        self.target_q_net1.load_state_dict(checkpoint['target_q_net1'])\n",
    "        self.target_q_net2.load_state_dict(checkpoint['target_q_net2'])\n",
    "        if self.automatic_alpha and 'log_alpha' in checkpoint and checkpoint['log_alpha'] is not None:\n",
    "            self.log_alpha.data.copy_(checkpoint['log_alpha'])\n",
    "            self.alpha = torch.clamp(self.log_alpha.exp(), min=0.01).detach()\n",
    "        else:\n",
    "            self.alpha = checkpoint['alpha']\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "# Define the Sweep Agent Wrapper\n",
    "class AgentSAC:\n",
    "    def __init__(self, env, config):\n",
    "        # Instantiate SACAgentRAM with environment and config\n",
    "        self.agent = SACAgentRAM(env, config)\n",
    "    \n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # Delegate training to SACAgentRAM\n",
    "        self.agent.train_agent(max_episodes, stop_criterion, criterion_episodes)\n",
    "    \n",
    "    def save(self, path):\n",
    "        # Delegate model saving to SACAgentRAM\n",
    "        self.agent.save_model(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        # Delegate model loading to SACAgentRAM\n",
    "        self.agent.load_model(path)\n",
    "\n",
    "# Define the Sweep Agent Function\n",
    "def sweep_agent():\n",
    "    try:\n",
    "        # Initialize a new wandb run\n",
    "        with wandb.init() as run:\n",
    "            config = wandb.config\n",
    "\n",
    "            # Create the environment with RAM observation type\n",
    "            env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "            print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "            print(\"Action Space:\", env.action_space)\n",
    "\n",
    "            # Ensure the observation is a 128-length vector\n",
    "            assert env.observation_space.shape == (128,), \"Observation space must be a 128-length vector.\"\n",
    "\n",
    "            # Instantiate AgentSAC with current wandb config\n",
    "            agent = AgentSAC(env=env, config=config)\n",
    "\n",
    "            # Define stopping criterion (optional)\n",
    "            def stopping_criterion(rewards):\n",
    "                # Example: stop if average reward over last 5 episodes >= 100\n",
    "                return np.mean(rewards) >= 100\n",
    "\n",
    "            # Start training\n",
    "            agent.train(\n",
    "                max_episodes=config.num_episodes,\n",
    "                stop_criterion=stopping_criterion,\n",
    "                criterion_episodes=config.criterion_episodes\n",
    "            )\n",
    "\n",
    "            # Save the trained model\n",
    "            agent.save_model('sac_discrete_boxing_ram.pth')\n",
    "\n",
    "            # Close the environment after training\n",
    "            env.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error to wandb\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Ensure the environment is closed in case of an error\n",
    "        try:\n",
    "            env.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Define the Sweep Configuration for SAC\n",
    "sweep_configuration_sac = {\n",
    "    \"method\": \"random\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"episode_reward\",  # The metric to optimize\n",
    "        \"goal\": \"maximize\"         # Whether to \"minimize\" or \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr_policy\": {  # Learning rate for the policy network\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-7,\n",
    "            \"max\": 3e-4\n",
    "        },\n",
    "        \"lr_q\": {  # Learning rate for the Q-networks\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-7,\n",
    "            \"max\": 3e-4\n",
    "        },\n",
    "        \"gamma\": {  # Discount factor for future rewards\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.95,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"alpha\": {  # Entropy temperature (only if automatic_alpha is False)\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 0.5\n",
    "        },\n",
    "        \"tau\": {  # Soft update coefficient for target networks\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.001,\n",
    "            \"max\": 0.01\n",
    "        },\n",
    "        \"batch_size\": {  # Mini-batch size for updates\n",
    "            \"values\": [64, 128]  # Two discrete values\n",
    "        },\n",
    "        \"replay_size\": {  # Replay buffer size\n",
    "            \"values\": [50000, 100000]  # Two discrete values\n",
    "        },\n",
    "        \"hidden_sizes\": {  # Architecture of hidden layers\n",
    "            \"values\": [\"(256,256)\", \"(512,512)\", \"(256,256,256)\"]  \n",
    "        },\n",
    "        \"target_update_interval\": {  # Frequency of target network updates\n",
    "            \"values\": [1]  # Fixed to 1 for SAC\n",
    "        },\n",
    "        \"num_episodes\": {  # Total number of training episodes\n",
    "            \"values\": [500]  # Fixed to 500\n",
    "        },\n",
    "        \"criterion_episodes\": {  # Number of episodes for stopping criterion\n",
    "            \"values\": [5]  # Fixed to 5\n",
    "        },\n",
    "        \"automatic_alpha\": {  # Whether to use automatic entropy tuning\n",
    "            \"values\": [True, False]  # Toggle between automatic and manual\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration_sac, project='SAC_Discrete_RAM')\n",
    "\n",
    "# Launch the Sweep with Limited Runs\n",
    "# Adjust 'count' based on your computational resources\n",
    "wandb.agent(sweep_id, function=sweep_agent, count=50)  # Example: 20 runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f2779-e112-4be3-8bec-4e1b28d805ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Soft Actor-Critic (SAC) Image Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42648f-1840-476b-a020-8468445ce244",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53340356-3b5a-44fb-b788-1731cb863ef2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Added class replay buffer which stores transitions (state, action, reward, next_state, terminated) for off-policy learning. Itsa deque with a fixed capacity, supporting sampling of random batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d54174-534d-44d1-a3f8-3ff8a72528a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = zip(*samples)\n",
    "        return (np.array(state_batch), np.array(action_batch), np.array(reward_batch),\n",
    "                np.array(next_state_batch), np.array(terminated_batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92712978-3b09-4347-90cd-7ae3ff8c10bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Updated Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc89ef-a04d-4951-805e-ec39ab077db6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Outputs logits for a categorical distribution over actions and uses the categorical distribution to sample actions and compute log probabilities. Also includes entropy regularization in the policy loss to encourage exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9752edb6-6aa0-4238-99e0-bb9b90197296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Entropy temperature coefficient\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # Input channels = 3 (RGB)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the size after CNN layers\n",
    "        self.flattened_size = self._get_flattened_size(input_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.flattened_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer for action probabilities\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def _get_flattened_size(self, input_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, input_size[0], input_size[1])  # Batch size 1\n",
    "            conv_out = self.conv_net(dummy_input)\n",
    "            return conv_out.view(1, -1).size(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.net(x)\n",
    "        return logits  # Logits for categorical distribution\n",
    "    \n",
    "    def sample(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob, dist.probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c98780c-369d-4217-b195-3cbb47a67959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### QNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbe997-4439-4047-8f8d-c6a2d0b39582",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Two Q-Networks (q_net1 and q_net2): Mitigate overestimation bias by taking the minimum of the two estimates.\n",
    "Target Networks: target_q_net1 and target_q_net2 are used to compute target Q-values for stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1657eb-99eb-4a3b-9eb0-843433a51444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_actions, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),  # Input channels = 3 (RGB)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the size after CNN layers\n",
    "        self.flattened_size = self._get_flattened_size(input_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.flattened_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer for Q-values for each action\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], num_actions))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def _get_flattened_size(self, input_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, input_size[0], input_size[1])  # Batch size 1\n",
    "            conv_out = self.conv_net(dummy_input)\n",
    "            return conv_out.view(1, -1).size(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        q_values = self.net(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47008e8a-6bf7-48dd-ba74-f714575c31cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Updated Agent SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b876a-7136-4108-96d1-49fef0874cba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Lots of modifications\n",
    "Parameters:\n",
    "gamma: Discount factor.\n",
    "alpha: Entropy temperature coefficient. Balances exploration and exploitation.\n",
    "tau: Soft update coefficient for the target networks.\n",
    "Methods:\n",
    "select_action: Chooses an action based on the policy network, either for training (sampling) or evaluation (greedy).\n",
    "update_parameters: Updates the Q-networks and policy network using samples from the replay buffer.\n",
    "soft_update: Performs a soft update of the target networks' parameters.\n",
    "train: Runs the training loop, collecting experiences and updating the networks.\n",
    "save and load: Save and load model parameters.\n",
    "Training Loop\n",
    "Episodes: Runs for a specified number of episodes.\n",
    "Interaction: In each step, the agent selects an action, observes the next state and reward, and stores the transition in the replay buffer.\n",
    "Parameter Updates: After each step, the agent updates the networks using samples from the replay buffer.\n",
    "Logging: Prints the total reward for each episode and plots the rewards over time.\n",
    "Entropy Regularization\n",
    "Policy Loss: Incorporates the entropy term 𝛼𝐸𝑎𝑡∼𝜋[−log𝜋(𝑎𝑡∣𝑠𝑡)] to encourage exploration.\n",
    "Adjustable 𝛼: The entropy temperature can be fixed or learned during training for adaptive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed83fd3b-9b12-4a6a-bfb3-fc418e19c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.2, hidden_sizes=(256, 256),\n",
    "                 lr_policy=1e-4, lr_q=1e-3, replay_size=100000, batch_size=64,\n",
    "                 target_update_interval=1, tau=0.005):\n",
    "        \n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Entropy temperature\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau  # Target smoothing coefficient\n",
    "        self.target_update_interval = target_update_interval\n",
    "        \n",
    "        self.num_actions = env.action_space.n\n",
    "        self.observation_shape = env.observation_space.shape\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = PolicyNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_policy, alpha)\n",
    "        \n",
    "        # Q-networks\n",
    "        self.q_net1 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q)\n",
    "        self.q_net2 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q)\n",
    "        \n",
    "        # Target Q-networks\n",
    "        self.target_q_net1 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q)\n",
    "        self.target_q_net2 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q)\n",
    "        \n",
    "        # Copy parameters from Q-networks to target networks\n",
    "        self.target_q_net1.load_state_dict(self.q_net1.state_dict())\n",
    "        self.target_q_net2.load_state_dict(self.q_net2.state_dict())\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).permute(0, 3, 1, 2)  # (1, C, H, W)\n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                logits = self.policy_net(state)\n",
    "                action = torch.argmax(logits, dim=-1).item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = self.policy_net.sample(state)\n",
    "                action = action.item()\n",
    "        return action\n",
    "    \n",
    "    def update_parameters(self, updates):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert batches to tensors\n",
    "        state_batch = torch.FloatTensor(state_batch).permute(0, 3, 1, 2)  # (batch_size, C, H, W)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).permute(0, 3, 1, 2)\n",
    "        action_batch = torch.LongTensor(action_batch)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1)\n",
    "        terminated_batch = torch.FloatTensor(terminated_batch).unsqueeze(1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Next action probabilities and log probabilities\n",
    "            next_logits = self.policy_net(next_state_batch)\n",
    "            next_probs = F.softmax(next_logits, dim=1)\n",
    "            next_log_probs = F.log_softmax(next_logits, dim=1)\n",
    "            \n",
    "            # Compute target Q-values\n",
    "            target_q1_values = self.target_q_net1(next_state_batch)\n",
    "            target_q2_values = self.target_q_net2(next_state_batch)\n",
    "            target_q_values = torch.min(target_q1_values, target_q2_values)\n",
    "            next_q = (next_probs * (target_q_values - self.alpha * next_log_probs)).sum(dim=1, keepdim=True)\n",
    "            \n",
    "            # Compute target values\n",
    "            q_target = reward_batch + (1 - terminated_batch) * self.gamma * next_q\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        current_q1 = self.q_net1(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        current_q2 = self.q_net2(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "        # Compute Q-network losses\n",
    "        q1_loss = self.criterion(current_q1, q_target)\n",
    "        q2_loss = self.criterion(current_q2, q_target)\n",
    "        \n",
    "        # Update Q-networks\n",
    "        self.q_net1.optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q_net1.optimizer.step()\n",
    "        \n",
    "        self.q_net2.optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q_net2.optimizer.step()\n",
    "        \n",
    "        # Update policy network\n",
    "        logits = self.policy_net(state_batch)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        q1_values = self.q_net1(state_batch)\n",
    "        q2_values = self.q_net2(state_batch)\n",
    "        min_q_values = torch.min(q1_values, q2_values)\n",
    "        \n",
    "        policy_loss = (probs * (self.alpha * log_probs - min_q_values)).sum(dim=1).mean()\n",
    "        \n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_net.optimizer.step()\n",
    "        \n",
    "        # Update target networks\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            self.soft_update(self.q_net1, self.target_q_net1)\n",
    "            self.soft_update(self.q_net2, self.target_q_net2)\n",
    "    \n",
    "    def soft_update(self, net, target_net):\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def train(self, num_episodes, max_steps_per_episode, updates_per_step):\n",
    "        total_rewards = []\n",
    "        updates = 0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for step in range(max_steps_per_episode):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "                state = next_state\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(updates)\n",
    "                updates += 1\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "        \n",
    "        # Plotting rewards\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(total_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'q_net1': self.q_net1.state_dict(),\n",
    "            'q_net2': self.q_net2.state_dict(),\n",
    "            'target_q_net1': self.target_q_net1.state_dict(),\n",
    "            'target_q_net2': self.target_q_net2.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.q_net1.load_state_dict(checkpoint['q_net1'])\n",
    "        self.q_net2.load_state_dict(checkpoint['q_net2'])\n",
    "        self.target_q_net1.load_state_dict(checkpoint['target_q_net1'])\n",
    "        self.target_q_net2.load_state_dict(checkpoint['target_q_net2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f856aea-41c3-4206-9c5b-4afdee9b43e6",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -2.0\n",
      "Episode 2: Total Reward = -2.0\n",
      "Episode 3: Total Reward = 2.0\n",
      "Episode 4: Total Reward = 6.0\n",
      "Episode 5: Total Reward = -1.0\n",
      "Episode 6: Total Reward = -3.0\n",
      "Episode 7: Total Reward = 0.0\n",
      "Episode 8: Total Reward = 7.0\n",
      "Episode 9: Total Reward = -2.0\n",
      "Episode 10: Total Reward = 0.0\n",
      "Episode 11: Total Reward = -12.0\n",
      "Episode 12: Total Reward = -9.0\n",
      "Episode 13: Total Reward = -1.0\n",
      "Episode 14: Total Reward = 0.0\n",
      "Episode 15: Total Reward = 0.0\n",
      "Episode 16: Total Reward = 6.0\n",
      "Episode 17: Total Reward = 1.0\n",
      "Episode 18: Total Reward = -1.0\n",
      "Episode 19: Total Reward = 0.0\n",
      "Episode 20: Total Reward = 1.0\n",
      "Episode 21: Total Reward = -1.0\n",
      "Episode 22: Total Reward = -7.0\n",
      "Episode 23: Total Reward = -8.0\n",
      "Episode 24: Total Reward = 4.0\n",
      "Episode 25: Total Reward = 8.0\n",
      "Episode 26: Total Reward = 5.0\n",
      "Episode 27: Total Reward = -5.0\n",
      "Episode 28: Total Reward = 1.0\n",
      "Episode 29: Total Reward = -5.0\n",
      "Episode 30: Total Reward = -3.0\n",
      "Episode 31: Total Reward = -17.0\n",
      "Episode 32: Total Reward = -3.0\n",
      "Episode 33: Total Reward = 4.0\n",
      "Episode 34: Total Reward = -4.0\n",
      "Episode 35: Total Reward = -2.0\n",
      "Episode 36: Total Reward = 0.0\n",
      "Episode 37: Total Reward = 1.0\n",
      "Episode 38: Total Reward = -1.0\n",
      "Episode 39: Total Reward = 8.0\n",
      "Episode 40: Total Reward = 0.0\n",
      "Episode 41: Total Reward = 6.0\n",
      "Episode 42: Total Reward = 2.0\n",
      "Episode 43: Total Reward = 0.0\n",
      "Episode 44: Total Reward = -3.0\n",
      "Episode 45: Total Reward = -4.0\n",
      "Episode 46: Total Reward = 5.0\n",
      "Episode 47: Total Reward = -2.0\n",
      "Episode 48: Total Reward = -1.0\n",
      "Episode 49: Total Reward = 1.0\n",
      "Episode 50: Total Reward = 1.0\n",
      "Episode 51: Total Reward = 0.0\n",
      "Episode 52: Total Reward = -1.0\n",
      "Episode 53: Total Reward = 0.0\n",
      "Episode 54: Total Reward = -3.0\n",
      "Episode 55: Total Reward = -2.0\n",
      "Episode 56: Total Reward = -6.0\n",
      "Episode 57: Total Reward = -3.0\n",
      "Episode 58: Total Reward = 7.0\n",
      "Episode 59: Total Reward = 5.0\n",
      "Episode 60: Total Reward = -12.0\n",
      "Episode 61: Total Reward = 11.0\n",
      "Episode 62: Total Reward = 0.0\n",
      "Episode 63: Total Reward = 5.0\n",
      "Episode 64: Total Reward = 4.0\n",
      "Episode 65: Total Reward = 6.0\n",
      "Episode 66: Total Reward = -23.0\n",
      "Episode 67: Total Reward = -2.0\n",
      "Episode 68: Total Reward = -2.0\n",
      "Episode 69: Total Reward = -12.0\n",
      "Episode 70: Total Reward = -2.0\n",
      "Episode 71: Total Reward = 2.0\n",
      "Episode 72: Total Reward = 3.0\n",
      "Episode 73: Total Reward = -3.0\n",
      "Episode 74: Total Reward = -8.0\n",
      "Episode 75: Total Reward = -9.0\n",
      "Episode 76: Total Reward = -5.0\n",
      "Episode 77: Total Reward = -2.0\n",
      "Episode 78: Total Reward = -3.0\n",
      "Episode 79: Total Reward = 5.0\n",
      "Episode 80: Total Reward = 6.0\n",
      "Episode 81: Total Reward = 3.0\n",
      "Episode 82: Total Reward = -1.0\n",
      "Episode 83: Total Reward = 0.0\n",
      "Episode 84: Total Reward = 4.0\n",
      "Episode 85: Total Reward = -2.0\n",
      "Episode 86: Total Reward = -6.0\n",
      "Episode 87: Total Reward = 4.0\n",
      "Episode 88: Total Reward = -7.0\n",
      "Episode 89: Total Reward = 4.0\n",
      "Episode 90: Total Reward = -3.0\n",
      "Episode 91: Total Reward = 3.0\n",
      "Episode 92: Total Reward = -8.0\n",
      "Episode 93: Total Reward = 2.0\n",
      "Episode 94: Total Reward = 4.0\n",
      "Episode 95: Total Reward = 5.0\n",
      "Episode 96: Total Reward = -4.0\n",
      "Episode 97: Total Reward = -1.0\n",
      "Episode 98: Total Reward = -2.0\n",
      "Episode 99: Total Reward = -6.0\n",
      "Episode 100: Total Reward = -12.0\n",
      "Episode 101: Total Reward = 0.0\n",
      "Episode 102: Total Reward = -4.0\n",
      "Episode 103: Total Reward = -7.0\n",
      "Episode 104: Total Reward = -2.0\n",
      "Episode 105: Total Reward = -5.0\n",
      "Episode 106: Total Reward = -4.0\n",
      "Episode 107: Total Reward = -5.0\n",
      "Episode 108: Total Reward = 1.0\n",
      "Episode 109: Total Reward = -3.0\n",
      "Episode 110: Total Reward = 1.0\n",
      "Episode 111: Total Reward = 4.0\n",
      "Episode 112: Total Reward = 4.0\n",
      "Episode 113: Total Reward = -5.0\n",
      "Episode 114: Total Reward = -2.0\n",
      "Episode 115: Total Reward = -1.0\n",
      "Episode 116: Total Reward = 2.0\n",
      "Episode 117: Total Reward = 0.0\n",
      "Episode 118: Total Reward = -13.0\n",
      "Episode 119: Total Reward = -8.0\n",
      "Episode 120: Total Reward = -1.0\n",
      "Episode 121: Total Reward = 2.0\n",
      "Episode 122: Total Reward = -7.0\n",
      "Episode 123: Total Reward = -16.0\n",
      "Episode 124: Total Reward = -1.0\n",
      "Episode 125: Total Reward = -12.0\n",
      "Episode 126: Total Reward = -7.0\n",
      "Episode 127: Total Reward = 1.0\n",
      "Episode 128: Total Reward = 4.0\n",
      "Episode 129: Total Reward = -2.0\n",
      "Episode 130: Total Reward = -2.0\n",
      "Episode 131: Total Reward = -4.0\n",
      "Episode 132: Total Reward = -4.0\n",
      "Episode 133: Total Reward = -5.0\n",
      "Episode 134: Total Reward = -1.0\n",
      "Episode 135: Total Reward = 2.0\n",
      "Episode 136: Total Reward = 4.0\n",
      "Episode 137: Total Reward = -2.0\n",
      "Episode 138: Total Reward = -4.0\n",
      "Episode 139: Total Reward = -5.0\n",
      "Episode 140: Total Reward = 5.0\n",
      "Episode 141: Total Reward = 2.0\n",
      "Episode 142: Total Reward = -3.0\n",
      "Episode 143: Total Reward = -5.0\n",
      "Episode 144: Total Reward = -3.0\n",
      "Episode 145: Total Reward = 4.0\n",
      "Episode 146: Total Reward = 1.0\n",
      "Episode 147: Total Reward = 5.0\n",
      "Episode 148: Total Reward = 8.0\n",
      "Episode 149: Total Reward = -1.0\n",
      "Episode 150: Total Reward = -2.0\n",
      "Episode 151: Total Reward = 3.0\n",
      "Episode 152: Total Reward = -3.0\n",
      "Episode 153: Total Reward = -1.0\n",
      "Episode 154: Total Reward = 5.0\n",
      "Episode 155: Total Reward = 0.0\n",
      "Episode 156: Total Reward = 4.0\n",
      "Episode 157: Total Reward = 0.0\n",
      "Episode 158: Total Reward = 0.0\n",
      "Episode 159: Total Reward = -3.0\n",
      "Episode 160: Total Reward = 7.0\n",
      "Episode 161: Total Reward = -4.0\n",
      "Episode 162: Total Reward = -3.0\n",
      "Episode 163: Total Reward = -2.0\n",
      "Episode 164: Total Reward = -16.0\n",
      "Episode 165: Total Reward = -9.0\n",
      "Episode 166: Total Reward = -3.0\n",
      "Episode 167: Total Reward = -13.0\n",
      "Episode 168: Total Reward = 0.0\n",
      "Episode 169: Total Reward = 3.0\n",
      "Episode 170: Total Reward = -2.0\n",
      "Episode 171: Total Reward = 2.0\n",
      "Episode 172: Total Reward = 4.0\n",
      "Episode 173: Total Reward = 2.0\n",
      "Episode 174: Total Reward = -10.0\n",
      "Episode 175: Total Reward = 0.0\n",
      "Episode 176: Total Reward = -15.0\n",
      "Episode 177: Total Reward = -7.0\n",
      "Episode 178: Total Reward = 7.0\n",
      "Episode 179: Total Reward = 0.0\n",
      "Episode 180: Total Reward = -8.0\n",
      "Episode 181: Total Reward = 6.0\n",
      "Episode 182: Total Reward = -4.0\n",
      "Episode 183: Total Reward = 2.0\n",
      "Episode 184: Total Reward = -2.0\n",
      "Episode 185: Total Reward = -4.0\n",
      "Episode 186: Total Reward = 6.0\n",
      "Episode 187: Total Reward = 0.0\n",
      "Episode 188: Total Reward = -9.0\n",
      "Episode 189: Total Reward = 1.0\n",
      "Episode 190: Total Reward = -6.0\n",
      "Episode 191: Total Reward = 1.0\n",
      "Episode 192: Total Reward = -4.0\n",
      "Episode 193: Total Reward = -8.0\n",
      "Episode 194: Total Reward = -4.0\n",
      "Episode 195: Total Reward = 0.0\n",
      "Episode 196: Total Reward = 3.0\n",
      "Episode 197: Total Reward = -2.0\n",
      "Episode 198: Total Reward = 1.0\n",
      "Episode 199: Total Reward = -2.0\n",
      "Episode 200: Total Reward = 1.0\n",
      "Episode 201: Total Reward = 3.0\n",
      "Episode 202: Total Reward = -16.0\n",
      "Episode 203: Total Reward = -4.0\n",
      "Episode 204: Total Reward = 5.0\n",
      "Episode 205: Total Reward = -1.0\n",
      "Episode 206: Total Reward = 7.0\n",
      "Episode 207: Total Reward = 4.0\n",
      "Episode 208: Total Reward = 6.0\n",
      "Episode 209: Total Reward = 6.0\n",
      "Episode 210: Total Reward = 0.0\n",
      "Episode 211: Total Reward = -9.0\n",
      "Episode 212: Total Reward = 1.0\n",
      "Episode 213: Total Reward = -5.0\n",
      "Episode 214: Total Reward = 1.0\n",
      "Episode 215: Total Reward = -2.0\n",
      "Episode 216: Total Reward = 6.0\n",
      "Episode 217: Total Reward = -5.0\n",
      "Episode 218: Total Reward = 0.0\n",
      "Episode 219: Total Reward = 1.0\n",
      "Episode 220: Total Reward = -8.0\n",
      "Episode 221: Total Reward = -7.0\n",
      "Episode 222: Total Reward = -1.0\n",
      "Episode 223: Total Reward = -9.0\n",
      "Episode 224: Total Reward = -4.0\n",
      "Episode 225: Total Reward = -2.0\n",
      "Episode 226: Total Reward = 8.0\n",
      "Episode 227: Total Reward = 1.0\n",
      "Episode 228: Total Reward = -5.0\n",
      "Episode 229: Total Reward = -1.0\n",
      "Episode 230: Total Reward = -23.0\n",
      "Episode 231: Total Reward = 5.0\n",
      "Episode 232: Total Reward = -2.0\n",
      "Episode 233: Total Reward = 0.0\n",
      "Episode 234: Total Reward = -8.0\n",
      "Episode 235: Total Reward = -4.0\n",
      "Episode 236: Total Reward = -9.0\n",
      "Episode 237: Total Reward = 1.0\n",
      "Episode 238: Total Reward = 0.0\n",
      "Episode 239: Total Reward = 0.0\n",
      "Episode 240: Total Reward = -5.0\n",
      "Episode 241: Total Reward = -4.0\n",
      "Episode 242: Total Reward = -8.0\n",
      "Episode 243: Total Reward = 3.0\n",
      "Episode 244: Total Reward = -6.0\n",
      "Episode 245: Total Reward = 4.0\n",
      "Episode 246: Total Reward = 0.0\n",
      "Episode 247: Total Reward = -1.0\n",
      "Episode 248: Total Reward = 1.0\n",
      "Episode 249: Total Reward = 6.0\n",
      "Episode 250: Total Reward = 2.0\n",
      "Episode 251: Total Reward = 2.0\n",
      "Episode 252: Total Reward = -4.0\n",
      "Episode 253: Total Reward = 1.0\n",
      "Episode 254: Total Reward = -2.0\n",
      "Episode 255: Total Reward = -3.0\n",
      "Episode 256: Total Reward = -14.0\n",
      "Episode 257: Total Reward = -1.0\n",
      "Episode 258: Total Reward = -4.0\n",
      "Episode 259: Total Reward = 7.0\n",
      "Episode 260: Total Reward = -20.0\n",
      "Episode 261: Total Reward = 1.0\n",
      "Episode 262: Total Reward = -1.0\n",
      "Episode 263: Total Reward = -8.0\n",
      "Episode 264: Total Reward = -2.0\n",
      "Episode 265: Total Reward = 0.0\n",
      "Episode 266: Total Reward = -5.0\n",
      "Episode 267: Total Reward = -21.0\n",
      "Episode 268: Total Reward = 2.0\n",
      "Episode 269: Total Reward = -1.0\n",
      "Episode 270: Total Reward = -15.0\n",
      "Episode 271: Total Reward = -4.0\n",
      "Episode 272: Total Reward = -9.0\n",
      "Episode 273: Total Reward = 2.0\n",
      "Episode 274: Total Reward = -2.0\n",
      "Episode 275: Total Reward = -1.0\n",
      "Episode 276: Total Reward = 0.0\n",
      "Episode 277: Total Reward = -11.0\n",
      "Episode 278: Total Reward = 5.0\n",
      "Episode 279: Total Reward = -9.0\n",
      "Episode 280: Total Reward = -1.0\n",
      "Episode 281: Total Reward = 3.0\n",
      "Episode 282: Total Reward = -2.0\n",
      "Episode 283: Total Reward = 0.0\n",
      "Episode 284: Total Reward = 4.0\n",
      "Episode 285: Total Reward = -7.0\n",
      "Episode 286: Total Reward = 5.0\n",
      "Episode 287: Total Reward = 6.0\n",
      "Episode 288: Total Reward = 1.0\n",
      "Episode 289: Total Reward = -5.0\n",
      "Episode 290: Total Reward = 11.0\n",
      "Episode 291: Total Reward = -8.0\n",
      "Episode 292: Total Reward = -6.0\n",
      "Episode 293: Total Reward = -2.0\n",
      "Episode 294: Total Reward = -13.0\n",
      "Episode 295: Total Reward = 1.0\n",
      "Episode 296: Total Reward = 0.0\n",
      "Episode 297: Total Reward = -3.0\n",
      "Episode 298: Total Reward = 0.0\n",
      "Episode 299: Total Reward = -6.0\n",
      "Episode 300: Total Reward = -3.0\n",
      "Episode 301: Total Reward = 5.0\n",
      "Episode 302: Total Reward = -8.0\n",
      "Episode 303: Total Reward = 16.0\n",
      "Episode 304: Total Reward = 5.0\n",
      "Episode 305: Total Reward = 5.0\n",
      "Episode 306: Total Reward = -1.0\n",
      "Episode 307: Total Reward = 3.0\n",
      "Episode 308: Total Reward = 0.0\n",
      "Episode 309: Total Reward = -9.0\n",
      "Episode 310: Total Reward = -4.0\n",
      "Episode 311: Total Reward = 6.0\n",
      "Episode 312: Total Reward = 0.0\n",
      "Episode 313: Total Reward = 2.0\n",
      "Episode 314: Total Reward = -1.0\n",
      "Episode 315: Total Reward = 8.0\n",
      "Episode 316: Total Reward = -3.0\n",
      "Episode 317: Total Reward = -19.0\n",
      "Episode 318: Total Reward = 5.0\n",
      "Episode 319: Total Reward = -6.0\n",
      "Episode 320: Total Reward = 0.0\n",
      "Episode 321: Total Reward = -2.0\n",
      "Episode 322: Total Reward = -2.0\n",
      "Episode 323: Total Reward = -11.0\n",
      "Episode 324: Total Reward = -9.0\n",
      "Episode 325: Total Reward = -1.0\n",
      "Episode 326: Total Reward = -4.0\n",
      "Episode 327: Total Reward = -8.0\n",
      "Episode 328: Total Reward = 0.0\n",
      "Episode 329: Total Reward = -4.0\n",
      "Episode 330: Total Reward = 1.0\n",
      "Episode 331: Total Reward = 0.0\n",
      "Episode 332: Total Reward = -12.0\n",
      "Episode 333: Total Reward = 17.0\n",
      "Episode 334: Total Reward = -12.0\n",
      "Episode 335: Total Reward = 2.0\n",
      "Episode 336: Total Reward = -3.0\n",
      "Episode 337: Total Reward = 1.0\n",
      "Episode 338: Total Reward = -4.0\n",
      "Episode 339: Total Reward = -13.0\n",
      "Episode 340: Total Reward = -15.0\n",
      "Episode 341: Total Reward = 14.0\n",
      "Episode 342: Total Reward = 1.0\n",
      "Episode 343: Total Reward = 2.0\n",
      "Episode 344: Total Reward = 3.0\n",
      "Episode 345: Total Reward = 4.0\n",
      "Episode 346: Total Reward = -3.0\n",
      "Episode 347: Total Reward = 0.0\n",
      "Episode 348: Total Reward = 0.0\n",
      "Episode 349: Total Reward = 4.0\n",
      "Episode 350: Total Reward = 2.0\n",
      "Episode 351: Total Reward = 0.0\n",
      "Episode 352: Total Reward = -16.0\n",
      "Episode 353: Total Reward = 5.0\n",
      "Episode 354: Total Reward = -5.0\n",
      "Episode 355: Total Reward = -1.0\n",
      "Episode 356: Total Reward = -2.0\n",
      "Episode 357: Total Reward = -1.0\n",
      "Episode 358: Total Reward = 0.0\n",
      "Episode 359: Total Reward = -10.0\n",
      "Episode 360: Total Reward = -2.0\n",
      "Episode 361: Total Reward = 1.0\n",
      "Episode 362: Total Reward = -7.0\n",
      "Episode 363: Total Reward = -5.0\n",
      "Episode 364: Total Reward = 0.0\n",
      "Episode 365: Total Reward = -1.0\n",
      "Episode 366: Total Reward = -1.0\n",
      "Episode 367: Total Reward = -4.0\n",
      "Episode 368: Total Reward = 4.0\n",
      "Episode 369: Total Reward = 9.0\n",
      "Episode 370: Total Reward = -3.0\n",
      "Episode 371: Total Reward = 4.0\n",
      "Episode 372: Total Reward = -4.0\n",
      "Episode 373: Total Reward = 4.0\n",
      "Episode 374: Total Reward = -6.0\n",
      "Episode 375: Total Reward = -5.0\n",
      "Episode 376: Total Reward = 2.0\n",
      "Episode 377: Total Reward = 2.0\n",
      "Episode 378: Total Reward = 3.0\n",
      "Episode 379: Total Reward = -1.0\n",
      "Episode 380: Total Reward = -9.0\n",
      "Episode 381: Total Reward = 4.0\n",
      "Episode 382: Total Reward = -4.0\n",
      "Episode 383: Total Reward = -7.0\n",
      "Episode 384: Total Reward = 7.0\n",
      "Episode 385: Total Reward = -7.0\n",
      "Episode 386: Total Reward = 5.0\n",
      "Episode 387: Total Reward = -20.0\n",
      "Episode 388: Total Reward = 2.0\n",
      "Episode 389: Total Reward = -12.0\n",
      "Episode 390: Total Reward = 3.0\n",
      "Episode 391: Total Reward = -11.0\n",
      "Episode 392: Total Reward = 0.0\n",
      "Episode 393: Total Reward = 2.0\n",
      "Episode 394: Total Reward = 4.0\n",
      "Episode 395: Total Reward = -3.0\n",
      "Episode 396: Total Reward = 4.0\n",
      "Episode 397: Total Reward = -2.0\n",
      "Episode 398: Total Reward = 0.0\n",
      "Episode 399: Total Reward = 2.0\n",
      "Episode 400: Total Reward = -1.0\n",
      "Episode 401: Total Reward = -4.0\n",
      "Episode 402: Total Reward = -1.0\n",
      "Episode 403: Total Reward = -1.0\n",
      "Episode 404: Total Reward = -4.0\n",
      "Episode 405: Total Reward = 0.0\n",
      "Episode 406: Total Reward = -3.0\n",
      "Episode 407: Total Reward = -2.0\n",
      "Episode 408: Total Reward = 6.0\n",
      "Episode 409: Total Reward = -3.0\n",
      "Episode 410: Total Reward = 3.0\n",
      "Episode 411: Total Reward = 1.0\n",
      "Episode 412: Total Reward = 10.0\n",
      "Episode 413: Total Reward = 1.0\n",
      "Episode 414: Total Reward = 0.0\n",
      "Episode 415: Total Reward = 3.0\n",
      "Episode 416: Total Reward = -10.0\n",
      "Episode 417: Total Reward = 4.0\n",
      "Episode 418: Total Reward = -7.0\n",
      "Episode 419: Total Reward = -7.0\n",
      "Episode 420: Total Reward = 1.0\n",
      "Episode 421: Total Reward = 4.0\n",
      "Episode 422: Total Reward = 4.0\n",
      "Episode 423: Total Reward = -1.0\n",
      "Episode 424: Total Reward = -6.0\n",
      "Episode 425: Total Reward = -3.0\n",
      "Episode 426: Total Reward = 0.0\n",
      "Episode 427: Total Reward = 4.0\n",
      "Episode 428: Total Reward = -1.0\n",
      "Episode 429: Total Reward = -15.0\n",
      "Episode 430: Total Reward = -8.0\n",
      "Episode 431: Total Reward = 1.0\n",
      "Episode 432: Total Reward = -6.0\n",
      "Episode 433: Total Reward = -4.0\n",
      "Episode 434: Total Reward = -1.0\n",
      "Episode 435: Total Reward = 1.0\n",
      "Episode 436: Total Reward = 2.0\n",
      "Episode 437: Total Reward = -1.0\n",
      "Episode 438: Total Reward = 3.0\n",
      "Episode 439: Total Reward = -9.0\n",
      "Episode 440: Total Reward = 2.0\n",
      "Episode 441: Total Reward = -11.0\n",
      "Episode 442: Total Reward = -8.0\n",
      "Episode 443: Total Reward = 0.0\n",
      "Episode 444: Total Reward = -6.0\n",
      "Episode 445: Total Reward = 4.0\n",
      "Episode 446: Total Reward = 0.0\n",
      "Episode 447: Total Reward = 4.0\n",
      "Episode 448: Total Reward = -3.0\n",
      "Episode 449: Total Reward = -10.0\n",
      "Episode 450: Total Reward = -1.0\n",
      "Episode 451: Total Reward = -4.0\n",
      "Episode 452: Total Reward = -9.0\n",
      "Episode 453: Total Reward = 13.0\n",
      "Episode 454: Total Reward = 1.0\n",
      "Episode 455: Total Reward = -5.0\n",
      "Episode 456: Total Reward = 6.0\n",
      "Episode 457: Total Reward = 0.0\n",
      "Episode 458: Total Reward = -3.0\n",
      "Episode 459: Total Reward = -12.0\n",
      "Episode 460: Total Reward = -1.0\n",
      "Episode 461: Total Reward = 7.0\n",
      "Episode 462: Total Reward = 5.0\n",
      "Episode 463: Total Reward = 1.0\n",
      "Episode 464: Total Reward = -6.0\n",
      "Episode 465: Total Reward = -2.0\n",
      "Episode 466: Total Reward = 1.0\n",
      "Episode 467: Total Reward = -1.0\n",
      "Episode 468: Total Reward = 0.0\n",
      "Episode 469: Total Reward = 2.0\n",
      "Episode 470: Total Reward = 7.0\n",
      "Episode 471: Total Reward = -8.0\n",
      "Episode 472: Total Reward = 6.0\n",
      "Episode 473: Total Reward = 1.0\n",
      "Episode 474: Total Reward = 7.0\n",
      "Episode 475: Total Reward = -8.0\n",
      "Episode 476: Total Reward = -7.0\n",
      "Episode 477: Total Reward = 1.0\n",
      "Episode 478: Total Reward = -7.0\n",
      "Episode 479: Total Reward = 2.0\n",
      "Episode 480: Total Reward = -5.0\n",
      "Episode 481: Total Reward = -9.0\n",
      "Episode 482: Total Reward = 0.0\n",
      "Episode 483: Total Reward = -6.0\n",
      "Episode 484: Total Reward = 9.0\n",
      "Episode 485: Total Reward = -10.0\n",
      "Episode 486: Total Reward = -12.0\n",
      "Episode 487: Total Reward = 3.0\n",
      "Episode 488: Total Reward = -1.0\n",
      "Episode 489: Total Reward = -5.0\n",
      "Episode 490: Total Reward = -15.0\n",
      "Episode 491: Total Reward = 1.0\n",
      "Episode 492: Total Reward = 5.0\n",
      "Episode 493: Total Reward = -16.0\n",
      "Episode 494: Total Reward = -2.0\n",
      "Episode 495: Total Reward = -9.0\n",
      "Episode 496: Total Reward = -1.0\n",
      "Episode 497: Total Reward = -16.0\n",
      "Episode 498: Total Reward = 0.0\n",
      "Episode 499: Total Reward = -4.0\n",
      "Episode 500: Total Reward = 3.0\n",
      "Episode 501: Total Reward = 0.0\n",
      "Episode 502: Total Reward = 0.0\n",
      "Episode 503: Total Reward = -1.0\n",
      "Episode 504: Total Reward = -4.0\n",
      "Episode 505: Total Reward = -1.0\n",
      "Episode 506: Total Reward = 6.0\n",
      "Episode 507: Total Reward = 2.0\n",
      "Episode 508: Total Reward = 7.0\n",
      "Episode 509: Total Reward = -6.0\n",
      "Episode 510: Total Reward = 3.0\n",
      "Episode 511: Total Reward = -6.0\n",
      "Episode 512: Total Reward = 0.0\n",
      "Episode 513: Total Reward = -5.0\n",
      "Episode 514: Total Reward = 1.0\n",
      "Episode 515: Total Reward = -4.0\n",
      "Episode 516: Total Reward = -3.0\n",
      "Episode 517: Total Reward = 0.0\n",
      "Episode 518: Total Reward = 3.0\n",
      "Episode 519: Total Reward = 1.0\n",
      "Episode 520: Total Reward = 0.0\n",
      "Episode 521: Total Reward = -3.0\n",
      "Episode 522: Total Reward = -2.0\n",
      "Episode 523: Total Reward = -7.0\n",
      "Episode 524: Total Reward = -5.0\n",
      "Episode 525: Total Reward = 7.0\n",
      "Episode 526: Total Reward = -5.0\n",
      "Episode 527: Total Reward = -12.0\n",
      "Episode 528: Total Reward = 1.0\n",
      "Episode 529: Total Reward = 2.0\n",
      "Episode 530: Total Reward = -6.0\n",
      "Episode 531: Total Reward = -3.0\n",
      "Episode 532: Total Reward = 1.0\n",
      "Episode 533: Total Reward = 4.0\n",
      "Episode 534: Total Reward = 10.0\n",
      "Episode 535: Total Reward = -9.0\n",
      "Episode 536: Total Reward = 4.0\n",
      "Episode 537: Total Reward = -7.0\n",
      "Episode 538: Total Reward = -5.0\n",
      "Episode 539: Total Reward = 9.0\n",
      "Episode 540: Total Reward = 4.0\n",
      "Episode 541: Total Reward = 5.0\n",
      "Episode 542: Total Reward = 1.0\n",
      "Episode 543: Total Reward = 7.0\n",
      "Episode 544: Total Reward = -8.0\n",
      "Episode 545: Total Reward = -11.0\n",
      "Episode 546: Total Reward = 0.0\n",
      "Episode 547: Total Reward = -1.0\n",
      "Episode 548: Total Reward = 0.0\n",
      "Episode 549: Total Reward = -5.0\n",
      "Episode 550: Total Reward = 2.0\n",
      "Episode 551: Total Reward = -1.0\n",
      "Episode 552: Total Reward = -22.0\n",
      "Episode 553: Total Reward = 2.0\n",
      "Episode 554: Total Reward = 3.0\n",
      "Episode 555: Total Reward = -1.0\n",
      "Episode 556: Total Reward = 3.0\n",
      "Episode 557: Total Reward = -3.0\n",
      "Episode 558: Total Reward = -19.0\n",
      "Episode 559: Total Reward = -6.0\n",
      "Episode 560: Total Reward = -2.0\n",
      "Episode 561: Total Reward = 0.0\n",
      "Episode 562: Total Reward = 11.0\n",
      "Episode 563: Total Reward = 4.0\n",
      "Episode 564: Total Reward = 0.0\n",
      "Episode 565: Total Reward = 6.0\n",
      "Episode 566: Total Reward = -1.0\n",
      "Episode 567: Total Reward = 5.0\n",
      "Episode 568: Total Reward = -5.0\n",
      "Episode 569: Total Reward = -9.0\n",
      "Episode 570: Total Reward = -3.0\n",
      "Episode 571: Total Reward = -2.0\n",
      "Episode 572: Total Reward = -15.0\n",
      "Episode 573: Total Reward = 4.0\n",
      "Episode 574: Total Reward = 0.0\n",
      "Episode 575: Total Reward = 0.0\n",
      "Episode 576: Total Reward = 2.0\n",
      "Episode 577: Total Reward = -4.0\n",
      "Episode 578: Total Reward = -10.0\n",
      "Episode 579: Total Reward = 7.0\n",
      "Episode 580: Total Reward = 0.0\n",
      "Episode 581: Total Reward = -11.0\n",
      "Episode 582: Total Reward = -10.0\n",
      "Episode 583: Total Reward = 3.0\n",
      "Episode 584: Total Reward = 2.0\n",
      "Episode 585: Total Reward = 0.0\n",
      "Episode 586: Total Reward = 7.0\n",
      "Episode 587: Total Reward = 6.0\n",
      "Episode 588: Total Reward = 3.0\n",
      "Episode 589: Total Reward = -10.0\n",
      "Episode 590: Total Reward = -20.0\n",
      "Episode 591: Total Reward = 4.0\n",
      "Episode 592: Total Reward = 1.0\n",
      "Episode 593: Total Reward = -5.0\n",
      "Episode 594: Total Reward = -6.0\n",
      "Episode 595: Total Reward = 0.0\n",
      "Episode 596: Total Reward = -5.0\n",
      "Episode 597: Total Reward = -3.0\n",
      "Episode 598: Total Reward = 0.0\n",
      "Episode 599: Total Reward = 3.0\n",
      "Episode 600: Total Reward = 4.0\n",
      "Episode 601: Total Reward = 3.0\n",
      "Episode 602: Total Reward = 1.0\n",
      "Episode 603: Total Reward = -7.0\n",
      "Episode 604: Total Reward = 0.0\n",
      "Episode 605: Total Reward = 4.0\n",
      "Episode 606: Total Reward = 14.0\n",
      "Episode 607: Total Reward = 4.0\n",
      "Episode 608: Total Reward = 3.0\n",
      "Episode 609: Total Reward = 0.0\n",
      "Episode 610: Total Reward = -2.0\n",
      "Episode 611: Total Reward = -4.0\n",
      "Episode 612: Total Reward = 5.0\n",
      "Episode 613: Total Reward = -6.0\n",
      "Episode 614: Total Reward = -4.0\n",
      "Episode 615: Total Reward = 3.0\n",
      "Episode 616: Total Reward = 3.0\n",
      "Episode 617: Total Reward = 5.0\n",
      "Episode 618: Total Reward = -14.0\n",
      "Episode 619: Total Reward = -3.0\n",
      "Episode 620: Total Reward = -1.0\n",
      "Episode 621: Total Reward = 0.0\n",
      "Episode 622: Total Reward = 3.0\n",
      "Episode 623: Total Reward = 4.0\n",
      "Episode 624: Total Reward = 0.0\n",
      "Episode 625: Total Reward = 7.0\n",
      "Episode 626: Total Reward = 9.0\n",
      "Episode 627: Total Reward = -17.0\n",
      "Episode 628: Total Reward = 1.0\n",
      "Episode 629: Total Reward = 4.0\n",
      "Episode 630: Total Reward = -2.0\n",
      "Episode 631: Total Reward = 0.0\n",
      "Episode 632: Total Reward = -14.0\n",
      "Episode 633: Total Reward = -1.0\n",
      "Episode 634: Total Reward = -1.0\n",
      "Episode 635: Total Reward = 0.0\n",
      "Episode 636: Total Reward = 1.0\n",
      "Episode 637: Total Reward = -7.0\n",
      "Episode 638: Total Reward = 1.0\n",
      "Episode 639: Total Reward = -1.0\n",
      "Episode 640: Total Reward = -6.0\n",
      "Episode 641: Total Reward = 2.0\n",
      "Episode 642: Total Reward = 1.0\n",
      "Episode 643: Total Reward = 5.0\n",
      "Episode 644: Total Reward = -10.0\n",
      "Episode 645: Total Reward = -8.0\n",
      "Episode 646: Total Reward = -9.0\n",
      "Episode 647: Total Reward = -4.0\n",
      "Episode 648: Total Reward = -3.0\n",
      "Episode 649: Total Reward = 4.0\n",
      "Episode 650: Total Reward = 2.0\n",
      "Episode 651: Total Reward = 4.0\n",
      "Episode 652: Total Reward = -8.0\n",
      "Episode 653: Total Reward = 1.0\n",
      "Episode 654: Total Reward = -14.0\n",
      "Episode 655: Total Reward = -2.0\n",
      "Episode 656: Total Reward = -3.0\n",
      "Episode 657: Total Reward = 0.0\n",
      "Episode 658: Total Reward = 0.0\n",
      "Episode 659: Total Reward = -8.0\n",
      "Episode 660: Total Reward = -8.0\n",
      "Episode 661: Total Reward = -7.0\n",
      "Episode 662: Total Reward = -3.0\n",
      "Episode 663: Total Reward = -2.0\n",
      "Episode 664: Total Reward = -2.0\n",
      "Episode 665: Total Reward = -1.0\n",
      "Episode 666: Total Reward = 3.0\n",
      "Episode 667: Total Reward = 4.0\n",
      "Episode 668: Total Reward = -1.0\n",
      "Episode 669: Total Reward = -8.0\n",
      "Episode 670: Total Reward = 7.0\n",
      "Episode 671: Total Reward = 0.0\n",
      "Episode 672: Total Reward = -1.0\n",
      "Episode 673: Total Reward = 2.0\n",
      "Episode 674: Total Reward = -14.0\n",
      "Episode 675: Total Reward = -2.0\n",
      "Episode 676: Total Reward = 0.0\n",
      "Episode 677: Total Reward = 2.0\n",
      "Episode 678: Total Reward = 0.0\n",
      "Episode 679: Total Reward = -5.0\n",
      "Episode 680: Total Reward = -3.0\n",
      "Episode 681: Total Reward = -6.0\n",
      "Episode 682: Total Reward = 5.0\n",
      "Episode 683: Total Reward = -1.0\n",
      "Episode 684: Total Reward = 0.0\n",
      "Episode 685: Total Reward = 2.0\n",
      "Episode 686: Total Reward = -7.0\n",
      "Episode 687: Total Reward = -4.0\n",
      "Episode 688: Total Reward = 2.0\n",
      "Episode 689: Total Reward = 3.0\n",
      "Episode 690: Total Reward = 0.0\n",
      "Episode 691: Total Reward = -2.0\n",
      "Episode 692: Total Reward = -2.0\n",
      "Episode 693: Total Reward = 1.0\n",
      "Episode 694: Total Reward = -3.0\n",
      "Episode 695: Total Reward = -9.0\n",
      "Episode 696: Total Reward = 3.0\n",
      "Episode 697: Total Reward = 1.0\n",
      "Episode 698: Total Reward = 1.0\n",
      "Episode 699: Total Reward = 2.0\n",
      "Episode 700: Total Reward = 0.0\n",
      "Episode 701: Total Reward = -7.0\n",
      "Episode 702: Total Reward = -4.0\n",
      "Episode 703: Total Reward = 1.0\n",
      "Episode 704: Total Reward = -5.0\n",
      "Episode 705: Total Reward = 1.0\n",
      "Episode 706: Total Reward = -3.0\n",
      "Episode 707: Total Reward = 4.0\n",
      "Episode 708: Total Reward = -13.0\n",
      "Episode 709: Total Reward = -4.0\n",
      "Episode 710: Total Reward = -13.0\n",
      "Episode 711: Total Reward = -2.0\n",
      "Episode 712: Total Reward = -7.0\n",
      "Episode 713: Total Reward = 3.0\n",
      "Episode 714: Total Reward = 2.0\n",
      "Episode 715: Total Reward = 3.0\n",
      "Episode 716: Total Reward = 2.0\n",
      "Episode 717: Total Reward = 3.0\n",
      "Episode 718: Total Reward = -7.0\n",
      "Episode 719: Total Reward = -3.0\n",
      "Episode 720: Total Reward = 0.0\n",
      "Episode 721: Total Reward = 6.0\n",
      "Episode 722: Total Reward = -3.0\n",
      "Episode 723: Total Reward = -1.0\n",
      "Episode 724: Total Reward = -1.0\n",
      "Episode 725: Total Reward = 0.0\n",
      "Episode 726: Total Reward = -1.0\n",
      "Episode 727: Total Reward = -9.0\n",
      "Episode 728: Total Reward = -14.0\n",
      "Episode 729: Total Reward = -7.0\n",
      "Episode 730: Total Reward = 0.0\n",
      "Episode 731: Total Reward = -3.0\n",
      "Episode 732: Total Reward = 4.0\n",
      "Episode 733: Total Reward = 3.0\n",
      "Episode 734: Total Reward = 1.0\n",
      "Episode 735: Total Reward = 0.0\n",
      "Episode 736: Total Reward = 4.0\n",
      "Episode 737: Total Reward = -6.0\n",
      "Episode 738: Total Reward = -8.0\n",
      "Episode 739: Total Reward = 1.0\n",
      "Episode 740: Total Reward = 1.0\n",
      "Episode 741: Total Reward = 3.0\n",
      "Episode 742: Total Reward = 6.0\n",
      "Episode 743: Total Reward = -6.0\n",
      "Episode 744: Total Reward = 0.0\n",
      "Episode 745: Total Reward = 5.0\n",
      "Episode 746: Total Reward = -7.0\n",
      "Episode 747: Total Reward = 1.0\n",
      "Episode 748: Total Reward = -9.0\n",
      "Episode 749: Total Reward = -10.0\n",
      "Episode 750: Total Reward = -1.0\n",
      "Episode 751: Total Reward = -5.0\n",
      "Episode 752: Total Reward = -10.0\n",
      "Episode 753: Total Reward = 4.0\n",
      "Episode 754: Total Reward = -2.0\n",
      "Episode 755: Total Reward = 6.0\n",
      "Episode 756: Total Reward = 13.0\n",
      "Episode 757: Total Reward = -2.0\n",
      "Episode 758: Total Reward = 13.0\n",
      "Episode 759: Total Reward = -3.0\n",
      "Episode 760: Total Reward = -3.0\n",
      "Episode 761: Total Reward = 15.0\n",
      "Episode 762: Total Reward = -11.0\n",
      "Episode 763: Total Reward = -8.0\n",
      "Episode 764: Total Reward = -4.0\n",
      "Episode 765: Total Reward = 6.0\n",
      "Episode 766: Total Reward = 4.0\n",
      "Episode 767: Total Reward = -20.0\n",
      "Episode 768: Total Reward = 1.0\n",
      "Episode 769: Total Reward = -5.0\n",
      "Episode 770: Total Reward = -5.0\n",
      "Episode 771: Total Reward = 4.0\n",
      "Episode 772: Total Reward = 0.0\n",
      "Episode 773: Total Reward = 4.0\n",
      "Episode 774: Total Reward = -6.0\n",
      "Episode 775: Total Reward = -8.0\n",
      "Episode 776: Total Reward = 9.0\n",
      "Episode 777: Total Reward = 9.0\n",
      "Episode 778: Total Reward = -1.0\n",
      "Episode 779: Total Reward = -6.0\n",
      "Episode 780: Total Reward = 8.0\n",
      "Episode 781: Total Reward = -5.0\n",
      "Episode 782: Total Reward = 2.0\n",
      "Episode 783: Total Reward = 6.0\n",
      "Episode 784: Total Reward = -5.0\n",
      "Episode 785: Total Reward = 3.0\n",
      "Episode 786: Total Reward = 1.0\n",
      "Episode 787: Total Reward = -17.0\n",
      "Episode 788: Total Reward = 1.0\n",
      "Episode 789: Total Reward = -4.0\n",
      "Episode 790: Total Reward = 4.0\n",
      "Episode 791: Total Reward = 1.0\n",
      "Episode 792: Total Reward = -1.0\n",
      "Episode 793: Total Reward = -1.0\n",
      "Episode 794: Total Reward = -3.0\n",
      "Episode 795: Total Reward = -22.0\n",
      "Episode 796: Total Reward = -6.0\n",
      "Episode 797: Total Reward = -4.0\n",
      "Episode 798: Total Reward = -6.0\n",
      "Episode 799: Total Reward = -21.0\n",
      "Episode 800: Total Reward = -5.0\n",
      "Episode 801: Total Reward = 7.0\n",
      "Episode 802: Total Reward = 0.0\n",
      "Episode 803: Total Reward = -13.0\n",
      "Episode 804: Total Reward = 3.0\n",
      "Episode 805: Total Reward = -7.0\n",
      "Episode 806: Total Reward = 2.0\n",
      "Episode 807: Total Reward = -3.0\n",
      "Episode 808: Total Reward = -7.0\n",
      "Episode 809: Total Reward = -2.0\n",
      "Episode 810: Total Reward = 0.0\n",
      "Episode 811: Total Reward = -5.0\n",
      "Episode 812: Total Reward = -1.0\n",
      "Episode 813: Total Reward = 2.0\n",
      "Episode 814: Total Reward = 6.0\n",
      "Episode 815: Total Reward = 4.0\n",
      "Episode 816: Total Reward = -7.0\n",
      "Episode 817: Total Reward = -4.0\n",
      "Episode 818: Total Reward = -4.0\n",
      "Episode 819: Total Reward = -7.0\n",
      "Episode 820: Total Reward = 3.0\n",
      "Episode 821: Total Reward = 5.0\n",
      "Episode 822: Total Reward = -8.0\n",
      "Episode 823: Total Reward = -1.0\n",
      "Episode 824: Total Reward = 2.0\n",
      "Episode 825: Total Reward = -2.0\n",
      "Episode 826: Total Reward = -13.0\n",
      "Episode 827: Total Reward = -7.0\n",
      "Episode 828: Total Reward = -7.0\n",
      "Episode 829: Total Reward = -7.0\n",
      "Episode 830: Total Reward = 7.0\n",
      "Episode 831: Total Reward = 6.0\n",
      "Episode 832: Total Reward = 2.0\n",
      "Episode 833: Total Reward = -10.0\n",
      "Episode 834: Total Reward = 6.0\n",
      "Episode 835: Total Reward = 0.0\n",
      "Episode 836: Total Reward = 1.0\n",
      "Episode 837: Total Reward = -8.0\n",
      "Episode 838: Total Reward = 0.0\n",
      "Episode 839: Total Reward = 5.0\n",
      "Episode 840: Total Reward = -3.0\n",
      "Episode 841: Total Reward = 4.0\n",
      "Episode 842: Total Reward = 0.0\n",
      "Episode 843: Total Reward = 0.0\n",
      "Episode 844: Total Reward = -7.0\n",
      "Episode 845: Total Reward = 0.0\n",
      "Episode 846: Total Reward = 1.0\n",
      "Episode 847: Total Reward = 4.0\n",
      "Episode 848: Total Reward = 1.0\n",
      "Episode 849: Total Reward = 6.0\n",
      "Episode 850: Total Reward = 1.0\n",
      "Episode 851: Total Reward = 1.0\n",
      "Episode 852: Total Reward = 10.0\n",
      "Episode 853: Total Reward = -4.0\n",
      "Episode 854: Total Reward = -13.0\n",
      "Episode 855: Total Reward = 0.0\n",
      "Episode 856: Total Reward = -11.0\n",
      "Episode 857: Total Reward = 0.0\n",
      "Episode 858: Total Reward = 2.0\n",
      "Episode 859: Total Reward = -3.0\n",
      "Episode 860: Total Reward = 4.0\n",
      "Episode 861: Total Reward = -1.0\n",
      "Episode 862: Total Reward = -6.0\n",
      "Episode 863: Total Reward = -2.0\n",
      "Episode 864: Total Reward = 2.0\n",
      "Episode 865: Total Reward = 2.0\n",
      "Episode 866: Total Reward = -6.0\n",
      "Episode 867: Total Reward = 7.0\n",
      "Episode 868: Total Reward = 5.0\n",
      "Episode 869: Total Reward = -2.0\n",
      "Episode 870: Total Reward = -1.0\n",
      "Episode 871: Total Reward = 5.0\n",
      "Episode 872: Total Reward = -4.0\n",
      "Episode 873: Total Reward = -9.0\n",
      "Episode 874: Total Reward = 0.0\n",
      "Episode 875: Total Reward = 9.0\n",
      "Episode 876: Total Reward = 5.0\n",
      "Episode 877: Total Reward = 0.0\n",
      "Episode 878: Total Reward = -8.0\n",
      "Episode 879: Total Reward = -9.0\n",
      "Episode 880: Total Reward = 1.0\n",
      "Episode 881: Total Reward = 9.0\n",
      "Episode 882: Total Reward = -10.0\n",
      "Episode 883: Total Reward = -6.0\n",
      "Episode 884: Total Reward = -2.0\n",
      "Episode 885: Total Reward = -1.0\n",
      "Episode 886: Total Reward = 6.0\n",
      "Episode 887: Total Reward = 1.0\n",
      "Episode 888: Total Reward = -13.0\n",
      "Episode 889: Total Reward = -5.0\n",
      "Episode 890: Total Reward = 1.0\n",
      "Episode 891: Total Reward = 3.0\n",
      "Episode 892: Total Reward = -9.0\n",
      "Episode 893: Total Reward = -3.0\n",
      "Episode 894: Total Reward = 3.0\n",
      "Episode 895: Total Reward = 7.0\n",
      "Episode 896: Total Reward = -4.0\n",
      "Episode 897: Total Reward = 10.0\n",
      "Episode 898: Total Reward = 4.0\n",
      "Episode 899: Total Reward = 0.0\n",
      "Episode 900: Total Reward = 17.0\n",
      "Episode 901: Total Reward = 2.0\n",
      "Episode 902: Total Reward = -2.0\n",
      "Episode 903: Total Reward = -4.0\n",
      "Episode 904: Total Reward = -3.0\n",
      "Episode 905: Total Reward = 1.0\n",
      "Episode 906: Total Reward = 1.0\n",
      "Episode 907: Total Reward = 1.0\n",
      "Episode 908: Total Reward = 5.0\n",
      "Episode 909: Total Reward = -6.0\n",
      "Episode 910: Total Reward = 3.0\n",
      "Episode 911: Total Reward = 0.0\n",
      "Episode 912: Total Reward = -13.0\n",
      "Episode 913: Total Reward = -2.0\n",
      "Episode 914: Total Reward = -2.0\n",
      "Episode 915: Total Reward = -10.0\n",
      "Episode 916: Total Reward = 4.0\n",
      "Episode 917: Total Reward = -18.0\n",
      "Episode 918: Total Reward = 5.0\n",
      "Episode 919: Total Reward = 0.0\n",
      "Episode 920: Total Reward = -1.0\n",
      "Episode 921: Total Reward = -1.0\n",
      "Episode 922: Total Reward = -1.0\n",
      "Episode 923: Total Reward = 4.0\n",
      "Episode 924: Total Reward = -4.0\n",
      "Episode 925: Total Reward = -22.0\n",
      "Episode 926: Total Reward = -21.0\n",
      "Episode 927: Total Reward = -3.0\n",
      "Episode 928: Total Reward = 5.0\n",
      "Episode 929: Total Reward = -4.0\n",
      "Episode 930: Total Reward = -8.0\n",
      "Episode 931: Total Reward = 7.0\n",
      "Episode 932: Total Reward = -1.0\n",
      "Episode 933: Total Reward = 4.0\n",
      "Episode 934: Total Reward = 0.0\n",
      "Episode 935: Total Reward = -9.0\n",
      "Episode 936: Total Reward = -2.0\n",
      "Episode 937: Total Reward = -18.0\n",
      "Episode 938: Total Reward = -1.0\n",
      "Episode 939: Total Reward = -1.0\n",
      "Episode 940: Total Reward = 0.0\n",
      "Episode 941: Total Reward = -3.0\n",
      "Episode 942: Total Reward = -2.0\n",
      "Episode 943: Total Reward = 2.0\n",
      "Episode 944: Total Reward = -2.0\n",
      "Episode 945: Total Reward = -3.0\n",
      "Episode 946: Total Reward = 0.0\n",
      "Episode 947: Total Reward = -12.0\n",
      "Episode 948: Total Reward = -3.0\n",
      "Episode 949: Total Reward = -7.0\n",
      "Episode 950: Total Reward = 0.0\n",
      "Episode 951: Total Reward = 2.0\n",
      "Episode 952: Total Reward = 1.0\n",
      "Episode 953: Total Reward = -2.0\n",
      "Episode 954: Total Reward = -3.0\n",
      "Episode 955: Total Reward = 4.0\n",
      "Episode 956: Total Reward = -4.0\n",
      "Episode 957: Total Reward = 1.0\n",
      "Episode 958: Total Reward = 11.0\n",
      "Episode 959: Total Reward = -2.0\n",
      "Episode 960: Total Reward = 2.0\n",
      "Episode 961: Total Reward = 5.0\n",
      "Episode 962: Total Reward = -16.0\n",
      "Episode 963: Total Reward = 2.0\n",
      "Episode 964: Total Reward = -6.0\n",
      "Episode 965: Total Reward = -8.0\n",
      "Episode 966: Total Reward = 1.0\n",
      "Episode 967: Total Reward = 1.0\n",
      "Episode 968: Total Reward = 1.0\n",
      "Episode 969: Total Reward = -9.0\n",
      "Episode 970: Total Reward = -4.0\n",
      "Episode 971: Total Reward = -1.0\n",
      "Episode 972: Total Reward = 2.0\n",
      "Episode 973: Total Reward = -2.0\n",
      "Episode 974: Total Reward = -8.0\n",
      "Episode 975: Total Reward = -3.0\n",
      "Episode 976: Total Reward = -2.0\n",
      "Episode 977: Total Reward = 4.0\n",
      "Episode 978: Total Reward = 8.0\n",
      "Episode 979: Total Reward = 4.0\n",
      "Episode 980: Total Reward = -1.0\n",
      "Episode 981: Total Reward = 3.0\n",
      "Episode 982: Total Reward = -1.0\n",
      "Episode 983: Total Reward = -1.0\n",
      "Episode 984: Total Reward = -4.0\n",
      "Episode 985: Total Reward = 2.0\n",
      "Episode 986: Total Reward = 2.0\n",
      "Episode 987: Total Reward = -5.0\n",
      "Episode 988: Total Reward = -7.0\n",
      "Episode 989: Total Reward = -4.0\n",
      "Episode 990: Total Reward = -2.0\n",
      "Episode 991: Total Reward = -3.0\n",
      "Episode 992: Total Reward = -13.0\n",
      "Episode 993: Total Reward = -3.0\n",
      "Episode 994: Total Reward = 2.0\n",
      "Episode 995: Total Reward = -5.0\n",
      "Episode 996: Total Reward = -1.0\n",
      "Episode 997: Total Reward = -6.0\n",
      "Episode 998: Total Reward = -7.0\n",
      "Episode 999: Total Reward = -2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/tmp/ipykernel_8720/2585660467.py:149: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000: Total Reward = 0.0\n",
      "Total Reward: -100.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Create and play video clip using the frames and given fps\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m clip \u001b[38;5;241m=\u001b[39m mpy\u001b[38;5;241m.\u001b[39mImageSequenceClip(frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     44\u001b[0m clip\u001b[38;5;241m.\u001b[39mipython_display(rd_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/moviepy/video/io/ImageSequenceClip.py:84\u001b[0m, in \u001b[0;36mImageSequenceClip.__init__\u001b[0;34m(self, sequence, fps, durations, with_mask, ismask, load_images)\u001b[0m\n\u001b[1;32m     82\u001b[0m    size \u001b[38;5;241m=\u001b[39m imread(sequence[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m    size \u001b[38;5;241m=\u001b[39m sequence[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[1;32m     87\u001b[0m     image1\u001b[38;5;241m=\u001b[39mimage\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\")\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "hidden_sizes = (256, 256)\n",
    "lr_policy = 1e-4\n",
    "lr_q = 1e-3\n",
    "replay_size = 100000\n",
    "batch_size = 64\n",
    "target_update_interval = 1\n",
    "tau = 0.005\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "updates_per_step = 1\n",
    "\n",
    "# Initialize the SAC agent\n",
    "agent = SACAgent(env, gamma, alpha, hidden_sizes, lr_policy, lr_q, replay_size, batch_size,\n",
    "                 target_update_interval, tau)\n",
    "\n",
    "# Train the agent\n",
    "agent.train(num_episodes, max_steps_per_episode, updates_per_step)\n",
    "\n",
    "# Visualize one episode\n",
    "state, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    frames.append(env.render())\n",
    "    action = agent.select_action(state, evaluate=True)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Create and play video clip using the frames and given fps\n",
    "clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "clip.ipython_display(rd_kwargs=dict(logger=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f6da4-ad5f-440f-ac53-b275639c0f39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Potential Improvements:\n",
    "Frame Stacking: Stack multiple consecutive frames to give the agent a sense of motion.\n",
    "Reward Clipping: Clip rewards to a certain range to stabilize training.\n",
    "Normalization: Normalize input images and rewards.\n",
    "Resize and grayscale images: To reduce training time and compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41e6c30b-08c0-4723-a294-97290163b180",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent.save(\"SAC_initial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027943e2-94de-49c2-b8f2-673e9dc55d9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Hyperparameter Sweep Soft-Actor-Critic (SAC)(RAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec58f3-8700-4a0c-ac54-afb39f7d19e4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd0081-ff11-46b1-90d7-0af938f21e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable anomaly detection for debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Define the Policy Network for Discrete Actions\n",
    "class PolicyNetworkRAM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        # Define network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        # Output layer for logits\n",
    "        layers.append(nn.Linear(last_size, output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output logits for Categorical distribution\n",
    "        return self.net(x)\n",
    "    \n",
    "    def update_policy(self, states, actions, q_values):\n",
    "        \"\"\"\n",
    "        Update the policy network.\n",
    "        Args:\n",
    "            states (Tensor): Current states [batch_size, state_dims]\n",
    "            actions (Tensor): Actions taken [batch_size, 1]\n",
    "            q_values (Tensor): Q-values corresponding to actions [batch_size]\n",
    "        Returns:\n",
    "            float: Policy loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.forward(states)  # [batch_size, num_actions]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions.squeeze(1))  # [batch_size]\n",
    "        entropy = dist.entropy()  # [batch_size]\n",
    "        # Policy loss with entropy regularization\n",
    "        policy_loss = (self.alpha * log_probs - q_values).mean()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return policy_loss.item()\n",
    "\n",
    "# Define the Q-Network\n",
    "class QNetworkRAM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_actions, learning_rate):\n",
    "        super().__init__()\n",
    "        # Define network layers\n",
    "        layers = []\n",
    "        last_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            last_size = hidden_size\n",
    "        # Output layer for Q-values\n",
    "        layers.append(nn.Linear(last_size, num_actions))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Output Q-values for each action\n",
    "        return self.net(x)  # [batch_size, num_actions]\n",
    "    \n",
    "    def update_q_network(self, parameters, loss):\n",
    "        \"\"\"\n",
    "        Update Q-network parameters.\n",
    "        Args:\n",
    "            parameters (iterable): Network parameters to update.\n",
    "            loss (Tensor): Computed loss.\n",
    "        Returns:\n",
    "            float: Q-network loss value\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(parameters, max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# Define the Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = zip(*samples)\n",
    "        \n",
    "        # Convert lists to single NumPy arrays for efficiency\n",
    "        state_batch = np.array(state_batch)\n",
    "        action_batch = np.array(action_batch)\n",
    "        reward_batch = np.array(reward_batch)\n",
    "        next_state_batch = np.array(next_state_batch)\n",
    "        terminated_batch = np.array(terminated_batch)\n",
    "        \n",
    "        # Convert to tensors and move to device\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)  # [batch_size, state_dims]\n",
    "        action_batch = torch.LongTensor(action_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)  # [batch_size, state_dims]\n",
    "        terminated_batch = torch.FloatTensor(terminated_batch).unsqueeze(1).to(device)  # [batch_size, 1]\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, terminated_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Define the SAC Agent for Discrete Actions\n",
    "class SACAgentRAM:\n",
    "    def __init__(self, env, config):\n",
    "        # Parse hyperparameters from config\n",
    "        self.gamma = config.gamma\n",
    "        self.alpha = config.alpha  # Entropy temperature\n",
    "        self.batch_size = config.batch_size\n",
    "        self.tau = config.tau  # Target smoothing coefficient\n",
    "        self.target_update_interval = config.target_update_interval\n",
    "        self.automatic_alpha = config.automatic_alpha\n",
    "        \n",
    "        # Validate observation space\n",
    "        continuous = isinstance(env.observation_space, gym.spaces.Box) and len(env.observation_space.shape) == 1\n",
    "        assert continuous, 'Observation space must be continuous with shape (n,)'\n",
    "        self.state_dims = env.observation_space.shape[0]\n",
    "        print(f\"State dimensions: {self.state_dims}\")\n",
    "    \n",
    "        # Validate action space\n",
    "        assert isinstance(env.action_space, gym.spaces.Discrete), 'Action space must be discrete'\n",
    "        self.num_actions = env.action_space.n\n",
    "        print(f\"Number of actions: {self.num_actions}\")\n",
    "        \n",
    "        # Parse hidden_sizes from string to tuple if necessary\n",
    "        if isinstance(config.hidden_sizes, str):\n",
    "            hidden_sizes = ast.literal_eval(config.hidden_sizes)\n",
    "        else:\n",
    "            hidden_sizes = config.hidden_sizes\n",
    "        \n",
    "        # Initialize Replay Buffer\n",
    "        self.replay_buffer = ReplayBuffer(config.replay_size)\n",
    "        \n",
    "        # Initialize Policy Network\n",
    "        self.policy_net = PolicyNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            output_size=self.num_actions,\n",
    "            learning_rate=config.lr_policy,\n",
    "            alpha=self.alpha\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize Q-Networks\n",
    "        self.q_net1 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        self.q_net2 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize Target Q-Networks\n",
    "        self.target_q_net1 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        self.target_q_net2 = QNetworkRAM(\n",
    "            input_size=self.state_dims,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_actions=self.num_actions,\n",
    "            learning_rate=config.lr_q\n",
    "        ).to(device)\n",
    "        \n",
    "        # Synchronize target networks with main Q-networks\n",
    "        self.target_q_net1.load_state_dict(self.q_net1.state_dict())\n",
    "        self.target_q_net2.load_state_dict(self.q_net2.state_dict())\n",
    "        \n",
    "        # Automatic entropy tuning\n",
    "        if self.automatic_alpha:\n",
    "            self.target_entropy = -math.log(1.0 / self.num_actions) * 0.98  # Slightly lower than maximum\n",
    "            self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "            self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.lr_policy)\n",
    "            print(\"Automatic alpha tuning enabled.\")\n",
    "        \n",
    "        # Store environment\n",
    "        self.env = env\n",
    "    \n",
    "    def policy(self, state, evaluate=False):\n",
    "        \"\"\"\n",
    "        Select action based on current policy.\n",
    "        Args:\n",
    "            state (array-like): Current state.\n",
    "            evaluate (bool): If True, select the best action deterministically.\n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        state = state.unsqueeze(0)  # [1, state_dims]\n",
    "        logits = self.policy_net(state)  # [1, num_actions]\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if evaluate:\n",
    "            action = dist.probs.argmax(dim=1).item()\n",
    "        else:\n",
    "            action = dist.sample().item()\n",
    "        # Validate action\n",
    "        if not (0 <= action < self.num_actions):\n",
    "            raise ValueError(f\"Sampled action {action} is out of bounds.\")\n",
    "        return action\n",
    "    \n",
    "    def update(self, updates):\n",
    "        \"\"\"\n",
    "        Update the SAC agent's networks.\n",
    "        Args:\n",
    "            updates (int): Current update step count.\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a minibatch from replay buffer\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            # Get logits and probabilities from the policy network for next states\n",
    "            next_logits = self.policy_net(next_state_batch)\n",
    "            next_probs = F.softmax(next_logits, dim=1)  # [batch_size, num_actions]\n",
    "            next_log_probs = F.log_softmax(next_logits, dim=1)  # [batch_size, num_actions]\n",
    "            \n",
    "            # Get target Q-values from target networks\n",
    "            target_q1_values = self.target_q_net1(next_state_batch)  # [batch_size, num_actions]\n",
    "            target_q2_values = self.target_q_net2(next_state_batch)  # [batch_size, num_actions]\n",
    "            target_q_values = torch.min(target_q1_values, target_q2_values)  # [batch_size, num_actions]\n",
    "            \n",
    "            # Compute expected Q-values for next states\n",
    "            expected_q = (next_probs * (target_q_values - self.alpha * next_log_probs)).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "            \n",
    "            # Compute target Q\n",
    "            q_target = reward_batch + (1 - terminated_batch) * self.gamma * expected_q  # [batch_size, 1]\n",
    "        \n",
    "        # Compute current Q-values from both Q-networks\n",
    "        current_q1 = self.q_net1(state_batch).gather(1, action_batch)  # [batch_size, 1]\n",
    "        current_q2 = self.q_net2(state_batch).gather(1, action_batch)  # [batch_size, 1]\n",
    "        \n",
    "        # Compute Q-network losses\n",
    "        q1_loss = self.q_net1.criterion(current_q1, q_target)\n",
    "        q2_loss = self.q_net2.criterion(current_q2, q_target)\n",
    "        \n",
    "        # Update Q-networks\n",
    "        q1_loss_value = self.q_net1.update_q_network(self.q_net1.parameters(), q1_loss)\n",
    "        q2_loss_value = self.q_net2.update_q_network(self.q_net2.parameters(), q2_loss)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        with torch.no_grad():\n",
    "            # Get Q-values from Q-networks\n",
    "            q1 = self.q_net1(state_batch)  # [batch_size, num_actions]\n",
    "            q2 = self.q_net2(state_batch)  # [batch_size, num_actions]\n",
    "            min_q = torch.min(q1, q2)  # [batch_size, num_actions]\n",
    "        \n",
    "        # Update policy network\n",
    "        policy_loss_value = self.policy_net.update_policy(state_batch, action_batch, min_q.gather(1, action_batch).squeeze(1))\n",
    "        \n",
    "        # Optional: Update alpha for entropy temperature\n",
    "        if self.automatic_alpha:\n",
    "            # Recompute log_probs for entropy adjustment\n",
    "            logits = self.policy_net(state_batch)  # [batch_size, num_actions]\n",
    "            log_probs = F.log_softmax(logits, dim=1)  # [batch_size, num_actions]\n",
    "            entropy = - (log_probs * F.softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "            \n",
    "            alpha_loss = -(self.log_alpha * (entropy - self.target_entropy)).mean()\n",
    "            \n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            \n",
    "            self.alpha = self.log_alpha.exp().detach()\n",
    "            print(f\"Updated alpha: {self.alpha.item():.4f}, Alpha Loss: {alpha_loss.item():.4f}\")\n",
    "        \n",
    "        # Update target networks\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            self.soft_update(self.q_net1, self.target_q_net1)\n",
    "            self.soft_update(self.q_net2, self.target_q_net2)\n",
    "            print(\"Target networks updated.\")\n",
    "        \n",
    "        return q1_loss_value, q2_loss_value, policy_loss_value  # Optionally return loss values for logging\n",
    "    \n",
    "    def soft_update(self, net, target_net):\n",
    "        \"\"\"\n",
    "        Perform soft update of target network parameters.\n",
    "        Args:\n",
    "            net (nn.Module): Main network.\n",
    "            target_net (nn.Module): Target network.\n",
    "        \"\"\"\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
    "    \n",
    "    def train_agent(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        \"\"\"\n",
    "        Train the SAC agent.\n",
    "        Args:\n",
    "            max_episodes (int): Maximum number of episodes for training.\n",
    "            stop_criterion (function): Function to determine stopping condition.\n",
    "            criterion_episodes (int): Number of episodes to evaluate the stopping criterion.\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "        updates = 0\n",
    "        \n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            state, _ = self.env.reset()\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            while not (terminated or truncated) and steps < 10000:  # Prevent infinite loops\n",
    "                # Select action by following behaviour policy\n",
    "                action = self.policy(state)\n",
    "\n",
    "                # Send the action to the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Add experience to replay buffer\n",
    "                self.replay_buffer.push(state, action, reward, next_state, terminated)\n",
    "\n",
    "                # Update state\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "\n",
    "                # Update Q-network and policy\n",
    "                self.update(updates)\n",
    "                updates += 1\n",
    "\n",
    "            # Append the reward for this episode\n",
    "            total_rewards.append(episode_reward)\n",
    "\n",
    "            # Log metrics to W&B\n",
    "            wandb.log({\n",
    "                'episode': episode,\n",
    "                'episode_reward': episode_reward,\n",
    "                'steps': steps,\n",
    "                'updates': updates,\n",
    "                'alpha': self.alpha.item() if self.automatic_alpha else self.alpha,\n",
    "                # Optionally, log losses if returned\n",
    "                # 'q1_loss': q1_loss_value,\n",
    "                # 'q2_loss': q2_loss_value,\n",
    "                # 'policy_loss': policy_loss_value,\n",
    "            })\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Steps = {steps}\")\n",
    "\n",
    "            # Check stopping criterion\n",
    "            if episode >= criterion_episodes:\n",
    "                recent_rewards = total_rewards[-criterion_episodes:]\n",
    "                if stop_criterion(recent_rewards):\n",
    "                    print(f\"\\nStopping criterion satisfied after {episode} episodes\")\n",
    "                    break\n",
    "\n",
    "        # Plot rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(range(1, len(total_rewards)+1), total_rewards, label='Rewards per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the model parameters.\n",
    "        Args:\n",
    "            path (str): File path to save the model.\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'q_net1': self.q_net1.state_dict(),\n",
    "            'q_net2': self.q_net2.state_dict(),\n",
    "            'target_q_net1': self.target_q_net1.state_dict(),\n",
    "            'target_q_net2': self.target_q_net2.state_dict(),\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"\n",
    "        Load the model parameters.\n",
    "        Args:\n",
    "            path (str): File path to load the model from.\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.q_net1.load_state_dict(checkpoint['q_net1'])\n",
    "        self.q_net2.load_state_dict(checkpoint['q_net2'])\n",
    "        self.target_q_net1.load_state_dict(checkpoint['target_q_net1'])\n",
    "        self.target_q_net2.load_state_dict(checkpoint['target_q_net2'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "# Define the Sweep Agent Wrapper\n",
    "class AgentSAC:\n",
    "    def __init__(self, env, config):\n",
    "        # Instantiate SACAgentRAM with environment and config\n",
    "        self.agent = SACAgentRAM(env, config)\n",
    "    \n",
    "    def train(self, max_episodes, stop_criterion, criterion_episodes):\n",
    "        # Delegate training to SACAgentRAM\n",
    "        self.agent.train_agent(max_episodes, stop_criterion, criterion_episodes)\n",
    "    \n",
    "    def save(self, path):\n",
    "        # Delegate model saving to SACAgentRAM\n",
    "        self.agent.save_model(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        # Delegate model loading to SACAgentRAM\n",
    "        self.agent.load_model(path)\n",
    "\n",
    "# Define the Sweep Agent Function\n",
    "def sweep_agent():\n",
    "    try:\n",
    "        # Initialize a new wandb run\n",
    "        with wandb.init() as run:\n",
    "            config = wandb.config\n",
    "\n",
    "            # Create the environment with RAM observation type\n",
    "            env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array_list\", obs_type=\"ram\")\n",
    "            print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "            print(\"Action Space:\", env.action_space)\n",
    "\n",
    "            # Ensure the observation is a 128-length vector\n",
    "            assert env.observation_space.shape == (128,), \"Observation space must be a 128-length vector.\"\n",
    "\n",
    "            # Instantiate AgentSAC with current wandb config\n",
    "            agent = AgentSAC(env=env, config=config)\n",
    "\n",
    "            # Define stopping criterion (optional)\n",
    "            def stopping_criterion(rewards):\n",
    "                # Example: stop if average reward over last 5 episodes >= 100\n",
    "                return np.mean(rewards) >= 100\n",
    "\n",
    "            # Start training\n",
    "            agent.train(\n",
    "                max_episodes=config.num_episodes,\n",
    "                stop_criterion=stopping_criterion,\n",
    "                criterion_episodes=config.criterion_episodes\n",
    "            )\n",
    "\n",
    "            # Save the trained model\n",
    "            agent.save('sac_discrete_boxing_ram.pth')\n",
    "\n",
    "            # Close the environment after training\n",
    "            env.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error to wandb\n",
    "        wandb.log({\"error\": str(e)})\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Ensure the environment is closed in case of an error\n",
    "        try:\n",
    "            env.close()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Define the Sweep Configuration for SAC\n",
    "sweep_configuration_sac = {\n",
    "    \"method\": \"random\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"episode_reward\",  # The metric to optimize\n",
    "        \"goal\": \"maximize\"         # Whether to \"minimize\" or \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr_policy\": {  # Learning rate for the policy network\n",
    "            \"distribution\": \"log_uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"lr_q\": {  # Learning rate for the Q-networks\n",
    "            \"distribution\": \"log_uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"gamma\": {  # Discount factor for future rewards\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.95,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"alpha\": {  # Entropy temperature (only if automatic_alpha is False)\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.1,\n",
    "            \"max\": 0.5\n",
    "        },\n",
    "        \"tau\": {  # Soft update coefficient for target networks\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.001,\n",
    "            \"max\": 0.01\n",
    "        },\n",
    "        \"batch_size\": {  # Mini-batch size for updates\n",
    "            \"values\": [32, 64, 128]  # Three discrete values\n",
    "        },\n",
    "        \"replay_size\": {  # Replay buffer size\n",
    "            \"values\": [5000, 10000, 20000]  # Three discrete values\n",
    "        },\n",
    "        \"hidden_sizes\": {  # Architecture of hidden layers\n",
    "            \"values\": [\"(64,64)\", \"(128,128)\", \"(64,64,64)\", \"(128,128,128)\"]  \n",
    "        },\n",
    "        \"target_update_interval\": {  # Frequency of target network updates\n",
    "            \"values\": [1]  # Fixed to 1 for SAC\n",
    "        },\n",
    "        \"num_episodes\": {  # Total number of training episodes\n",
    "            \"values\": [500]  # Fixed to 500\n",
    "        },\n",
    "        \"criterion_episodes\": {  # Number of episodes for stopping criterion\n",
    "            \"values\": [5]  # Fixed to 5\n",
    "        },\n",
    "        \"automatic_alpha\": {  # Whether to use automatic entropy tuning\n",
    "            \"values\": [True, False]  # Toggle between automatic and manual\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration_sac, project='SAC_Discrete_RAM')\n",
    "\n",
    "# Launch the Sweep with Limited Runs\n",
    "# Adjust 'count' based on your computational resources\n",
    "wandb.agent(sweep_id, function=sweep_agent, count=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a4c5a-856f-43e4-9945-c0f07f622f43",
   "metadata": {},
   "source": [
    "I am also going to add Weights and biases into the equation So I can monitor and log all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41de25f-da2a-4c25-afc4-d138d553de7b",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-17T12:05:54.483825Z",
     "iopub.status.busy": "2024-10-17T12:05:54.483600Z",
     "iopub.status.idle": "2024-10-17T12:05:55.156815Z",
     "shell.execute_reply": "2024-10-17T12:05:55.156471Z",
     "shell.execute_reply.started": "2024-10-17T12:05:54.483811Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7796ae9bd7d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb tristancarlisle/SAC_Discrete_RAM/sweeps/tc9vx463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0496d-b30a-4a6c-9f66-c3f104b99d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "be7dbdba13c56adea3d21db27fce4bc75f0271e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84cd8eb9-2fbc-449d-8545-5ce858f8950e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T13:11:04.818368Z",
     "iopub.status.busy": "2024-10-14T13:11:04.817992Z",
     "iopub.status.idle": "2024-10-14T13:11:04.820215Z",
     "shell.execute_reply": "2024-10-14T13:11:04.819911Z",
     "shell.execute_reply.started": "2024-10-14T13:11:04.818353Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5a899b-ebc8-41e5-8288-85176463a202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T02:05:08.789125Z",
     "iopub.status.busy": "2024-10-14T02:05:08.788707Z",
     "iopub.status.idle": "2024-10-14T02:05:10.046623Z",
     "shell.execute_reply": "2024-10-14T02:05:10.046209Z",
     "shell.execute_reply.started": "2024-10-14T02:05:08.789109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages (4.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install swig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce449b60-1884-4c42-b6d8-4b35b151b68d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T13:21:02.570194Z",
     "iopub.status.busy": "2024-10-14T13:21:02.569867Z",
     "iopub.status.idle": "2024-10-14T13:21:02.572394Z",
     "shell.execute_reply": "2024-10-14T13:21:02.572134Z",
     "shell.execute_reply.started": "2024-10-14T13:21:02.570182Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile SAC_spedup_and_vectorised.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b67f2-1cbe-4fa8-a706-183f86125db5",
   "metadata": {},
   "source": [
    "SAC Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fceb3c20-204d-49eb-88bd-ebd45d00ff08",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-14T01:37:50.752014Z",
     "iopub.status.busy": "2024-10-14T01:37:50.751837Z",
     "iopub.status.idle": "2024-10-14T01:37:54.018393Z",
     "shell.execute_reply": "2024-10-14T01:37:54.017785Z",
     "shell.execute_reply.started": "2024-10-14T01:37:50.752001Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtristancarlisle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241014_093752-89jvmuas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/SAC-Boxing/runs/89jvmuas' target=\"_blank\">absurd-leaf-2</a></strong> to <a href='https://wandb.ai/tristancarlisle/SAC-Boxing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/SAC-Boxing' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC-Boxing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/SAC-Boxing/runs/89jvmuas' target=\"_blank\">https://wandb.ai/tristancarlisle/SAC-Boxing/runs/89jvmuas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'action_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 521\u001b[0m\n\u001b[1;32m    505\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAC-Boxing\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: gamma,\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m: alpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_envs\u001b[39m\u001b[38;5;124m'\u001b[39m: num_envs,\n\u001b[1;32m    519\u001b[0m })\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# Initialize the SAC agent (multiple environments this run though )\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m agent \u001b[38;5;241m=\u001b[39m SACAgent(env_fns, gamma, alpha, hidden_sizes, lr_policy, lr_q, replay_size, batch_size,\n\u001b[1;32m    522\u001b[0m                  target_update_interval, tau)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m    525\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(num_episodes, max_steps_per_episode, updates_per_step)\n",
      "Cell \u001b[0;32mIn[4], line 132\u001b[0m, in \u001b[0;36mSACAgent.__init__\u001b[0;34m(self, env, gamma, alpha, hidden_sizes, lr_policy, lr_q, replay_size, batch_size, target_update_interval, tau, load_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m=\u001b[39m tau  \u001b[38;5;66;03m# Target smoothing coefficient\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_interval \u001b[38;5;241m=\u001b[39m target_update_interval\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39msingle_observation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Replay buffer\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'action_space'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Worker<AsyncVectorEnv>-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 626, in _worker_shared_memory\n",
      "    command, data = pipe.recv()\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 685, in _worker_shared_memory\n",
      "    pipe.send((None, False))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Process Worker<AsyncVectorEnv>-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 626, in _worker_shared_memory\n",
      "    command, data = pipe.recv()\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 685, in _worker_shared_memory\n",
      "    pipe.send((None, False))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Process Worker<AsyncVectorEnv>-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 626, in _worker_shared_memory\n",
      "    command, data = pipe.recv()\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 685, in _worker_shared_memory\n",
      "    pipe.send((None, False))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Process Worker<AsyncVectorEnv>-0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 626, in _worker_shared_memory\n",
      "    command, data = pipe.recv()\n",
      "                    ^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py\", line 685, in _worker_shared_memory\n",
      "    pipe.send((None, False))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 206, in send\n",
      "    self._send_bytes(_ForkingPickler.dumps(obj))\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/tristan/miniconda3/envs/ReinforcementLearning/lib/python3.11/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import moviepy.editor as mpy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import wandb\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility (optional)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Replay buffer for off-policy learning\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, terminated):\n",
    "        self.buffer.append((state, action, reward, next_state, terminated))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = zip(*samples)\n",
    "        return (np.array(state_batch), np.array(action_batch), np.array(reward_batch),\n",
    "                np.array(next_state_batch), np.array(terminated_batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Soft Actor-Critic agent for discrete action spaces with mixed precision\n",
    "class SACAgent:\n",
    "    def __init__(self, env, gamma=0.99, alpha=0.2, hidden_sizes=(128, 128),\n",
    "                 lr_policy=1e-4, lr_q=1e-3, replay_size=100000, batch_size=128,\n",
    "                 target_update_interval=1, tau=0.005, load_path=None):\n",
    "        \n",
    "        self.envs = gym.vector.AsyncVectorEnv(env_fns) # created async vectorised environment to increase speed\n",
    "        self.num_envs = len(env_fns)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Entropy temperature\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau  # Target smoothing coefficient\n",
    "        self.target_update_interval = target_update_interval\n",
    "        \n",
    "        self.num_actions = env.action_space.n\n",
    "        self.observation_shape = self.envs.single_observation_space.shape\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(replay_size)\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = PolicyNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_policy, alpha).to(device)\n",
    "        \n",
    "        # Q-networks\n",
    "        self.q_net1 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        self.q_net2 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        \n",
    "        # Target Q-networks\n",
    "        self.target_q_net1 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        self.target_q_net2 = QNetwork(self.observation_shape, hidden_sizes, self.num_actions, lr_q).to(device)\n",
    "        \n",
    "        # Copy parameters from Q-networks to target networks\n",
    "        self.target_q_net1.load_state_dict(self.q_net1.state_dict())\n",
    "        self.target_q_net2.load_state_dict(self.q_net2.state_dict())\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Mixed precision scaler\n",
    "        self.scaler = GradScaler()\n",
    "\n",
    "        #model load \n",
    "        if load_path and os.path.isfile(load_path):\n",
    "            self.load(load_path)\n",
    "            print(f\"Model loaded from {load_path}\")\n",
    "        \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # (1, k, H, W)\n",
    "        if evaluate:\n",
    "            with torch.no_grad():\n",
    "                logits = self.policy_net(state)\n",
    "                action = torch.argmax(logits, dim=-1).item()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = self.policy_net.sample(state)\n",
    "                action = action.item()\n",
    "        return action\n",
    "    \n",
    "    def update_parameters(self, updates):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch, terminated_batch = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert batches to tensors\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)  # (batch_size, k, H, W)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)\n",
    "        action_batch = torch.LongTensor(action_batch).to(device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1).to(device)\n",
    "        terminated_batch = torch.FloatTensor(terminated_batch).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Normalize rewards\n",
    "        reward_batch = torch.clamp(reward_batch, -1, 1)  # Reward clipping\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Next action probabilities and log probabilities\n",
    "            with autocast():\n",
    "                next_logits = self.policy_net(next_state_batch)\n",
    "                next_probs = F.softmax(next_logits, dim=1)\n",
    "                next_log_probs = F.log_softmax(next_logits, dim=1)\n",
    "                \n",
    "                # Compute target Q-values\n",
    "                target_q1_values = self.target_q_net1(next_state_batch)\n",
    "                target_q2_values = self.target_q_net2(next_state_batch)\n",
    "                target_q_values = torch.min(target_q1_values, target_q2_values)\n",
    "                next_q = (next_probs * (target_q_values - self.alpha * next_log_probs)).sum(dim=1, keepdim=True)\n",
    "                \n",
    "                # Compute target values\n",
    "                q_target = reward_batch + (1 - terminated_batch) * self.gamma * next_q\n",
    "        \n",
    "        # Compute current Q-values\n",
    "        with autocast():\n",
    "            current_q1 = self.q_net1(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "            current_q2 = self.q_net2(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "        \n",
    "            # Compute Q-network losses\n",
    "            q1_loss = self.criterion(current_q1, q_target)\n",
    "            q2_loss = self.criterion(current_q2, q_target)\n",
    "        \n",
    "        # Update Q-networks\n",
    "        self.q_net1.optimizer.zero_grad()\n",
    "        self.scaler.scale(q1_loss).backward()\n",
    "        self.scaler.step(self.q_net1.optimizer)\n",
    "        \n",
    "        self.q_net2.optimizer.zero_grad()\n",
    "        self.scaler.scale(q2_loss).backward()\n",
    "        self.scaler.step(self.q_net2.optimizer)\n",
    "        \n",
    "        # Update policy network\n",
    "        with autocast():\n",
    "            logits = self.policy_net(state_batch)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            \n",
    "            q1_values = self.q_net1(state_batch)\n",
    "            q2_values = self.q_net2(state_batch)\n",
    "            min_q_values = torch.min(q1_values, q2_values)\n",
    "            \n",
    "            policy_loss = (probs * (self.alpha * log_probs - min_q_values)).sum(dim=1).mean()\n",
    "        \n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        self.scaler.scale(policy_loss).backward()\n",
    "        self.scaler.step(self.policy_net.optimizer)\n",
    "        \n",
    "        self.scaler.update()\n",
    "        \n",
    "        # Update target networks\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            self.soft_update(self.q_net1, self.target_q_net1)\n",
    "            self.soft_update(self.q_net2, self.target_q_net2)\n",
    "            \n",
    "        return q1_loss.item(), q2_loss.item(), policy_loss.item() #just for logging\n",
    "    \n",
    "    def soft_update(self, net, target_net):\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def train(self, num_episodes, max_steps_per_episode, updates_per_step, eval_interval=50):\n",
    "        total_rewards = []\n",
    "        updates = 0\n",
    "        # Just adding this so If I run a keyboard interruption the model is still saved\n",
    "        try:\n",
    "            # have to alter pretty heavily so we can run multiple envs \n",
    "            #want to track average losses \n",
    "            q1_losses = []\n",
    "            q2_losses = []\n",
    "            policy_losses = []\n",
    "            states, infos = self.envs.reset()\n",
    "            episode_rewards = np.zeros(self.num_envs)\n",
    "            episode_lengths = np.zeros(self.num_envs)\n",
    "            episode_counts = np.zeros(self.num_envs)\n",
    "            total_episodes = 0\n",
    "            while total_episodes < num_episodes:\n",
    "                actions = self.select_action(states)\n",
    "                next_states, rewards, terminateds, truncateds, infos = self.envs.step(actions)\n",
    "                episode_rewards += rewards\n",
    "                episode_lengths += 1\n",
    "                \n",
    "                for idx in range(self.num_envs):\n",
    "                    self.replay_buffer.push(states[idx], actions[idx], rewards[idx], next_states[idx], terminateds[idx])\n",
    "                \n",
    "                states = next_states\n",
    "                \n",
    "                q1_loss, q2_loss, policy_loss = self.update_parameters(updates)\n",
    "                updates += 1\n",
    "\n",
    "                if q1_loss is not None: #just make sure its not none before we add them to the log\n",
    "                    q1_losses.append(q1_loss)\n",
    "                    q2_losses.append(q2_loss)\n",
    "                    policy_losses.append(policy_loss)\n",
    "                \n",
    "                for idx, done in enumerate(terminateds):\n",
    "                    if done or episode_lengths[idx] >= max_steps_per_episode:\n",
    "                        total_rewards.append(episode_rewards[idx])\n",
    "                        #just add wandb logging \n",
    "                        wandb.log({\n",
    "                        'episode': len(total_rewards),\n",
    "                        'episode_reward': episode_rewards[idx],\n",
    "                        'episode_length': episode_lengths[idx],\n",
    "                        'average_q1_loss': np.mean(q1_losses) if q1_losses else 0,\n",
    "                        'average_q2_loss': np.mean(q2_losses) if q2_losses else 0,\n",
    "                        'average_policy_loss': np.mean(policy_losses) if policy_losses else 0,\n",
    "                    })\n",
    "                        print(f\"Episode {len(total_rewards)}: Total Reward = {episode_rewards[idx]}\")\n",
    "                        episode_rewards[idx] = 0\n",
    "                        episode_lengths[idx] = 0\n",
    "                        episode_counts[idx] += 1\n",
    "                        total_episodes += 1\n",
    "\n",
    "                        # need to reset losses\n",
    "                        q1_losses = []\n",
    "                        q2_losses = []\n",
    "                        policy_losses = []\n",
    "                        \n",
    "                        # I also wanted to add a evaluation step every n intervals so I dont train for 5 days and the lil agent is no good\n",
    "                        if total_episodes % eval_interval == 0:\n",
    "                            self.evaluate(total_episodes)\n",
    "                        \n",
    "                        if total_episodes >= num_episodes:\n",
    "                            break\n",
    "            self.plot_rewards(total_rewards)        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Training interrupted. Saving model...\")\n",
    "            self.save('checkpoint.pth')\n",
    "            print(\"Model saved.\")\n",
    "            # Optionally plot the rewards collected so far\n",
    "            self.plot_rewards(total_rewards)\n",
    "\n",
    "        \n",
    "    #just made the plotting a function so I could call it instead\n",
    "    def plot_rewards(self, total_rewards):\n",
    "            plt.figure(dpi=100)\n",
    "            plt.plot(total_rewards)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.title('Training Rewards over Episodes')\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        \n",
    "    def evaluate(self, episode_num):#its basically the same as the evaluation we use at the end but just log it to WandB as well\n",
    "        eval_env = make_env()() \n",
    "        state, _ = eval_env.reset()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            frames.append(eval_env.render(mode='rgb_array'))\n",
    "            action = self.select_action([state], evaluate=True)[0]\n",
    "            state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        eval_env.close()\n",
    "        video = np.stack(frames)\n",
    "        wandb.log({\n",
    "            'evaluation_reward': total_reward,\n",
    "            'evaluation_episode': episode_num,\n",
    "            'evaluation_video': wandb.Video(video, fps=30, format=\"mp4\")\n",
    "        })\n",
    "        print(f\"Evaluation Episode {episode_num}: Total Reward = {total_reward}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'q_net1': self.q_net1.state_dict(),\n",
    "            'q_net2': self.q_net2.state_dict(),\n",
    "            'target_q_net1': self.target_q_net1.state_dict(),\n",
    "            'target_q_net2': self.target_q_net2.state_dict(),\n",
    "        }, path)\n",
    "\n",
    "        wandb.save(path) #log path\n",
    "    \n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.q_net1.load_state_dict(checkpoint['q_net1'])\n",
    "        self.q_net2.load_state_dict(checkpoint['q_net2'])\n",
    "        self.target_q_net1.load_state_dict(checkpoint['target_q_net1'])\n",
    "        self.target_q_net2.load_state_dict(checkpoint['target_q_net2'])\n",
    "\n",
    "# Policy network for SAC with simplified architecture\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_sizes, output_size, learning_rate, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Entropy temperature coefficient\n",
    "        \n",
    "        # Simplified CNN layers\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, kernel_size=8, stride=4),  # Reduced filters from 32 to 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),              # Reduced filters from 64 to 32\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the size after CNN layers\n",
    "        self.flattened_size = self._get_flattened_size(input_shape)\n",
    "        \n",
    "        # Reduced fully connected layers\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.flattened_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer for action probabilities\n",
    "        layers.append(nn.Linear(hidden_sizes[0], output_size))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def _get_flattened_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape).to(device)\n",
    "            conv_out = self.conv_net(dummy_input)\n",
    "            return conv_out.view(1, -1).size(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.net(x)\n",
    "        return logits  # Logits for categorical distribution\n",
    "    \n",
    "    def sample(self, state):\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob, dist.probs\n",
    "\n",
    "# Q-Network for SAC with simplified architecture\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_sizes, num_actions, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simplified CNN layers\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 16, kernel_size=8, stride=4),  # Reduced filters from 32 to 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),              # Reduced filters from 64 to 32\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Compute the size after CNN layers\n",
    "        self.flattened_size = self._get_flattened_size(input_shape)\n",
    "        \n",
    "        # Reduced fully connected layers\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.flattened_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer for Q-values for each action\n",
    "        layers.append(nn.Linear(hidden_sizes[0], num_actions))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def _get_flattened_size(self, input_shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape).to(device)\n",
    "            conv_out = self.conv_net(dummy_input)\n",
    "            return conv_out.view(1, -1).size(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        q_values = self.net(x)\n",
    "        return q_values  # Q-values for each action\n",
    "\n",
    "# Create the environment with preprocessing and frame stacking\n",
    "def make_env():\n",
    "    def _init():\n",
    "        # Original environment\n",
    "        env = gym.make('ALE/Boxing-v5', render_mode=\"human\")\n",
    "        # Preprocess frames\n",
    "        env = PreprocessFrame(env, height=84, width=84, grayscale=True)\n",
    "        # Increased frame skip\n",
    "        env = FrameSkip(env, skip=4)\n",
    "        # Frame stack\n",
    "        env = FrameStack(env, k=4)\n",
    "        # Reduce action space\n",
    "        action_mapping = [0, 2, 3, 4, 5, 1]  # Indices of actions: NOOP, UP, RIGHT, LEFT, DOWN, FIRE\n",
    "        env = ActionSpaceReducer(env, action_mapping)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "# multiple environment instances\n",
    "num_envs = 4  \n",
    "env_fns = [make_env() for _ in range(num_envs)]\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "hidden_sizes = (128, )  # Reduced hidden layer size\n",
    "lr_policy = 1e-4\n",
    "lr_q = 1e-3\n",
    "replay_size = 100000\n",
    "batch_size = 128  # Increased batch size ccause it took way to long\n",
    "target_update_interval = 1\n",
    "tau = 0.005\n",
    "num_episodes = 500  # Reduced number of episodes took too long\n",
    "max_steps_per_episode = 500  # Reduced max steps per episode took too long\n",
    "updates_per_step = 1\n",
    "\n",
    "wandb.init(project='SAC-Boxing', config={\n",
    "    'gamma': gamma,\n",
    "    'alpha': alpha,\n",
    "    'hidden_sizes': hidden_sizes,\n",
    "    'lr_policy': lr_policy,\n",
    "    'lr_q': lr_q,\n",
    "    'replay_size': replay_size,\n",
    "    'batch_size': batch_size,\n",
    "    'target_update_interval': target_update_interval,\n",
    "    'tau': tau,\n",
    "    'num_episodes': num_episodes,\n",
    "    'max_steps_per_episode': max_steps_per_episode,\n",
    "    'updates_per_step': updates_per_step,\n",
    "    'num_envs': num_envs,\n",
    "})\n",
    "# Initialize the SAC agent (multiple environments this run though )\n",
    "agent = SACAgent(env_fns, gamma, alpha, hidden_sizes, lr_policy, lr_q, replay_size, batch_size,\n",
    "                 target_update_interval, tau)\n",
    "\n",
    "# Train the agent\n",
    "agent.train(num_episodes, max_steps_per_episode, updates_per_step)\n",
    "\n",
    "# Visualize one episode\n",
    "env = make_env()()\n",
    "state, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = agent.select_action([state], evaluate=True)[0]\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Create and play video clip using the frames and given fps\n",
    "clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "clip.ipython_display(rd_kwargs=dict(logger=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f67b67-6f2d-4509-b7d0-f558dce1e011",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d824e59-9a6e-4b63-aa97-e79b1d87bc10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Modifications\n",
    "1. PPOAgent Class\n",
    "Initialization:\n",
    "\n",
    "Replaced SACAgent with PPOAgent.\n",
    "Added parameters specific to PPO like lam (GAE lambda), clip_epsilon, entropy_coef, etc.\n",
    "Removed the replay buffer since PPO is an on-policy algorithm.\n",
    "select_action Method:\n",
    "\n",
    "Returns not just the action but also the action probabilities, state value, log probability, and entropy.\n",
    "Uses the PolicyValueNetwork to get action probabilities and state value.\n",
    "compute_gae Method:\n",
    "\n",
    "Computes Generalized Advantage Estimation (GAE) for the collected rollouts.\n",
    "update Method:\n",
    "\n",
    "Performs PPO updates using the collected rollouts.\n",
    "Calculates policy loss using the clipped surrogate objective.\n",
    "Calculates value loss using mean squared error.\n",
    "Includes entropy bonus to encourage exploration.\n",
    "train Method:\n",
    "\n",
    "Collects rollouts by interacting with the environment.\n",
    "Stores experiences in RolloutStorage.\n",
    "After each rollout, computes advantages and returns.\n",
    "Calls the update method to update the policy and value networks.\n",
    "Logs training metrics to W&B.\n",
    "\n",
    "2. PolicyValueNetwork Class\n",
    "Combines both the policy network and the value network into a single network with shared layers.\n",
    "Outputs action probabilities (after applying softmax) and state value.\n",
    "Uses convolutional layers followed by fully connected layers.\n",
    "3. RolloutStorage Class\n",
    "Stores collected experiences during rollouts.\n",
    "Keeps track of states, actions, rewards, values, log probabilities, dones, advantages, and returns.\n",
    "Provides methods to add data and clear the storage.\n",
    "4. Environment Setup\n",
    "Increased num_envs to 8 for better sample efficiency.\n",
    "Kept the same preprocessing steps as before.\n",
    "5. W&B Integration\n",
    "Changed the project name to 'PPO-Boxing'.\n",
    "Logged additional metrics such as policy loss, value loss, and entropy.\n",
    "Continued logging episode rewards and lengths.\n",
    "Logged evaluation videos at specified intervals.\n",
    "6. Training Parameters\n",
    "Adjusted hyperparameters suitable for PPO:\n",
    "Set batch_size to 256.\n",
    "Set rollout_length to 128.\n",
    "Adjusted entropy_coef and vf_coef.\n",
    "7. Evaluation\n",
    "Modified the evaluate method within PPOAgent to work with the new agent structure.\n",
    "Ensured that evaluation episodes are logged to W&B with videos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3a6ee-7819-4caa-b5e6-c73612013f0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Key Changes:\n",
    "Objective Function:\n",
    "\n",
    "Transition from a standard Actor-Critic loss to the PPO v2 clipped surrogate objective.\n",
    "Advantage Estimation:\n",
    "\n",
    "Implement Generalized Advantage Estimation (GAE) for computing advantages.\n",
    "Policy Update:\n",
    "\n",
    "Modify the policy update to incorporate the clipped surrogate objective.\n",
    "Rollout Collection:\n",
    "\n",
    "Accumulate rollouts (trajectories) before performing updates, aligning with PPO's batch update mechanism.\n",
    "Data Normalization:\n",
    "\n",
    "Normalize advantages to stabilize training.\n",
    "Batch Processing:\n",
    "\n",
    "Update the networks using mini-batches extracted from the collected rollouts.\n",
    "Logging and Evaluation:\n",
    "\n",
    "Integrate logging mechanisms to monitor training progress and perform periodic evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d42ef-061a-451e-80e6-7f2dac7cff58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Proximal Policy Optimization (PPO) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a560b-c156-4a8a-93df-6dc79552ce79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Proximal Policy Optimization (PPO) RAM Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca5453-c5d6-45dc-9d67-beced461770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Policy network for approximating policy function \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Value network for approximating value function\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc6d466-934c-46af-9cab-9f0f67d97953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollout storage for PPO\n",
    "class RolloutStorage:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "    \n",
    "    def add(self, state, action, reward, value, log_prob, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.advantages = []\n",
    "        self.returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e7a73-342d-49e5-91dc-8121807c131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the environment with RAM observation type\n",
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array\", obs_type=\"ram\")\n",
    "print(\"Observation Space High:\", env.observation_space.high)\n",
    "print(\"Observation Space Low:\", env.observation_space.low)\n",
    "print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "\n",
    "# Ensure the observation is a 128-length vector\n",
    "assert env.observation_space.shape == (128,), \"Observation space must be a 128-length vector.\"\n",
    "\n",
    "# Define hyperparameters with updated hidden_sizes\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "hidden_sizes = (128, 128, 128)  # Updated to include three hidden layers\n",
    "lr_policy = 3e-4\n",
    "lr_value = 1e-3\n",
    "clip_epsilon = 0.2\n",
    "epochs = 4\n",
    "batch_size = 64\n",
    "entropy_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_episodes = 1000\n",
    "rollout_length = 128    # Number of steps to collect before an update\n",
    "eval_interval = 50      # Evaluate every 50 episodes\n",
    "criterion_episodes = 5  # Number of episodes to consider for stopping criterion\n",
    "\n",
    "# Initialize W&B with updated hidden_sizes\n",
    "wandb.init(\n",
    "    project='PPO-Boxing',\n",
    "    name='PPO_v2_Boxing_Ram',\n",
    "    config={\n",
    "        'gamma': gamma,\n",
    "        'lam': lam,\n",
    "        'hidden_sizes': hidden_sizes,  # Updated hidden_sizes\n",
    "        'lr_policy': lr_policy,\n",
    "        'lr_value': lr_value,\n",
    "        'clip_epsilon': clip_epsilon,\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'entropy_coef': entropy_coef,\n",
    "        'vf_coef': vf_coef,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'num_episodes': num_episodes,\n",
    "        'rollout_length': rollout_length,\n",
    "        'eval_interval': eval_interval,\n",
    "        'criterion_episodes': criterion_episodes\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb00fd-1221-4304-b1c2-9be399f98f50",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, gamma=0.99, lam=0.95, hidden_sizes=(128, 128, 128), \n",
    "                 lr_policy=3e-4, lr_value=1e-3, clip_epsilon=0.2, epochs=4, \n",
    "                 batch_size=64, entropy_coef=0.01, vf_coef=0.5, max_grad_norm=0.5):\n",
    "        # Initialize environment and hyperparameters\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam  # GAE lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        # Get action and state dimensions\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.state_dims = self.env.observation_space.shape[0]  # 128-length vector\n",
    "\n",
    "        # Initialize policy and value networks\n",
    "        self.policynet = PolicyNetwork(self.state_dims, hidden_sizes, self.num_actions).to(device)\n",
    "        self.valuenet = ValueNetwork(self.state_dims, hidden_sizes).to(device)\n",
    "\n",
    "        # Initialize optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policynet.parameters(), lr=lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.valuenet.parameters(), lr=lr_value)\n",
    "\n",
    "        # Initialize rollout storage\n",
    "        self.storage = RolloutStorage()\n",
    " ########################################################################################################################################################################################################    \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        logits = self.policynet(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if evaluate:\n",
    "            action = torch.argmax(dist.probs, dim=-1)\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        value = self.valuenet(state)\n",
    "        return action.item(), log_prob.item(), entropy.item(), value.item()\n",
    "########################################################################################################################################################################################################     \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        values = values.tolist() + [0]  # Append 0 for terminal state\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        return advantages\n",
    "########################################################################################################################################################################################################     \n",
    "    def update(self):\n",
    "        # Convert rollout storage to tensors\n",
    "        states = torch.FloatTensor(self.storage.states).to(device)\n",
    "        actions = torch.LongTensor(self.storage.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.storage.log_probs).to(device)\n",
    "        returns = torch.FloatTensor(self.storage.returns).to(device)\n",
    "        advantages = torch.FloatTensor(self.storage.advantages).to(device)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Prepare dataset for mini-batch updates\n",
    "        dataset = torch.utils.data.TensorDataset(states, actions, old_log_probs, returns, advantages)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropies = []\n",
    "\n",
    "        # Perform multiple epochs of updates\n",
    "        for _ in range(self.epochs):\n",
    "            for batch in loader:\n",
    "                b_states, b_actions, b_old_log_probs, b_returns, b_advantages = batch\n",
    "\n",
    "                # Forward pass through policy network\n",
    "                logits = self.policynet(b_states)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                new_log_probs = dist.log_prob(b_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # Compute ratio for PPO clipping\n",
    "                ratio = (new_log_probs - b_old_log_probs).exp()  # r(theta)\n",
    "                surr1 = ratio * b_advantages  # r(theta) * A\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * b_advantages  # Clipped\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()  # PPO clipped objective\n",
    "\n",
    "                # Compute value loss\n",
    "                values = self.valuenet(b_states).squeeze(-1)\n",
    "                value_loss = F.mse_loss(values, b_returns)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + self.vf_coef * value_loss - self.entropy_coef * entropy\n",
    "\n",
    "                # Backpropagation\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                self.value_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_norm_(self.policynet.parameters(), self.max_grad_norm)\n",
    "                nn.utils.clip_grad_norm_(self.valuenet.parameters(), self.max_grad_norm)\n",
    "\n",
    "                # Update networks\n",
    "                self.policy_optimizer.step()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # Logging losses\n",
    "                policy_losses.append(policy_loss.item())\n",
    "                value_losses.append(value_loss.item())\n",
    "                entropies.append(entropy.item())\n",
    "\n",
    "        # Clear rollout storage after updates\n",
    "        self.storage.clear()\n",
    "\n",
    "        # Log average losses to W&B\n",
    "        wandb.log({\n",
    "            'Policy Loss': np.mean(policy_losses),\n",
    "            'Value Loss': np.mean(value_losses),\n",
    "            'Entropy': np.mean(entropies)\n",
    "        })\n",
    "\n",
    "        return np.mean(policy_losses), np.mean(value_losses), np.mean(entropies)\n",
    "########################################################################################################################################################################################################     \n",
    "    def train_agent(self, max_episodes, rollout_length, eval_interval=50, stop_criterion=None, criterion_episodes=5):\n",
    "        total_rewards = []\n",
    "        moving_average = deque(maxlen=criterion_episodes)\n",
    "    \n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            # Reset environment and get initial state\n",
    "            state, _ = self.env.reset()\n",
    "            state = state.flatten()  # ccheck state is a 1D vector of length 128\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "    \n",
    "            for step in range(rollout_length):\n",
    "                # Select action using current policy\n",
    "                action, log_prob, entropy, value = self.select_action(state)\n",
    "                \n",
    "                # Execute action in the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                next_state = next_state.flatten()  # Ensure next_state is a 1D vector of length 128\n",
    "    \n",
    "                # Store experience in rollout\n",
    "                self.storage.add(state, action, reward, value, log_prob, terminated or truncated)\n",
    "    \n",
    "                # Update state and cumulative reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "    \n",
    "                # If episode is done, exit the loop\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "    \n",
    "            # Append the reward for this episode\n",
    "            total_rewards.append(episode_reward)\n",
    "            moving_average.append(episode_reward)\n",
    "    \n",
    "            # Compute returns and advantages after rollout\n",
    "            rewards = np.array(self.storage.rewards)\n",
    "            values = np.array(self.storage.values)\n",
    "            dones = np.array(self.storage.dones)\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            returns = advantages + values  # Element-wise addition using NumPy arrays\n",
    "    \n",
    "            # Store advantages and returns in rollout storage for updating\n",
    "            self.storage.advantages = advantages\n",
    "            self.storage.returns = returns.tolist()\n",
    "    \n",
    "            # Perform PPO update using collected rollouts\n",
    "            avg_policy_loss, avg_value_loss, avg_entropy = self.update()\n",
    "    \n",
    "            # Log episode reward to W&B\n",
    "            wandb.log({\n",
    "                'Episode': episode,\n",
    "                'Reward': episode_reward,\n",
    "                'Episode Length': episode_length\n",
    "            })\n",
    "    \n",
    "            # Print progress\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Length = {episode_length}, \"\n",
    "                  f\"Policy Loss = {avg_policy_loss:.4f}, Value Loss = {avg_value_loss:.4f}, Entropy = {avg_entropy:.4f}\")\n",
    "    \n",
    "            # Check stopping criterion\n",
    "            if stop_criterion is not None and stop_criterion(list(moving_average)):\n",
    "                print(f\"\\nStopping criterion satisfied after {episode} episodes.\")\n",
    "                break\n",
    "    \n",
    "            # Periodic evaluation\n",
    "            if episode % eval_interval == 0:\n",
    "                self.evaluate(episode_num=episode)\n",
    "    \n",
    "        # Plot rewards after training\n",
    "        self.plot_rewards(total_rewards)\n",
    "\n",
    "########################################################################################################################################################################################################     \n",
    "    def evaluate(self, episode_num):\n",
    "        state, _ = self.env.reset()\n",
    "        state = state.flatten()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            # Render environment frame and store for video\n",
    "            frame = self.env.render()\n",
    "            frames.append(frame)\n",
    "\n",
    "            # Select action deterministically\n",
    "            action, _, _, _ = self.select_action(state, evaluate=True)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "\n",
    "            # Update state and cumulative reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        v= VideoRecorderRAM('PPO')\n",
    "        v.frames=frames\n",
    "        vfilename = f'PPO_RAM_epsiode_{episode_num}.mp4'\n",
    "        v.save(vfilename)\n",
    "        video_path=\"PPO/\" + vfilename\n",
    "        # Save video of the evaluation episode\n",
    "        #video = np.stack(frames)\n",
    "        #video_path = f\"evaluation_episode_{episode_num}.mp4\"\n",
    "        #mpy_clip = mpy.ImageSequenceClip(list(video), fps=30)\n",
    "        #mpy_clip.write_videofile(video_path, codec=\"libx264\")\n",
    "\n",
    "        # Log evaluation results and video to W&B\n",
    "        wandb.log({\n",
    "            'Evaluation Episode': episode_num,\n",
    "            'Evaluation Reward': total_reward,\n",
    "            'Evaluation Video': wandb.Video(video_path, fps=30, format=\"mp4\")\n",
    "        })\n",
    "\n",
    "        # Logging evaluation result\n",
    "        print(f\"Evaluation Episode {episode_num}: Total Reward = {total_reward}\")\n",
    "######################################################################################################################################################################################################## \n",
    "    def plot_rewards(self, total_rewards):\n",
    "        # Plot the rewards received during training\n",
    "        plt.figure(dpi=100)\n",
    "        plt.plot(total_rewards, label='Rewards per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Training Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "########################################################################################################################################################################################################     \n",
    "    def save(self, path):\n",
    "        # Save network weights to a file\n",
    "        torch.save({\n",
    "            'policy_net': self.policynet.state_dict(),\n",
    "            'value_net': self.valuenet.state_dict()\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "########################################################################################################################################################################################################     \n",
    "    def load(self, path):\n",
    "        # Load network weights from a file\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policynet.load_state_dict(checkpoint['policy_net'])\n",
    "        self.valuenet.load_state_dict(checkpoint['value_net'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "######################################################################################################################################################################################################## \n",
    "# Initialize PPOAgent with updated hidden_sizes\n",
    "agent = PPOAgent(\n",
    "    env=env,\n",
    "    gamma=gamma,\n",
    "    lam=lam,\n",
    "    hidden_sizes=hidden_sizes,  # (128, 128, 128)\n",
    "    lr_policy=lr_policy,\n",
    "    lr_value=lr_value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    entropy_coef=entropy_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm\n",
    ")\n",
    "\n",
    "# Define stopping criterion (optional)\n",
    "def stopping_criterion(rewards):\n",
    "    return np.mean(rewards) >= 100  # Example: stop if average reward over last 5 episodes >= 100\n",
    "\n",
    "# Start training\n",
    "agent.train_agent(\n",
    "    max_episodes=num_episodes,\n",
    "    rollout_length=rollout_length,\n",
    "    eval_interval=eval_interval,\n",
    "    stop_criterion=stopping_criterion,\n",
    "    criterion_episodes=criterion_episodes\n",
    ")\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80f30317-d49c-420a-b3cb-0954eac17773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T12:13:54.251997Z",
     "iopub.status.busy": "2024-10-17T12:13:54.251793Z",
     "iopub.status.idle": "2024-10-17T12:13:54.969019Z",
     "shell.execute_reply": "2024-10-17T12:13:54.968703Z",
     "shell.execute_reply.started": "2024-10-17T12:13:54.251984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/PPO-Boxing/runs/x8enwys0?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7796aeb36990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb tristancarlisle/PPO-Boxing/runs/x8enwys0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c36d14-99db-4388-82df-b673b263d879",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PPO also Won no knowout though "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccf3ab-7801-4f2b-b936-ee88a5d3de65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Proximal Policy Optimization (PPO) Image Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed595ac5-a824-478d-bb8e-bfcd3c9aaf9e",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Policy Value Network\n",
    "Similar to the Dueling structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fded590-ba78-4fc9-8ed0-c65eac9378ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from REINFORCE actor-critic prac 9 \n",
    "#changed dramatically for images though \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_sizes, num_actions):\n",
    "        super().__init__()\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        #Conv feature extraction section\n",
    "        layers.append(nn.Conv2d(input_size, 32, kernel_size=8, stride=4))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(32, 64, kernel_size=4, stride=2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(64, 64, kernel_size=3, stride=1))    \n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Flatten())\n",
    "        conv_out_size\n",
    "        #Fully connectlayer\n",
    "        layers.append\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Policy head\n",
    "        self.policy_head = nn.Linear(hidden_sizes[0], num_actions)\n",
    "        \n",
    "        # Value head\n",
    "        self.value_head = nn.Linear(hidden_sizes[0], 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_net(x)\n",
    "        x = self.shared_fc(x)\n",
    "        logits = self.policy_head(x)\n",
    "        action_probs = F.softmax(logits, dim=-1)\n",
    "        state_value = self.value_head(x)\n",
    "        return action_probs, state_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738dd496-ba0f-4545-acd2-0254b42368d0",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99f78f76-c2d6-4c7e-b3bd-3f65929693d8",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-10-15T02:10:07.241630Z",
     "iopub.status.busy": "2024-10-15T02:10:07.241471Z",
     "iopub.status.idle": "2024-10-15T02:10:11.675432Z",
     "shell.execute_reply": "2024-10-15T02:10:11.674979Z",
     "shell.execute_reply.started": "2024-10-15T02:10:07.241618Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4eqecqx3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁</td></tr><tr><td>episode_reward</td><td>▁</td></tr><tr><td>epsilon threshold</td><td>▁</td></tr><tr><td>steps</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>99</td></tr><tr><td>episode_reward</td><td>-15</td></tr><tr><td>epsilon threshold</td><td>0.0767</td></tr><tr><td>steps</td><td>178600</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DDQN dueling</strong> at: <a href='https://wandb.ai/tristancarlisle/DQN/runs/4eqecqx3' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN/runs/4eqecqx3</a><br/> View project at: <a href='https://wandb.ai/tristancarlisle/DQN' target=\"_blank\">https://wandb.ai/tristancarlisle/DQN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241014_212918-4eqecqx3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4eqecqx3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tristan/UniStuff/ReinforcementLearning/Assignment/wandb/run-20241015_101007-p9pe6sx9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristancarlisle/PPO-Boxing/runs/p9pe6sx9' target=\"_blank\">smart-surf-1</a></strong> to <a href='https://wandb.ai/tristancarlisle/PPO-Boxing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristancarlisle/PPO-Boxing' target=\"_blank\">https://wandb.ai/tristancarlisle/PPO-Boxing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristancarlisle/PPO-Boxing/runs/p9pe6sx9' target=\"_blank\">https://wandb.ai/tristancarlisle/PPO-Boxing/runs/p9pe6sx9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'PreprocessFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 331\u001b[0m\n\u001b[1;32m    312\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO-Boxing\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m: gamma,\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlam\u001b[39m\u001b[38;5;124m'\u001b[39m: lam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_envs\u001b[39m\u001b[38;5;124m'\u001b[39m: num_envs,\n\u001b[1;32m    328\u001b[0m })\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Initialize the PPO agent\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPOAgent(\n\u001b[1;32m    332\u001b[0m     env_fns,\n\u001b[1;32m    333\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mgamma,\n\u001b[1;32m    334\u001b[0m     lam\u001b[38;5;241m=\u001b[39mlam,\n\u001b[1;32m    335\u001b[0m     hidden_sizes\u001b[38;5;241m=\u001b[39mhidden_sizes,\n\u001b[1;32m    336\u001b[0m     lr_policy\u001b[38;5;241m=\u001b[39mlr_policy,\n\u001b[1;32m    337\u001b[0m     lr_value\u001b[38;5;241m=\u001b[39mlr_value,\n\u001b[1;32m    338\u001b[0m     clip_epsilon\u001b[38;5;241m=\u001b[39mclip_epsilon,\n\u001b[1;32m    339\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    340\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    341\u001b[0m     entropy_coef\u001b[38;5;241m=\u001b[39mentropy_coef,\n\u001b[1;32m    342\u001b[0m     vf_coef\u001b[38;5;241m=\u001b[39mvf_coef,\n\u001b[1;32m    343\u001b[0m     max_grad_norm\u001b[38;5;241m=\u001b[39mmax_grad_norm\n\u001b[1;32m    344\u001b[0m )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m    347\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(num_episodes, max_steps_per_episode, rollout_length)\n",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m, in \u001b[0;36mPPOAgent.__init__\u001b[0;34m(self, env_fns, gamma, lam, hidden_sizes, lr_policy, lr_value, clip_epsilon, epochs, batch_size, entropy_coef, vf_coef, max_grad_norm)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, hidden_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m      3\u001b[0m              lr_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, lr_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, clip_epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      4\u001b[0m              entropy_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, vf_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, max_grad_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mvector\u001b[38;5;241m.\u001b[39mAsyncVectorEnv(env_fns)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env_fns)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/miniconda3/envs/ReinforcementLearning/lib/python3.11/site-packages/gymnasium/vector/async_vector_env.py:105\u001b[0m, in \u001b[0;36mAsyncVectorEnv.__init__\u001b[0;34m(self, env_fns, observation_space, action_space, shared_memory, copy, context, daemon, worker)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared_memory \u001b[38;5;241m=\u001b[39m shared_memory\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 105\u001b[0m dummy_env \u001b[38;5;241m=\u001b[39m env_fns[\u001b[38;5;241m0\u001b[39m]()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m dummy_env\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (observation_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (action_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "Cell \u001b[0;32mIn[36], line 284\u001b[0m, in \u001b[0;36mmake_env.<locals>._init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init\u001b[39m():\n\u001b[1;32m    283\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALE/Boxing-v5\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 284\u001b[0m     env \u001b[38;5;241m=\u001b[39m PreprocessFrame(env, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m84\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m84\u001b[39m, grayscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    285\u001b[0m     env \u001b[38;5;241m=\u001b[39m FrameSkip(env, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    286\u001b[0m     env \u001b[38;5;241m=\u001b[39m FrameStack(env, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PreprocessFrame' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class PPOAgent:\n",
    "    def __init__(self, gamma=0.99, lam=0.95, hidden_sizes=(128,128),\n",
    "                 lr_policy=1e-4, lr_value=1e-3, clip_epsilon=0.2, episodes=100, batch_size=128,\n",
    "                 entropy_coef=0.01, vf_coef=0.5, max_grad_norm=0.5):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam  # GAE lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.episodes = episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "        \n",
    "        # Policy and Value network\n",
    "        self.policy_net = PolicyValueNetwork(self.observation_shape, hidden_sizes, self.num_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr_policy)\n",
    "        \n",
    "        # Rollout storage\n",
    "        self.storage = RolloutStorage()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            action_probs, value = self.policy_net(state)\n",
    "            dist = torch.distributions.Categorical(probs=action_probs)\n",
    "            if evaluate:\n",
    "                action = torch.argmax(action_probs, dim=-1)\n",
    "            else:\n",
    "                action = dist.sample()\n",
    "        return action.cpu().numpy()[0], action_probs.cpu().numpy()[0], value.cpu().numpy()[0], dist.log_prob(action).cpu().numpy()[0], dist.entropy().cpu().numpy()[0]\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        values = values + [0]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        return advantages\n",
    "    \n",
    "    def update(self):\n",
    "        states = torch.FloatTensor(np.array(self.storage.states)).to(device)\n",
    "        actions = torch.LongTensor(self.storage.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.storage.log_probs).to(device)\n",
    "        returns = torch.FloatTensor(self.storage.returns).to(device)\n",
    "        advantages = torch.FloatTensor(self.storage.advantages).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(states, actions, old_log_probs, returns, advantages)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False) \n",
    "        \n",
    "        total_policy_loss = 0\n",
    "        total_value_loss = 0\n",
    "        total_entropy = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            b_states, b_actions, b_old_log_probs, b_returns, b_advantages = batch\n",
    "            \n",
    "            action_probs, values = self.policy_net(b_states)\n",
    "            dist = torch.distributions.Categorical(probs=action_probs)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(b_actions)\n",
    "            \n",
    "            # Ratio for PPO clipping\n",
    "            ratio = (new_log_probs - b_old_log_probs).exp()\n",
    "            surr1 = ratio * b_advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * b_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value function loss\n",
    "            value_loss = F.mse_loss(values.squeeze(-1), b_returns)# have to squeeze values again\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + self.vf_coef * value_loss - self.entropy_coef * entropy\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "            total_entropy += entropy.item()\n",
    "        \n",
    "        num_updates =  len(loader)\n",
    "        avg_policy_loss = total_policy_loss / num_updates\n",
    "        avg_value_loss = total_value_loss / num_updates\n",
    "        avg_entropy = total_entropy / num_updates\n",
    "        \n",
    "        # Clear storage\n",
    "        self.storage.clear()\n",
    "        \n",
    "        return avg_policy_loss, avg_value_loss, avg_entropy\n",
    "    \n",
    "    def train(self, num_episodes, max_steps_per_episode, rollout_length, eval_interval=50):\n",
    "        total_rewards = []\n",
    "        try:\n",
    "            state = self.env.reset()[0]\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            total_episodes = 0\n",
    "            step = 0\n",
    "            \n",
    "           while total_episodes < num_episodes:\n",
    "                # Collect rollout\n",
    "                for _ in range(rollout_length):\n",
    "                    action, action_probs, value, log_prob, entropy = self.select_action(state)\n",
    "                    next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                    \n",
    "                    self.storage.add(state, action, reward, value, log_prob, terminated or truncated)\n",
    "                    \n",
    "                    episode_reward += reward\n",
    "                    episode_length += 1\n",
    "                    step += 1\n",
    "                    \n",
    "                    state = next_state\n",
    "                    \n",
    "\n",
    "                    if terminated or truncated or episode_length >= max_steps_per_episode:\n",
    "                        total_rewards.append(episode_reward)\n",
    "                        \n",
    "                        # Log to W&B\n",
    "                        wandb.log({\n",
    "                            'episode': len(total_rewards),\n",
    "                            'episode_reward': episode_rewards[idx],\n",
    "                            'episode_length': episode_lengths[idx],\n",
    "                        })\n",
    "                        print(f\"Episode {len(total_rewards)}: Total Reward = {episode_rewards[idx]}\")\n",
    "                        episode_rewards[idx] = 0\n",
    "                        episode_lengths[idx] = 0\n",
    "                        total_episodes += 1\n",
    "\n",
    "                        #added to check how training is going so I can stop it its no good. I just aet to half way first\n",
    "                        if total_episodes % eval_interval == 0:\n",
    "                            self.evaluate(total_episodes)\n",
    "                        \n",
    "                        #end training after max number of episodes\n",
    "                        if total_episodes >= num_episodes:\n",
    "                            break\n",
    "        \n",
    "                \n",
    "                # Compute returns and advantages\n",
    "                with torch.no_grad():\n",
    "                    _, next_value =self.policy_net(torch.FloatTensor(state).unsqueeze(0).to(device))\n",
    "                    next_value = next_value.squeeze(-1).cpu().numpy() #back to cpu to convert to numpy :| , also have to resqueeze\n",
    "                advantages = self.compute_gae(self.storage.rewards, self.storage.values + [next_value], self.storage.dones)\n",
    "                returns = [adv + val for adv, val in zip(advantages, self.storage.values)]\n",
    "                self.storage.advantages = advantages\n",
    "                self.storage.returns = returns\n",
    "                \n",
    "                # Update policy and value networks\n",
    "                avg_policy_loss, avg_value_loss, avg_entropy = self.update()\n",
    "                \n",
    "                # Log to W&B\n",
    "                wandb.log({\n",
    "                    'policy_loss': avg_policy_loss,\n",
    "                    'value_loss': avg_value_loss,\n",
    "                    'entropy': avg_entropy,\n",
    "                })\n",
    "                \n",
    "            # Plot rewards\n",
    "            self.plot_rewards(total_rewards)\n",
    "\n",
    "        #I kept interupting training and I wanted to see a plot so I added a keyboard exception so if I cntrl c I can see how the training was going \n",
    "        #I also save the check point so I can continue where I left off \n",
    "        except KeyboardInterrupt: \n",
    "            print(\"training interrupted saving model\")\n",
    "            self.save('ppo_checkpoint.pth')\n",
    "            print(\"model saved.\")\n",
    "            self.plot_rewards(total_rewards)\n",
    "    \n",
    "    #just made the plotting a function so I could call it instead\n",
    "    def plot_rewards(self, total_rewards):\n",
    "            plt.figure(dpi=100)\n",
    "            plt.plot(total_rewards)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.title('Training Rewards over Episodes')\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "    #turned the evaluation section into a function because I got sick of redoing it \n",
    "    def evaluate(self, episode_num):\n",
    "        eval_env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "        eval_env = gym.wrappers.ResizeObservation(eval_env, shape=(84, 84))\n",
    "        eval_env = gym.wrappers.NormalizeObservation(eval_env)\n",
    "        \n",
    "        state = eval_env.reset()[0]\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            frame = eval_env.render(mode='rgb_array')\n",
    "            frames.append(frame)\n",
    "            action, _, _, _, _ = self.select_action(state, evaluate=True)\n",
    "            state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        eval_env.close()\n",
    "        video = np.stack(frames)\n",
    "        wandb.log({\n",
    "            'evaluation_reward': total_reward,\n",
    "            'evaluation_episode': episode_num,\n",
    "            'evaluation_video': wandb.Video(video, fps=30, format=\"mp4\")\n",
    "        })\n",
    "        print(f\"Evaluation Episode {episode_num}: Total Reward = {total_reward}\")\n",
    "\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path)\n",
    "        wandb.save(path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.policy_net.load_state_dict(torch.load(path))gamma = 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a3f39-3083-4f62-a9ad-b2d5c0bd4e7f",
   "metadata": {},
   "source": [
    "Added a rollout storage function for PPO to keep track "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4f47d-c6d2-423a-9468-b5b406f0573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutStorage:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "    \n",
    "    def add(self, state, action, reward, value, log_prob, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.advantages = []\n",
    "        self.returns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798c429-08ed-45b9-a820-e22371713c59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Env build, logging and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c140f-e10d-45e5-ad40-54793a4dcfda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the environment with preprocessing and frame stacking\n",
    "env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array\",obs_type=\"grayscale\")\n",
    "env = gym.wrappers.ResizeObservation(env, shape=(84, 84))    # Resize to 84x84\n",
    "env = gym.wrappers.NormalizeObservation(env)\n",
    "env =  gym.wrappers.FrameStack(env, num_stack=4)\n",
    "\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "hidden_sizes = (128, 128)\n",
    "lr_policy = 1e-4\n",
    "lr_value = 1e-3\n",
    "clip_epsilon = 0.2\n",
    "batch_size = 256\n",
    "entropy_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "rollout_length = 128\n",
    "\n",
    "wandb.init(project='PPO-Boxing', config={\n",
    "    'gamma': gamma,\n",
    "    'lam': lam,\n",
    "    'hidden_sizes': hidden_sizes,\n",
    "    'lr_policy': lr_policy,\n",
    "    'lr_value': lr_value,\n",
    "    'clip_epsilon': clip_epsilon,\n",
    "    'batch_size': batch_size,\n",
    "    'entropy_coef': entropy_coef,\n",
    "    'vf_coef': vf_coef,\n",
    "    'max_grad_norm': max_grad_norm,\n",
    "    'num_episodes': num_episodes,\n",
    "    'max_steps_per_episode': max_steps_per_episode,\n",
    "    'rollout_length': rollout_length,\n",
    "})\n",
    "\n",
    "# Initialize the PPO agent\n",
    "PPOagent = PPOAgent(\n",
    "    env,\n",
    "    gamma=gamma,\n",
    "    lam=lam,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    lr_policy=lr_policy,\n",
    "    lr_value=lr_value,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    batch_size=batch_size,\n",
    "    entropy_coef=entropy_coef,\n",
    "    vf_coef=vf_coef,\n",
    "    max_grad_norm=max_grad_norm\n",
    ")\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "PPOagent.train(num_episodes, max_steps_per_episode, rollout_length)\n",
    "\n",
    "# Visualize one episode\n",
    "env = make_env()()\n",
    "state, _ = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "frames = []\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action, _, _, _, _ = agent.select_action([state])\n",
    "    state, reward, terminated, truncated, _ = env.step(action[0])\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "# Create and play video clip using the frames and given fps\n",
    "#clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "#clip.ipython_display(rd_kwargs=dict(logger=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0bc23-ccc5-44a1-a034-74080f42cc75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Hyperparameter Sweep PPO (RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be65fc1-4e92-4dc0-a128-10073aac07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "class RolloutStorage:\n",
    "    def __init__(self):\n",
    "        self.clear()\n",
    "    \n",
    "    def add(self, state, action, reward, value, log_prob, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.dones = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "\n",
    "# 3. Define the Policy and Value Networks\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86dbc2b-0607-4525-b169-5b66c27eebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Define the Sweep Configuration\n",
    "sweep_configuration = {\n",
    "    \"method\": \"random\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"Evaluation Reward\",  # The metric to optimize\n",
    "        \"goal\": \"maximize\"             # Whether to \"minimize\" or \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"gamma\": {  # Discount factor for future rewards\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.90,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"lam\": {  # GAE lambda\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.90,\n",
    "            \"max\": 0.99\n",
    "        },\n",
    "        \"lr_policy\": {  # Learning rate for the policy network\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"lr_value\": {   # Learning rate for the value network\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 1e-3\n",
    "        },\n",
    "        \"clip_epsilon\": {  # PPO clipping parameter\n",
    "            \"values\": [0.1, 0.2, 0.3]\n",
    "        },\n",
    "        \"hidden_sizes\": {  # Architecture of hidden layers\n",
    "            \"values\": [\"(64,64,64)\", \"(128,128,128)\", \"(256,256,256)\"]\n",
    "        },\n",
    "        \"entropy_coef\": {  # Entropy regularization coefficient\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.0,\n",
    "            \"max\": 0.1\n",
    "        },\n",
    "        \"vf_coef\": {  # Value function loss coefficient\n",
    "            \"values\": [0.5, 1.0]\n",
    "        },\n",
    "        \"batch_size\": {  # Mini-batch size for updates\n",
    "            \"values\": [32, 64, 128]\n",
    "        },\n",
    "        \"epochs\": {  # Number of PPO update epochs\n",
    "            \"values\": [3, 4, 5]\n",
    "        },\n",
    "        \"max_grad_norm\": {  # Gradient clipping norm\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0.3,\n",
    "            \"max\": 1.0\n",
    "        },\n",
    "        \"num_episodes\": {  # Total number of training episodes\n",
    "            \"values\": [500, 1000]\n",
    "        },\n",
    "        \"rollout_length\": {  # Number of steps to collect before an update\n",
    "            \"values\": [128, 256]\n",
    "        },\n",
    "        \"eval_interval\": {  # Interval (in episodes) for evaluations\n",
    "            \"values\": [50, 100]\n",
    "        },\n",
    "        \"criterion_episodes\": {  # Number of episodes to consider for stopping criterion\n",
    "            \"values\": [5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project='PPO-Boxing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cceac5-f264-4251-ab3c-925205cd0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, config):\n",
    "        # Initialize environment and hyperparameters from wandb config\n",
    "        self.env = env\n",
    "        self.gamma = config.gamma\n",
    "        self.lam = config.lam  # GAE lambda\n",
    "        self.clip_epsilon = config.clip_epsilon\n",
    "        self.epochs = config.epochs\n",
    "        self.batch_size = config.batch_size\n",
    "        self.entropy_coef = config.entropy_coef\n",
    "        self.vf_coef = config.vf_coef\n",
    "        self.max_grad_norm = config.max_grad_norm\n",
    "\n",
    "        # Get action and state dimensions\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.state_dims = self.env.observation_space.shape[0]  # 128-length vector\n",
    "\n",
    "        # Parse hidden_sizes from string to tuple\n",
    "        if isinstance(config.hidden_sizes, str):\n",
    "            self.hidden_sizes = ast.literal_eval(config.hidden_sizes)\n",
    "        else:\n",
    "            self.hidden_sizes = config.hidden_sizes\n",
    "\n",
    "        # Initialize policy and value networks\n",
    "        self.policynet = PolicyNetwork(self.state_dims, self.hidden_sizes, self.num_actions).to(device)\n",
    "        self.valuenet = ValueNetwork(self.state_dims, self.hidden_sizes).to(device)\n",
    "\n",
    "        # Initialize optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policynet.parameters(), lr=config.lr_policy)\n",
    "        self.value_optimizer = optim.Adam(self.valuenet.parameters(), lr=config.lr_value)\n",
    "\n",
    "        # Initialize rollout storage\n",
    "        self.storage = RolloutStorage()\n",
    "########################################################################################################################################################################################################    \n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "        logits = self.policynet(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        if evaluate:\n",
    "            action = torch.argmax(dist.probs, dim=-1)\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        value = self.valuenet(state)\n",
    "        return action.item(), log_prob.item(), entropy.item(), value.item()\n",
    "########################################################################################################################################################################################################      \n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        values = values.tolist() + [0]  # Append 0 for terminal state\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        return advantages\n",
    "  ########################################################################################################################################################################################################    \n",
    "    def update(self):\n",
    "        # Convert rollout storage to tensors\n",
    "        states = torch.FloatTensor(self.storage.states).to(device)\n",
    "        actions = torch.LongTensor(self.storage.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.storage.log_probs).to(device)\n",
    "        returns = torch.FloatTensor(self.storage.returns).to(device)\n",
    "        advantages = torch.FloatTensor(self.storage.advantages).to(device)\n",
    "\n",
    "        # Debugging: Print the sizes of each tensor\n",
    "        print(f\"States size: {states.size(0)}\")\n",
    "        print(f\"Actions size: {actions.size(0)}\")\n",
    "        print(f\"Old Log Probs size: {old_log_probs.size(0)}\")\n",
    "        print(f\"Returns size: {returns.size(0)}\")\n",
    "        print(f\"Advantages size: {advantages.size(0)}\")\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Prepare dataset for mini-batch updates\n",
    "        dataset = torch.utils.data.TensorDataset(states, actions, old_log_probs, returns, advantages)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropies = []\n",
    "\n",
    "        # Perform multiple epochs of updates\n",
    "        for _ in range(self.epochs):\n",
    "            for batch in loader:\n",
    "                b_states, b_actions, b_old_log_probs, b_returns, b_advantages = batch\n",
    "\n",
    "                # Forward pass through policy network\n",
    "                logits = self.policynet(b_states)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                new_log_probs = dist.log_prob(b_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                # Compute ratio for PPO clipping\n",
    "                ratio = (new_log_probs - b_old_log_probs).exp()  # r(theta)\n",
    "                surr1 = ratio * b_advantages  # r(theta) * A\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * b_advantages  # Clipped\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()  # PPO clipped objective\n",
    "\n",
    "                # Compute value loss\n",
    "                values = self.valuenet(b_states).squeeze(-1)\n",
    "                value_loss = F.mse_loss(values, b_returns)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + self.vf_coef * value_loss - self.entropy_coef * entropy\n",
    "\n",
    "                # Backpropagation\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                self.value_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_norm_(self.policynet.parameters(), self.max_grad_norm)\n",
    "                nn.utils.clip_grad_norm_(self.valuenet.parameters(), self.max_grad_norm)\n",
    "\n",
    "                # Update networks\n",
    "                self.policy_optimizer.step()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # Logging losses\n",
    "                policy_losses.append(policy_loss.item())\n",
    "                value_losses.append(value_loss.item())\n",
    "                entropies.append(entropy.item())\n",
    "\n",
    "        # Clear rollout storage after updates\n",
    "        self.storage.clear()\n",
    "\n",
    "        # Log average losses to W&B\n",
    "        wandb.log({\n",
    "            'Policy Loss': np.mean(policy_losses),\n",
    "            'Value Loss': np.mean(value_losses),\n",
    "            'Entropy': np.mean(entropies)\n",
    "        })\n",
    "\n",
    "        return np.mean(policy_losses), np.mean(value_losses), np.mean(entropies)\n",
    "########################################################################################################################################################################################################      \n",
    "    def train_agent(self, max_episodes, rollout_length, eval_interval=50, stop_criterion=None, criterion_episodes=5):\n",
    "        total_rewards = []\n",
    "        moving_average = deque(maxlen=criterion_episodes)\n",
    "\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            # Reset environment and get initial state\n",
    "            state, _ = self.env.reset()\n",
    "            state = state.flatten()  # making sure state is a 1D vector of length 128\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "\n",
    "            for step in range(rollout_length):\n",
    "                # Select action using current policy\n",
    "                action, log_prob, entropy, value = self.select_action(state)\n",
    "                \n",
    "                # Execute action in the environment\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                next_state = next_state.flatten()  # Ensure next_state is a 1D vector of length 128\n",
    "\n",
    "                # Store experience in rollout\n",
    "                self.storage.add(state, action, reward, value, log_prob, terminated or truncated)\n",
    "\n",
    "                # Update state and cumulative reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "\n",
    "                # If episode is done, exit the loop\n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "\n",
    "            # Append the reward for this episode\n",
    "            total_rewards.append(episode_reward)\n",
    "            moving_average.append(episode_reward)\n",
    "\n",
    "            # Compute returns and advantages after rollout\n",
    "            rewards = np.array(self.storage.rewards)\n",
    "            values = np.array(self.storage.values)\n",
    "            dones = np.array(self.storage.dones)\n",
    "            advantages = self.compute_gae(rewards, values, dones)\n",
    "            returns = advantages + values  # Element-wise addition using NumPy arrays\n",
    "\n",
    "            # Store advantages and returns in rollout storage for updating\n",
    "            self.storage.advantages = advantages\n",
    "            self.storage.returns = returns.tolist()\n",
    "\n",
    "            # Perform PPO update using collected rollouts\n",
    "            avg_policy_loss, avg_value_loss, avg_entropy = self.update()\n",
    "\n",
    "            # Log episode reward \n",
    "            wandb.log({\n",
    "                'Episode': episode,\n",
    "                'Reward': episode_reward,\n",
    "                'Episode Length': episode_length\n",
    "            })\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward}, Length = {episode_length}, \"\n",
    "                  f\"Policy Loss = {avg_policy_loss:.4f}, Value Loss = {avg_value_loss:.4f}, Entropy = {avg_entropy:.4f}\")\n",
    "\n",
    "            # Check stopping criterion\n",
    "            if stop_criterion is not None and stop_criterion(list(moving_average)):\n",
    "                print(f\"\\nStopping criterion satisfied after {episode} episodes.\")\n",
    "                break\n",
    "\n",
    "            # Periodic evaluation\n",
    "            if episode % eval_interval == 0:\n",
    "                self.evaluate(episode_num=episode)\n",
    "\n",
    "\n",
    "########################################################################################################################################################################################################      \n",
    "    def evaluate(self, episode_num):\n",
    "        state, _ = self.env.reset()\n",
    "        state = state.flatten()\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            # Render environment frame and store for video\n",
    "            frame = self.env.render() \n",
    "            frames.append(frame)\n",
    "\n",
    "            # Select action deterministically\n",
    "            action, _, _, _ = self.select_action(state, evaluate=True)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = next_state.flatten()\n",
    "\n",
    "            # Update state and cumulative reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # Save video of the evaluation episode\n",
    "        video = np.stack(frames)\n",
    "        video_path = f\"evaluation_episode_{episode_num}.mp4\"\n",
    "        mpy_clip = mpy.ImageSequenceClip(list(video), fps=30)\n",
    "        mpy_clip.write_videofile(video_path, codec=\"libx264\", verbose=False, logger=None)\n",
    "\n",
    "        # Log evaluation results and video to W&B\n",
    "        wandb.log({\n",
    "            'Evaluation Episode': episode_num,\n",
    "            'Evaluation Reward': total_reward,\n",
    "            'Evaluation Video': wandb.Video(video_path, fps=30, format=\"mp4\")\n",
    "        })\n",
    "\n",
    "        # Logging evaluation result\n",
    "        print(f\"Evaluation Episode {episode_num}: Total Reward = {total_reward}\")\n",
    "\n",
    "########################################################################################################################################################################################################      \n",
    "    def save(self, path):\n",
    "        # Save network weights to a file\n",
    "        torch.save({\n",
    "            'policy_net': self.policynet.state_dict(),\n",
    "            'value_net': self.valuenet.state_dict()\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "########################################################################################################################################################################################################      \n",
    "    def load(self, path):\n",
    "        # Load network weights from a file\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        self.policynet.load_state_dict(checkpoint['policy_net'])\n",
    "        self.valuenet.load_state_dict(checkpoint['value_net'])\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc7370-9f25-4343-96bf-2b7c383394bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ecc116-9c67-49b0-8a26-330c94d8b19c",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sweep_agent():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "\n",
    "        # Create the environment with RAM observation type\n",
    "        env = gym.make('ALE/Boxing-v5', render_mode=\"rgb_array\", obs_type=\"ram\")\n",
    "        print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "        \n",
    "        # Ensure the observation is a 128-length vector\n",
    "        assert env.observation_space.shape == (128,), \"Observation space must be a 128-length vector.\"\n",
    "\n",
    "        # Initialise PPOAgent with current wandb config\n",
    "        agent = PPOAgent(env=env, config=config)\n",
    "\n",
    "        # Define stopping criterion\n",
    "        def stopping_criterion(rewards):\n",
    "            return np.mean(rewards) >= 100 \n",
    "\n",
    "        # Start training\n",
    "        agent.train_agent(\n",
    "            max_episodes=config.num_episodes,\n",
    "            rollout_length=config.rollout_length,\n",
    "            eval_interval=config.eval_interval,\n",
    "            stop_criterion=stopping_criterion,\n",
    "            criterion_episodes=config.criterion_episodes\n",
    "        )\n",
    "\n",
    "        # Close the environment after training\n",
    "        env.close()\n",
    "\n",
    "# 6. Launch the Sweep\n",
    "wandb.agent(sweep_id, function=sweep_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133c7f82-9f5d-4277-b068-530f9cd8f35d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-17T12:04:45.772001Z",
     "iopub.status.busy": "2024-10-17T12:04:45.771645Z",
     "iopub.status.idle": "2024-10-17T12:04:46.399657Z",
     "shell.execute_reply": "2024-10-17T12:04:46.399322Z",
     "shell.execute_reply.started": "2024-10-17T12:04:45.771989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/tristancarlisle/PPO-Boxing/sweeps/crve6wfu?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7795fac47690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%wandb  tristancarlisle/PPO-Boxing/sweeps/crve6wfu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a549d-c72b-4619-a61b-7357705f292c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {
    "zotero": {
     "18759246/FUI8Z6RK": {
      "author": [
       {
        "family": "Machado",
        "given": "Marlos C."
       },
       {
        "family": "Bellemare",
        "given": "Marc G."
       },
       {
        "family": "Talvitie",
        "given": "Erik"
       },
       {
        "family": "Veness",
        "given": "Joel"
       },
       {
        "family": "Hausknecht",
        "given": "Matthew J."
       },
       {
        "family": "Bowling",
        "given": "Michael"
       }
      ],
      "container-title": "Journal of Artificial Intelligence Research",
      "id": "18759246/FUI8Z6RK",
      "issued": {
       "date-parts": [
        [
         2018
        ]
       ]
      },
      "page": "523–562",
      "system_id": "zotero|18759246/FUI8Z6RK",
      "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents",
      "type": "article-journal",
      "volume": "61"
     },
     "18759246/KRBTYLUS": {
      "author": [
       {
        "family": "Bellemare",
        "given": "M. G."
       },
       {
        "family": "Naddaf",
        "given": "Y."
       },
       {
        "family": "Veness",
        "given": "J."
       },
       {
        "family": "Bowling",
        "given": "M."
       }
      ],
      "container-title": "Journal of Artificial Intelligence Research",
      "id": "18759246/KRBTYLUS",
      "issued": {
       "date-parts": [
        [
         2013,
         6
        ]
       ]
      },
      "page": "253–279",
      "system_id": "zotero|18759246/KRBTYLUS",
      "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
      "type": "article-journal",
      "volume": "47"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python (ReinforcementLearning)",
   "language": "python",
   "name": "reinforcementlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
